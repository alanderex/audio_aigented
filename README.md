# ğŸ™ï¸ Audio Transcription Pipeline

A modular, GPU-accelerated audio processing pipeline that automates the transcription of spoken audio using NVIDIA NeMo's state-of-the-art ASR models.

## âœ¨ Features

- **ğŸš€ GPU-Accelerated ASR** using NVIDIA NeMo conformer models
- **ğŸ¤ Speaker Diarization** for identifying who spoke when
- **ğŸ“ Batch Processing** of multiple audio files
- **ğŸ¯ High Accuracy** with confidence scoring
- **ğŸ“Š Structured Output** in JSON and human-readable text formats
- **âš™ï¸ Configurable Pipeline** with YAML configuration
- **ğŸ”„ Modular Architecture** for easy extension and maintenance
- **ğŸ§ª Comprehensive Testing** with pytest suite
- **ğŸ’¾ Smart Caching** to avoid re-processing files

## ğŸ—ï¸ Architecture

The pipeline consists of 5 processing stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ load_audio  â”‚ -> â”‚ transcribe   â”‚ -> â”‚ diarize     â”‚ -> â”‚ format      â”‚ -> â”‚ write_output â”‚ -> â”‚ Results     â”‚
â”‚             â”‚    â”‚              â”‚    â”‚             â”‚    â”‚             â”‚    â”‚              â”‚    â”‚             â”‚
â”‚ â€¢ Load .wav â”‚    â”‚ â€¢ NVIDIA     â”‚    â”‚ â€¢ Speaker   â”‚    â”‚ â€¢ Structure â”‚    â”‚ â€¢ JSON files â”‚    â”‚ â€¢ Per-file  â”‚
â”‚ â€¢ Validate  â”‚    â”‚   NeMo ASR   â”‚    â”‚   detection â”‚    â”‚ â€¢ Timestampsâ”‚    â”‚ â€¢ TXT files  â”‚    â”‚   outputs   â”‚
â”‚ â€¢ Resample  â”‚    â”‚ â€¢ GPU accel  â”‚    â”‚ â€¢ Segments  â”‚    â”‚ â€¢ Confidenceâ”‚    â”‚ â€¢ Attributed â”‚    â”‚ â€¢ Summaries â”‚
â”‚             â”‚    â”‚ â€¢ Segments   â”‚    â”‚ â€¢ Clusteringâ”‚    â”‚ â€¢ Speakers  â”‚    â”‚   TXT files  â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## ğŸ“„ Output Formats

The pipeline generates multiple output formats for each processed audio file:

### 1. JSON Format (`transcript.json`)
Structured data with complete metadata, timestamps, and confidence scores:
```json
{
  "audio_file": {
    "path": "audio.wav",
    "duration": 45.2,
    "sample_rate": 16000
  },
  "transcription": {
    "full_text": "Hello there! How are you today?",
    "segments": [
      {
        "text": "Hello there!",
        "start_time": 0.0,
        "end_time": 1.5,
        "confidence": 0.95,
        "speaker_id": "SPEAKER_00"
      }
    ]
  }
}
```

### 2. Human-Readable Text (`transcript.txt`)
Formatted report with statistics and detailed segment breakdown.

### 3. Theater-Style Attribution (`transcript_attributed.txt`)
**New Feature!** Dialog format for conversations with speaker labels:
```
SPEAKER_00: Hello there! How are you doing today?
SPEAKER_01: I'm doing great, thanks for asking!
SPEAKER_00: That's wonderful to hear.
```

This format maintains natural conversation flow and is perfect for:
- Meeting transcripts
- Interview recordings  
- Podcast dialog
- Conference call notes

## ğŸ“¦ Installation

### Prerequisites

- Python 3.12+
- CUDA 12.8 (for GPU acceleration)
- NVIDIA RTX Titan or compatible GPU (recommended)
- [uv](https://docs.astral.sh/uv/) package manager

### Install uv

First, install uv if you haven't already:

```bash
# On macOS and Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# On Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Or using pipx (recommended if you have it)
pipx install uv

# Or as a fallback with pip
pip install uv
```

### Install Dependencies

```bash
# Clone the repository
git clone <your-repo-url>
cd audio_aigented

# Install the package and dependencies
uv pip install -e .

# For development (includes testing tools)
uv pip install -e ".[dev]"
```

### NVIDIA NeMo Installation

The pipeline requires NVIDIA NeMo for ASR functionality:

```bash
uv pip install "nemo-toolkit[asr]"
```

## ğŸš€ Quick Start

### 1. Basic Usage with CLI

```bash
# Process all .wav files in ./inputs/ directory
uv run python main.py --input-dir ./inputs

# Use custom output directory
uv run python main.py --input-dir ./my_audio --output-dir ./my_results

# Use CPU instead of GPU
uv run python main.py --input-dir ./inputs --device cpu

# Disable speaker diarization (faster processing)
uv run python main.py --input-dir ./inputs --disable-diarization

# Show help for all options
uv run python main.py --help
```

### 2. Using the Python API

```python
from pathlib import Path
from src.audio_aigented.pipeline import TranscriptionPipeline

# Initialize pipeline with default settings
pipeline = TranscriptionPipeline()

# Process all files in input directory
results = pipeline.process_directory(Path("./inputs"))

# Process a single file
result = pipeline.process_single_file(Path("./inputs/meeting.wav"))
print(f"Transcription: {result.full_text}")
```

### 3. Custom Configuration

Create a custom configuration file:

```yaml
# config/my_config.yaml
input_dir: "./my_audio_files"
output_dir: "./my_results"

audio:
  sample_rate: 16000
  batch_size: 4

transcription:
  model_name: "stt_en_conformer_ctc_large"
  device: "cuda"
  enable_confidence_scores: true

processing:
  enable_diarization: true
  enable_caching: true
  log_level: "INFO"

output:
  formats: ["json", "txt", "attributed_txt"]
  include_timestamps: true
  pretty_json: true
```

Use with CLI:
```bash
uv run python main.py --config ./config/my_config.yaml
```

## ğŸ“ Input and Output

### Input Structure
```
inputs/
â”œâ”€â”€ meeting_recording.wav
â”œâ”€â”€ interview_audio.wav
â””â”€â”€ conference_call.wav
```

### Output Structure
```
outputs/
â”œâ”€â”€ meeting_recording/
â”‚   â”œâ”€â”€ transcript.json              # Structured data with timestamps & speakers
â”‚   â”œâ”€â”€ transcript.txt               # Human-readable format with statistics
â”‚   â””â”€â”€ transcript_attributed.txt    # Theater-style speaker dialog
â”œâ”€â”€ interview_audio/
â”‚   â”œâ”€â”€ transcript.json
â”‚   â”œâ”€â”€ transcript.txt
â”‚   â””â”€â”€ transcript_attributed.txt
â””â”€â”€ processing_summary.txt           # Overall processing report
```

### Sample JSON Output
```json
{
  "audio_file": {
    "path": "./inputs/meeting.wav",
    "duration": 125.3,
    "sample_rate": 16000
  },
  "transcription": {
    "full_text": "Good morning everyone, let's begin the meeting...",
    "segments": [
      {
        "text": "Good morning everyone",
        "start_time": 0.0,
        "end_time": 2.1,
        "confidence": 0.95
      }
    ]
  },
  "processing": {
    "processing_time": 12.4,
    "model_info": {"name": "stt_en_conformer_ctc_large"}
  }
}
```

## âš™ï¸ Configuration Options

### Audio Processing
- `sample_rate`: Target sample rate (default: 16000 Hz)
- `batch_size`: Number of files to process in parallel
- `max_duration`: Maximum segment duration for processing

### ASR Settings
- `model_name`: NVIDIA NeMo model to use
  - `stt_en_conformer_ctc_small` (fast, lower accuracy)
  - `stt_en_conformer_ctc_medium` (balanced)
  - `stt_en_conformer_ctc_large` (slow, higher accuracy)
- `device`: Processing device (`cuda` or `cpu`)
- `enable_confidence_scores`: Include confidence scores in output

### Speaker Diarization
- `enable_diarization`: Enable/disable speaker identification (default: `true`)
- Command line: `--enable-diarization` / `--disable-diarization`
- When enabled, segments are automatically labeled with speaker IDs (SPEAKER_00, SPEAKER_01, etc.)
- Uses NVIDIA NeMo's clustering diarization for accurate speaker separation

### Processing Options
- `enable_caching`: Cache models and intermediate results
- `parallel_workers`: Number of parallel processing workers
- `log_level`: Logging verbosity (DEBUG, INFO, WARNING, ERROR)

### Output Options
- `formats`: Output formats (`["json", "txt", "attributed_txt"]`)
- `include_timestamps`: Include timing information
- `include_confidence`: Include confidence scores in output
- `pretty_json`: Format JSON with indentation

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=src --cov-report=term-missing

# Run specific test file
uv run pytest tests/test_models.py

# Run with verbose output
uv run pytest -v
```

## ğŸƒâ€â™‚ï¸ Performance

### GPU Performance (RTX Titan 24GB)
- Small model: ~15x real-time speed
- Medium model: ~8x real-time speed  
- Large model: ~4x real-time speed

### CPU Performance (16-core)
- Small model: ~1.5x real-time speed
- Medium model: ~0.8x real-time speed
- Large model: ~0.4x real-time speed

## ğŸ› ï¸ Development

### Project Structure
```
audio_aigented/
â”œâ”€â”€ src/audio_aigented/           # Main package
â”‚   â”œâ”€â”€ audio/                    # Audio loading and preprocessing
â”‚   â”œâ”€â”€ transcription/            # ASR processing with NeMo
â”‚   â”œâ”€â”€ formatting/               # Output formatting
â”‚   â”œâ”€â”€ output/                   # File writing
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”œâ”€â”€ models/                   # Pydantic data models
â”‚   â””â”€â”€ pipeline.py               # Main orchestration
â”œâ”€â”€ tests/                        # Test suite
â”œâ”€â”€ config/                       # Configuration files
â”œâ”€â”€ examples/                     # Usage examples
â””â”€â”€ main.py                       # CLI entry point
```

### Code Style
- Uses `ruff` for linting and formatting
- Follows PEP8 with type hints
- Google-style docstrings
- Maximum 500 lines per file

### Adding New Features
1. Create feature branch
2. Implement with comprehensive tests
3. Update documentation
4. Submit pull request

## ğŸ”§ Troubleshooting

### Common Issues

**CUDA Out of Memory**
```bash
# Use smaller batch size
uv run python main.py --input-dir ./inputs --config config/cpu_config.yaml

# Or switch to CPU
uv run python main.py --input-dir ./inputs --device cpu
```

**No Audio Files Found**
- Ensure `.wav` files are in the input directory
- Check file permissions
- Use `--dry-run` to see what files would be processed

**Model Download Issues**
- NeMo models are downloaded automatically on first use
- Ensure internet connection for initial model download
- Models are cached in `~/.cache/torch/NeMo/`

## ğŸ“„ License

[Your License Here]

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Update documentation
5. Submit a pull request

## ğŸ“ Support

- Create an issue for bugs or feature requests
- Check existing issues before creating new ones
- Provide detailed information including error logs

---

**Built with â¤ï¸ using NVIDIA NeMo and modern Python practices**
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Audio Transcription Pipeline","text":"<p>A GPU-accelerated audio transcription pipeline using NVIDIA NeMo for automatic speech recognition (ASR) with speaker diarization capabilities.</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in minutes with our simple installation process</p> <p> Getting started</p> </li> <li> <p>:material-gpu:{ .lg .middle } GPU Accelerated</p> <p>Leverage NVIDIA GPUs for fast, accurate transcription</p> <p> Performance guide</p> </li> <li> <p> Speaker Diarization</p> <p>Automatically identify and separate different speakers</p> <p> Diarization guide</p> </li> <li> <p> Docker Support</p> <p>Deploy easily with our pre-configured Docker setup</p> <p> Docker deployment</p> </li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\ude80 High Performance: GPU-accelerated processing for fast transcription</li> <li>\ud83c\udfaf Accurate ASR: State-of-the-art NVIDIA NeMo models</li> <li>\ud83d\udc65 Speaker Diarization: Automatic speaker identification and segmentation</li> <li>\ud83d\udcdd Multiple Output Formats: JSON, plain text, and attributed dialogue</li> <li>\ud83d\udd27 Configurable: Flexible YAML-based configuration</li> <li>\ud83d\udc33 Docker Ready: Pre-configured containers for easy deployment</li> <li>\ud83d\udcca Batch Processing: Process entire directories of audio files</li> <li>\ud83d\udcbe Smart Caching: Avoid reprocessing with intelligent caching</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>This pipeline provides a complete solution for transcribing audio files with speaker attribution:</p> <pre><code>graph LR\n    A[Audio Files] --&gt; B[Audio Loader]\n    B --&gt; C{Diarization?}\n    C --&gt;|Yes| D[Speaker Diarization]\n    C --&gt;|No| E[Full Audio]\n    D --&gt; F[Speaker Segments]\n    E --&gt; F\n    F --&gt; G[ASR Transcription]\n    G --&gt; H[Output Formatter]\n    H --&gt; I[Multiple Formats]</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Process a directory of audio files\npython main.py --input-dir ./inputs --output-dir ./outputs\n\n# Process with speaker diarization disabled (faster)\npython main.py --input-dir ./inputs --disable-diarization\n\n# Use Docker for deployment\ndocker-compose run --rm audio-transcription\n</code></pre>"},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>GPU: NVIDIA GPU with 4GB+ VRAM (8GB+ recommended)</li> <li>CUDA: Version 11.8 or higher</li> <li>Python: 3.10 or higher</li> <li>RAM: 16GB minimum</li> <li>Storage: ~3GB for models plus audio file space</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Installation</p> <p>Set up your development environment</p> </li> <li> <p> Configuration</p> <p>Customize the pipeline for your needs</p> </li> <li> <p> API Reference</p> <p>Explore the complete API documentation</p> </li> <li> <p> Contributing</p> <p>Join us in improving the project</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"IMPLEMENTATION_PLAN/","title":"\ud83c\udfd7\ufe0f Core ASR Transcription Pipeline - Implementation Plan","text":""},{"location":"IMPLEMENTATION_PLAN/#architecture-overview","title":"\ud83d\udcd0 Architecture Overview","text":"<pre><code>graph LR\n    A[Audio Files&lt;br/&gt;.wav] --&gt; B[Audio Loader]\n    B --&gt; C{Diarization&lt;br/&gt;Enabled?}\n    C --&gt;|Yes| D[Speaker Diarizer&lt;br/&gt;NVIDIA NeMo]\n    C --&gt;|No| E[Single Speaker&lt;br/&gt;Assignment]\n    D --&gt; F[ASR Transcriber&lt;br/&gt;Per Speaker Segment]\n    E --&gt; G[ASR Transcriber&lt;br/&gt;Full Audio]\n    F --&gt; H[Output Formatter]\n    G --&gt; H\n    H --&gt; I[File Writer]\n    I --&gt; J[Results&lt;br/&gt;JSON + TXT]\n\n    K[Config YAML] --&gt; B\n    K --&gt; D\n    K --&gt; F\n    K --&gt; G\n    K --&gt; H\n    K --&gt; I\n\n    subgraph \"Pipeline Stages\"\n        B\n        C\n        D\n        E\n        F\n        G\n        H\n        I\n    end</code></pre>"},{"location":"IMPLEMENTATION_PLAN/#core-components","title":"\ud83e\udde9 Core Components","text":"Component Purpose Key Features Audio Loader Load and prepare <code>.wav</code> files Resampling, validation, batching Speaker Diarizer Identify speaker segments NVIDIA NeMo clustering, RTTM parsing, temporal mapping ASR Transcriber Speech-to-text conversion NVIDIA NeMo integration, GPU optimization, per-speaker processing Output Formatter Structure transcription results Timestamps, confidence scores, speaker attribution File Writer Save processed results JSON/TXT/attributed output, directory creation Config Manager Handle YAML configurations OmegaConf integration, validation Main Pipeline Orchestrate processing stages Diarization-first workflow, error handling, logging"},{"location":"IMPLEMENTATION_PLAN/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>audio_aigented/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 audio_aigented/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 audio/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 loader.py          # Audio file loading &amp; preprocessing\n\u2502       \u251c\u2500\u2500 transcription/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 asr.py             # NVIDIA NeMo ASR integration\n\u2502       \u251c\u2500\u2500 formatting/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 formatter.py       # Output structuring\n\u2502       \u251c\u2500\u2500 output/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 writer.py          # File writing (JSON/TXT)\n\u2502       \u251c\u2500\u2500 config/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 manager.py         # Configuration management\n\u2502       \u251c\u2500\u2500 models/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 schemas.py         # Pydantic data models\n\u2502       \u2514\u2500\u2500 pipeline.py            # Main pipeline orchestration\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_audio/\n\u2502   \u251c\u2500\u2500 test_transcription/\n\u2502   \u251c\u2500\u2500 test_formatting/\n\u2502   \u251c\u2500\u2500 test_output/\n\u2502   \u2514\u2500\u2500 test_pipeline.py\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 default.yaml               # Default configuration\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 sample_audio/\n\u2502   \u2514\u2500\u2500 basic_usage.py\n\u251c\u2500\u2500 inputs/                        # Default input directory for .wav files\n\u251c\u2500\u2500 outputs/                       # Default output directory structure\n\u2502   \u2514\u2500\u2500 [audio_filename]/          # Per-file output directories\n\u2502       \u251c\u2500\u2500 transcript.json        # Structured transcription data\n\u2502       \u2514\u2500\u2500 transcript.txt         # Human-readable transcript\n\u251c\u2500\u2500 cache/                         # ASR model and processing cache\n\u251c\u2500\u2500 main.py                        # CLI entry point\n\u251c\u2500\u2500 pyproject.toml                 # Dependencies &amp; project config\n\u2514\u2500\u2500 README.md                      # Documentation\n</code></pre>"},{"location":"IMPLEMENTATION_PLAN/#directory-management-strategy","title":"\ud83d\udcc2 Directory Management Strategy","text":""},{"location":"IMPLEMENTATION_PLAN/#input-directory-structure","title":"Input Directory Structure","text":"<ul> <li>Default Location: <code>./inputs/</code></li> <li>File Pattern: <code>*.wav</code> files in the input directory</li> <li>Validation: Check file existence, format, and audio properties</li> <li>Scanning: Recursive scanning with configurable depth</li> </ul>"},{"location":"IMPLEMENTATION_PLAN/#output-directory-structure","title":"Output Directory Structure","text":"<ul> <li>Default Location: <code>./outputs/</code></li> <li>Per-File Directories: Each audio file gets its own subdirectory</li> <li>Naming Convention: <code>[audio_filename_without_extension]/</code></li> <li>Contents:</li> <li><code>transcript.json</code> - Structured transcription with timestamps, confidence scores</li> <li><code>transcript.txt</code> - Human-readable transcript format</li> </ul>"},{"location":"IMPLEMENTATION_PLAN/#example-directory-layout","title":"Example Directory Layout","text":"<pre><code>inputs/\n\u251c\u2500\u2500 meeting_recording.wav\n\u251c\u2500\u2500 interview_audio.wav\n\u2514\u2500\u2500 conference_call.wav\n\noutputs/\n\u251c\u2500\u2500 meeting_recording/\n\u2502   \u251c\u2500\u2500 transcript.json\n\u2502   \u2514\u2500\u2500 transcript.txt\n\u251c\u2500\u2500 interview_audio/\n\u2502   \u251c\u2500\u2500 transcript.json\n\u2502   \u2514\u2500\u2500 transcript.txt\n\u2514\u2500\u2500 conference_call/\n    \u251c\u2500\u2500 transcript.json\n    \u2514\u2500\u2500 transcript.txt\n</code></pre>"},{"location":"IMPLEMENTATION_PLAN/#implementation-steps","title":"\ud83d\udd27 Implementation Steps","text":""},{"location":"IMPLEMENTATION_PLAN/#phase-1-project-foundation","title":"Phase 1: Project Foundation","text":"<ol> <li>Dependencies Setup - Add NVIDIA NeMo, PyTorch, librosa, pydantic, omegaconf</li> <li>Directory Structure Creation - Create <code>inputs/</code>, <code>outputs/</code>, <code>cache/</code>, <code>src/</code>, <code>tests/</code>, <code>config/</code>, <code>examples/</code> directories</li> <li>Data Models - Create Pydantic schemas for transcription results</li> <li>Configuration System - YAML-based config with OmegaConf</li> <li>Basic Project Structure - Create module directories and <code>__init__.py</code> files</li> </ol>"},{"location":"IMPLEMENTATION_PLAN/#phase-2-core-components","title":"Phase 2: Core Components","text":"<ol> <li>Audio Loader - <code>.wav</code> file loading, validation, resampling for NeMo compatibility</li> <li>Speaker Diarizer - NVIDIA NeMo clustering diarization, RTTM output parsing</li> <li>ASR Transcriber - NVIDIA NeMo integration with per-speaker segment processing</li> <li>Output Formatter - Structure results with timestamps, confidence scores, and speaker IDs</li> <li>File Writer - Save JSON/TXT/attributed outputs with proper directory structure</li> </ol>"},{"location":"IMPLEMENTATION_PLAN/#phase-3-pipeline-integration","title":"Phase 3: Pipeline Integration","text":"<ol> <li>Main Pipeline - Orchestrate diarization-first workflow with conditional processing</li> <li>CLI Interface - Command-line interface through <code>main.py</code> with diarization toggle</li> <li>Caching System - Cache both diarization and ASR results to avoid re-processing</li> <li>Logging &amp; Monitoring - Comprehensive logging for debugging</li> </ol>"},{"location":"IMPLEMENTATION_PLAN/#phase-4-testing-documentation","title":"Phase 4: Testing &amp; Documentation","text":"<ol> <li>Unit Tests - Pytest tests for each component (minimum 3 tests per module)</li> <li>Integration Tests - End-to-end pipeline testing</li> <li>Documentation - Update README with usage examples</li> <li>Example Scripts - Demonstration scripts with sample audio</li> </ol>"},{"location":"IMPLEMENTATION_PLAN/#key-technical-details","title":"\u2699\ufe0f Key Technical Details","text":""},{"location":"IMPLEMENTATION_PLAN/#diarization-first-workflow","title":"Diarization-First Workflow","text":"<p>When diarization is enabled: 1. Speaker Diarization: Process entire audio file to identify speaker segments 2. Segment Extraction: Extract audio segments for each speaker change 3. Per-Speaker Transcription: Transcribe each segment individually 4. Result Merging: Combine transcriptions with speaker attribution</p> <p>When diarization is disabled: 1. Single Speaker Assignment: Assume entire audio is from one speaker (SPEAKER_00) 2. Full Audio Transcription: Process entire file in chunks 3. Consistent Output Format: Maintain same output structure for compatibility</p>"},{"location":"IMPLEMENTATION_PLAN/#nvidia-nemo-integration","title":"NVIDIA NeMo Integration","text":"<ul> <li>Use pre-trained conformer models (e.g., <code>stt_en_conformer_ctc_large</code>)</li> <li>Use clustering diarizer with TitanNet embeddings for speaker identification</li> <li>GPU acceleration with CUDA 12.8</li> <li>Batch processing for efficiency</li> <li>Automatic model caching</li> </ul>"},{"location":"IMPLEMENTATION_PLAN/#configuration-schema","title":"Configuration Schema","text":"<pre><code>audio:\n  sample_rate: 16000\n  batch_size: 8\n\ntranscription:\n  model_name: \"stt_en_conformer_ctc_large\"\n  device: \"cuda\"\n  enable_confidence_scores: true\n\ndiarization:\n  enabled: true\n  min_speakers: 1\n  max_speakers: 8\n  config_path: \"./config/diarization_config.yaml\"\n\noutput:\n  formats: [\"json\", \"txt\", \"attributed_txt\"]\n  include_timestamps: true\n  output_dir: \"./outputs\"\n\nprocessing:\n  enable_caching: true\n  cache_dir: \"./cache\"\n  enable_diarization: true\n</code></pre>"},{"location":"IMPLEMENTATION_PLAN/#data-models","title":"Data Models","text":"<ul> <li><code>TranscriptionResult</code> - Complete transcription with metadata</li> <li><code>AudioSegment</code> - Individual audio segments with timestamps</li> <li><code>ProcessingConfig</code> - Configuration validation schema</li> </ul>"},{"location":"IMPLEMENTATION_PLAN/#success-criteria","title":"\ud83c\udfaf Success Criteria","text":"<ul> <li>Successfully transcribe <code>.wav</code> files using NVIDIA NeMo</li> <li>Generate structured JSON and readable TXT outputs</li> <li>GPU-optimized processing with caching</li> <li>Comprehensive test coverage (&gt;90%)</li> <li>Clear documentation and usage examples</li> <li>Modular, maintainable codebase (&lt;500 lines per file)</li> </ul>"},{"location":"IMPLEMENTATION_PLAN/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Ready to switch to Code Mode for implementation starting with Phase 1: Project Foundation.</p>"},{"location":"deployment/docker/","title":"Docker Setup for Audio Transcription Pipeline","text":"<p>This guide explains how to run the audio transcription pipeline using Docker.</p>"},{"location":"deployment/docker/#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker and Docker Compose installed</li> <li>NVIDIA Docker runtime for GPU support:    <pre><code># Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre></li> </ol>"},{"location":"deployment/docker/#directory-structure","title":"Directory Structure","text":"<p>The Docker setup uses the following volume mappings:</p> <pre><code>audio_aigented/\n\u251c\u2500\u2500 inputs/       # Place your .wav files here\n\u251c\u2500\u2500 outputs/      # Transcription results will be saved here\n\u251c\u2500\u2500 models/       # NeMo models will be cached here\n\u251c\u2500\u2500 cache/        # General cache directory\n\u2514\u2500\u2500 config/       # Configuration files\n</code></pre>"},{"location":"deployment/docker/#quick-start","title":"Quick Start","text":""},{"location":"deployment/docker/#1-build-the-docker-image","title":"1. Build the Docker Image","text":"<pre><code>docker-compose build\n</code></pre>"},{"location":"deployment/docker/#2-test-setup","title":"2. Test Setup","text":"<p>Run the test script to verify everything is configured correctly:</p> <pre><code>./scripts/docker-test.sh\n</code></pre>"},{"location":"deployment/docker/#3-process-audio-files","title":"3. Process Audio Files","text":"<p>Place your <code>.wav</code> files in the <code>inputs/</code> directory, then run:</p> <pre><code># Using the helper script (recommended)\n./scripts/run-transcription.sh\n\n# Or manually with docker-compose\necho \"Y\" | docker-compose run --rm audio-transcription\n\n# Process specific files with custom output\ndocker-compose run --rm \\\n  -v /path/to/audio:/data/inputs:ro \\\n  -v /path/to/results:/data/outputs \\\n  audio-transcription\n</code></pre>"},{"location":"deployment/docker/#4-development-mode","title":"4. Development Mode","text":"<p>For interactive development with the container:</p> <pre><code>docker-compose run --rm --entrypoint bash audio-transcription\n</code></pre>"},{"location":"deployment/docker/#docker-components","title":"Docker Components","text":""},{"location":"deployment/docker/#dockerfile","title":"Dockerfile","text":"<p>Multi-stage build with: - NVIDIA CUDA 11.8 base image for GPU support - Python 3.10 and all required system dependencies - Non-root user for security - Proper cache directory configuration for NeMo models - Optimized layer caching for faster rebuilds</p>"},{"location":"deployment/docker/#docker-composeyml","title":"docker-compose.yml","text":"<p>Main orchestration file with: - GPU runtime configuration - Volume mappings for inputs, outputs, models, and cache - Resource limits (16GB RAM) - Development profile for interactive debugging</p>"},{"location":"deployment/docker/#docker-entrypointsh","title":"docker-entrypoint.sh","text":"<p>Smart entrypoint script that: - Checks GPU availability - Validates input/output directories - Provides helpful error messages - Supports both batch processing and interactive mode</p>"},{"location":"deployment/docker/#processing-options","title":"Processing Options","text":""},{"location":"deployment/docker/#default-processing-with-diarization","title":"Default Processing (with diarization)","text":"<pre><code>./scripts/run-transcription.sh\n</code></pre>"},{"location":"deployment/docker/#fast-processing-without-diarization","title":"Fast Processing (without diarization)","text":"<pre><code>docker-compose run --rm audio-transcription python main.py --input-dir /data/inputs --output-dir /data/outputs --disable-diarization\n</code></pre>"},{"location":"deployment/docker/#custom-configuration","title":"Custom Configuration","text":"<pre><code>docker-compose run --rm \\\n  -v ./custom_config.yaml:/app/config/default.yaml:ro \\\n  audio-transcription\n</code></pre>"},{"location":"deployment/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/docker/#gpu-not-detected","title":"GPU Not Detected","text":"<p>If you see \"No GPU detected\" warning: 1. Check NVIDIA drivers: <code>nvidia-smi</code> 2. Verify Docker GPU access: <code>docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi</code> 3. Ensure NVIDIA Docker runtime is installed</p>"},{"location":"deployment/docker/#permission-issues","title":"Permission Issues","text":"<p>If you encounter permission errors: - The container runs as user <code>appuser</code> (UID 1000) - Ensure input/output directories have appropriate permissions - Use <code>--user $(id -u):$(id -g)</code> flag if needed</p>"},{"location":"deployment/docker/#model-download-issues","title":"Model Download Issues","text":"<p>Models are automatically downloaded on first use: - Downloads are cached in <code>models/</code> directory - Ensure you have ~3GB free space - Check internet connectivity</p>"},{"location":"deployment/docker/#advanced-usage","title":"Advanced Usage","text":""},{"location":"deployment/docker/#running-with-specific-gpu","title":"Running with Specific GPU","text":"<pre><code>docker-compose run --rm -e CUDA_VISIBLE_DEVICES=0 audio-transcription\n</code></pre>"},{"location":"deployment/docker/#using-different-models","title":"Using Different Models","text":"<p>Edit <code>config/default.yaml</code> or mount a custom config: <pre><code>model:\n  name: \"stt_en_conformer_ctc_large\"  # or another NeMo model\n</code></pre></p>"},{"location":"deployment/docker/#batch-processing-multiple-directories","title":"Batch Processing Multiple Directories","text":"<pre><code>for dir in /path/to/audio/*/; do\n  docker-compose run --rm \\\n    -v \"$dir\":/data/inputs:ro \\\n    -v \"./outputs/$(basename \"$dir\")\":/data/outputs \\\n    audio-transcription\ndone\n</code></pre>"},{"location":"deployment/docker/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>GPU: NVIDIA GPU with 4GB+ VRAM (8GB+ recommended)</li> <li>RAM: 16GB minimum</li> <li>Storage: ~3GB for models, plus space for audio files</li> <li>CUDA: Version 11.8 (handled by Docker)</li> </ul>"},{"location":"deployment/models/","title":"Model Management Strategy","text":"<p>This document outlines the approach for managing large NVIDIA NeMo model files in this repository.</p>"},{"location":"deployment/models/#current-models","title":"Current Models","text":"<p>The pipeline uses the following pre-trained models: - ASR: <code>stt_en_conformer_ctc_large</code> (~700MB) - VAD: <code>vad_multilingual_marblenet</code> (~10MB) - Speaker Embeddings: <code>titanet-l</code> (~100MB)</p> <p>Total size: ~810MB per model set</p>"},{"location":"deployment/models/#recommended-approach-automatic-download","title":"Recommended Approach: Automatic Download","text":"<p>Models are not included in the repository. Instead, they are automatically downloaded on first use:</p> <ol> <li>Models are downloaded to the <code>models/</code> directory (excluded from git)</li> <li>The directory structure is preserved for caching</li> <li>Subsequent runs use the cached models</li> </ol>"},{"location":"deployment/models/#benefits","title":"Benefits","text":"<ul> <li>Keeps repository size small</li> <li>Models always up-to-date from NVIDIA's servers</li> <li>No Git LFS complexity</li> <li>Works seamlessly with Docker setup</li> </ul>"},{"location":"deployment/models/#setup-instructions","title":"Setup Instructions","text":"<p>For users: <pre><code># Models will be auto-downloaded to models/ on first run\n# Ensure you have ~1GB free space\npython main.py --input-dir ./inputs\n\n# Or with Docker (models persist in volume)\ndocker-compose run --rm audio-transcription\n</code></pre></p> <p>For developers who want to pre-download: <pre><code>from nemo.collections.asr.models import EncDecCTCModel\nfrom nemo.collections.asr.models.classification_models import EncDecClassificationModel\n\n# Pre-download models\nasr_model = EncDecCTCModel.from_pretrained(\"stt_en_conformer_ctc_large\")\nvad_model = EncDecClassificationModel.from_pretrained(\"vad_multilingual_marblenet\")\n</code></pre></p>"},{"location":"deployment/models/#alternative-git-lfs-not-recommended","title":"Alternative: Git LFS (Not Recommended)","text":"<p>If you must version control models:</p> <pre><code># Initialize Git LFS\ngit lfs install\n\n# Track model files\ngit lfs track \"models/**/*.nemo\"\ngit add .gitattributes\n\n# Add and commit models\ngit add models/\ngit commit -m \"Add NeMo models with LFS\"\n</code></pre> <p>Drawbacks: - Requires Git LFS setup for all users - Large clone sizes - LFS bandwidth/storage costs - Models may become outdated</p>"},{"location":"deployment/models/#docker-considerations","title":"Docker Considerations","text":"<p>The Docker setup handles models efficiently: - Models are stored in a persistent volume - Shared across container runs - Automatically downloaded if missing - No rebuild needed when models change</p> <pre><code>volumes:\n  - ./models:/home/appuser/.cache/torch/NeMo\n</code></pre>"},{"location":"deployment/models/#verification","title":"Verification","text":"<p>To verify models are properly cached:</p> <pre><code># Check model directory\nls -la models/\n\n# Expected structure:\n# models/\n# \u251c\u2500\u2500 stt_en_conformer_ctc_large/\n# \u2502   \u2514\u2500\u2500 &lt;hash&gt;/\n# \u2502       \u2514\u2500\u2500 stt_en_conformer_ctc_large.nemo\n# \u251c\u2500\u2500 titanet-l/\n# \u2502   \u2514\u2500\u2500 &lt;hash&gt;/\n# \u2502       \u2514\u2500\u2500 titanet-l.nemo\n# \u2514\u2500\u2500 vad_multilingual_marblenet/\n#     \u2514\u2500\u2500 &lt;hash&gt;/\n#         \u2514\u2500\u2500 vad_multilingual_marblenet.nemo\n</code></pre>"},{"location":"deployment/models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/models/#models-not-downloading","title":"Models Not Downloading","text":"<ul> <li>Check internet connectivity</li> <li>Verify write permissions to <code>models/</code> directory</li> <li>Ensure sufficient disk space (~1GB)</li> <li>Check NVIDIA NGC is accessible</li> </ul>"},{"location":"deployment/models/#using-custom-models","title":"Using Custom Models","text":"<p>Edit <code>config/default.yaml</code>: <pre><code>model:\n  name: \"your_custom_model\"  # Must be a valid NeMo model name\n\ndiarization:\n  embedding:\n    model_path: \"titanet-l\"  # Or path to custom embedding model\n</code></pre></p>"},{"location":"deployment/performance/","title":"Performance Tuning","text":"<p>This guide covers optimization strategies to maximize the performance of the Audio Transcription Pipeline.</p>"},{"location":"deployment/performance/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"deployment/performance/#gpu-performance-rtx-3090","title":"GPU Performance (RTX 3090)","text":"Model Audio Duration Processing Time Speed Factor GPU Memory Small 5 min 20s 15x 2GB Medium 5 min 38s 8x 3GB Large 5 min 75s 4x 4GB"},{"location":"deployment/performance/#cpu-performance-16-core","title":"CPU Performance (16-core)","text":"Model Audio Duration Processing Time Speed Factor RAM Usage Small 5 min 200s 1.5x 4GB Medium 5 min 375s 0.8x 6GB Large 5 min 750s 0.4x 8GB"},{"location":"deployment/performance/#gpu-optimization","title":"GPU Optimization","text":""},{"location":"deployment/performance/#cuda-configuration","title":"CUDA Configuration","text":"<pre><code>import torch\n\n# Check GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\nprint(f\"Current GPU: {torch.cuda.get_device_name()}\")\n\n# Set GPU device\ntorch.cuda.set_device(0)  # Use first GPU\n</code></pre>"},{"location":"deployment/performance/#memory-management","title":"Memory Management","text":"<pre><code># Optimize GPU memory usage\ntranscription:\n  batch_size: 1                    # Reduce for less memory\n  model:\n    compute_dtype: \"float16\"       # Use mixed precision\n\n  gpu:\n    memory_fraction: 0.8           # Reserve 20% for system\n    allow_growth: true             # Dynamic allocation\n</code></pre>"},{"location":"deployment/performance/#multi-gpu-support","title":"Multi-GPU Support","text":"<pre><code># Use multiple GPUs\nclass MultiGPUPipeline:\n    def __init__(self, gpu_ids=[0, 1]):\n        self.models = []\n        for gpu_id in gpu_ids:\n            model = load_model(device=f\"cuda:{gpu_id}\")\n            self.models.append(model)\n\n    def process_batch(self, files):\n        # Distribute files across GPUs\n        results = []\n        for i, file in enumerate(files):\n            gpu_idx = i % len(self.models)\n            result = self.models[gpu_idx].transcribe(file)\n            results.append(result)\n        return results\n</code></pre>"},{"location":"deployment/performance/#cpu-optimization","title":"CPU Optimization","text":""},{"location":"deployment/performance/#thread-configuration","title":"Thread Configuration","text":"<pre><code># CPU optimization settings\nprocessing:\n  num_threads: 16                  # Match CPU cores\n  use_mkl: true                    # Intel MKL acceleration\n\ntorch:\n  num_threads: 8                   # PyTorch threads\n  num_interop_threads: 4           # Inter-op parallelism\n</code></pre>"},{"location":"deployment/performance/#numa-optimization","title":"NUMA Optimization","text":"<pre><code># Pin process to NUMA node\nnumactl --cpunodebind=0 --membind=0 python main.py\n\n# Check NUMA configuration\nnumactl --hardware\n</code></pre>"},{"location":"deployment/performance/#batch-processing-optimization","title":"Batch Processing Optimization","text":""},{"location":"deployment/performance/#optimal-batch-sizes","title":"Optimal Batch Sizes","text":"GPU Memory Batch Size (Small) Batch Size (Medium) Batch Size (Large) 4GB 16 8 4 8GB 32 16 8 16GB 64 32 16 24GB 96 48 24"},{"location":"deployment/performance/#dynamic-batching","title":"Dynamic Batching","text":"<pre><code>def get_optimal_batch_size(gpu_memory_gb, model_size):\n    \"\"\"Calculate optimal batch size based on GPU memory\"\"\"\n    base_sizes = {\n        \"small\": 16,\n        \"medium\": 8,\n        \"large\": 4\n    }\n\n    memory_multiplier = gpu_memory_gb / 4.0\n    optimal_size = int(base_sizes[model_size] * memory_multiplier)\n\n    return max(1, optimal_size)\n</code></pre>"},{"location":"deployment/performance/#pipeline-optimization","title":"Pipeline Optimization","text":""},{"location":"deployment/performance/#preprocessing-cache","title":"Preprocessing Cache","text":"<pre><code>class CachedAudioLoader:\n    def __init__(self, cache_dir=\".cache/audio\"):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n\n    def load(self, audio_path):\n        cache_path = self.cache_dir / f\"{audio_path.stem}_16k.npy\"\n\n        if cache_path.exists():\n            return np.load(cache_path)\n\n        # Load and preprocess\n        audio, sr = sf.read(audio_path)\n        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n\n        # Cache preprocessed audio\n        np.save(cache_path, audio)\n        return audio\n</code></pre>"},{"location":"deployment/performance/#parallel-pipeline","title":"Parallel Pipeline","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n\nclass ParallelPipeline:\n    def __init__(self, num_workers=4):\n        self.num_workers = num_workers\n\n    def process_directory(self, input_dir):\n        files = list(Path(input_dir).glob(\"*.wav\"))\n\n        # Parallel audio loading\n        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n            audio_data = list(executor.map(self.load_audio, files))\n\n        # Parallel diarization\n        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n            diarization_results = list(executor.map(self.diarize, audio_data))\n\n        # Batch transcription on GPU\n        transcription_results = self.transcribe_batch(audio_data, diarization_results)\n\n        return transcription_results\n</code></pre>"},{"location":"deployment/performance/#model-optimization","title":"Model Optimization","text":""},{"location":"deployment/performance/#model-quantization","title":"Model Quantization","text":"<pre><code># Quantize model for faster inference\ndef quantize_model(model):\n    # Dynamic quantization\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, \n        {torch.nn.Linear, torch.nn.Conv1d}, \n        dtype=torch.qint8\n    )\n    return quantized_model\n</code></pre>"},{"location":"deployment/performance/#onnx-export","title":"ONNX Export","text":"<pre><code># Export to ONNX for optimized inference\ndef export_to_onnx(model, output_path):\n    dummy_input = torch.randn(1, 16000)  # 1 second audio\n\n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        export_params=True,\n        opset_version=14,\n        do_constant_folding=True,\n        input_names=['audio'],\n        output_names=['transcription'],\n        dynamic_axes={'audio': {0: 'batch_size', 1: 'sequence'}}\n    )\n</code></pre>"},{"location":"deployment/performance/#storage-optimization","title":"Storage Optimization","text":""},{"location":"deployment/performance/#output-compression","title":"Output Compression","text":"<pre><code>output:\n  compression:\n    enable: true\n    format: \"gzip\"              # or \"lz4\" for faster\n    level: 6                    # 1-9, higher = smaller\n\n  # Only save required formats\n  formats: [\"json\"]             # Skip txt if not needed\n\n  # Reduce JSON size\n  json:\n    pretty: false               # Minified JSON\n    exclude_words: true         # Skip word-level data\n</code></pre>"},{"location":"deployment/performance/#streaming-output","title":"Streaming Output","text":"<pre><code>class StreamingWriter:\n    def __init__(self, output_file):\n        self.output_file = output_file\n        self.file = open(output_file, 'w')\n        self.file.write('{\"segments\": [')\n        self.first = True\n\n    def write_segment(self, segment):\n        if not self.first:\n            self.file.write(',')\n        self.file.write(json.dumps(segment))\n        self.file.flush()\n        self.first = False\n\n    def close(self):\n        self.file.write(']}')\n        self.file.close()\n</code></pre>"},{"location":"deployment/performance/#network-optimization","title":"Network Optimization","text":""},{"location":"deployment/performance/#model-download-cache","title":"Model Download Cache","text":"<pre><code># Pre-download models\nexport TORCH_HOME=/shared/models\nexport HF_HOME=/shared/models/huggingface\n\n# Share across machines\nmkdir -p /shared/models\nchmod 755 /shared/models\n</code></pre>"},{"location":"deployment/performance/#distributed-processing","title":"Distributed Processing","text":"<pre><code># Process on multiple machines\nfrom multiprocessing.managers import BaseManager\n\nclass DistributedPipeline:\n    def __init__(self, worker_addresses):\n        self.workers = []\n        for addr, port in worker_addresses:\n            manager = BaseManager(address=(addr, port))\n            manager.connect()\n            self.workers.append(manager)\n\n    def process_files(self, files):\n        # Distribute files to workers\n        chunk_size = len(files) // len(self.workers)\n        futures = []\n\n        for i, worker in enumerate(self.workers):\n            start = i * chunk_size\n            end = start + chunk_size if i &lt; len(self.workers)-1 else len(files)\n            chunk = files[start:end]\n\n            future = worker.process_async(chunk)\n            futures.append(future)\n\n        # Collect results\n        results = []\n        for future in futures:\n            results.extend(future.get())\n\n        return results\n</code></pre>"},{"location":"deployment/performance/#monitoring-and-profiling","title":"Monitoring and Profiling","text":""},{"location":"deployment/performance/#performance-metrics","title":"Performance Metrics","text":"<pre><code>import psutil\nimport GPUtil\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.start_time = time.time()\n        self.start_cpu = psutil.cpu_percent()\n        self.start_memory = psutil.virtual_memory().percent\n\n    def log_metrics(self):\n        # CPU metrics\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n\n        # GPU metrics\n        gpus = GPUtil.getGPUs()\n        if gpus:\n            gpu = gpus[0]\n            gpu_util = gpu.load * 100\n            gpu_memory = gpu.memoryUtil * 100\n        else:\n            gpu_util = gpu_memory = 0\n\n        metrics = {\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_gb\": memory.used / (1024**3),\n            \"gpu_util\": gpu_util,\n            \"gpu_memory\": gpu_memory,\n            \"elapsed_time\": time.time() - self.start_time\n        }\n\n        return metrics\n</code></pre>"},{"location":"deployment/performance/#profiling-tools","title":"Profiling Tools","text":"<pre><code># Profile with cProfile\npython -m cProfile -o profile.stats main.py\n\n# Analyze profile\npython -m pstats profile.stats\n\n# GPU profiling with nvprof\nnvprof python main.py\n\n# PyTorch profiler\npython -m torch.profiler main.py\n</code></pre>"},{"location":"deployment/performance/#best-practices","title":"Best Practices","text":""},{"location":"deployment/performance/#configuration-templates","title":"Configuration Templates","text":""},{"location":"deployment/performance/#high-throughput","title":"High Throughput","text":"<pre><code># config/high_throughput.yaml\nmodel:\n  name: \"stt_en_conformer_ctc_small\"\n\ntranscription:\n  batch_size: 32\n  device: \"cuda\"\n  compute_dtype: \"float16\"\n\ndiarization:\n  enable: false                    # Skip for speed\n\nprocessing:\n  parallel_workers: 8\n\noutput:\n  formats: [\"json\"]                # Minimal output\n  compression:\n    enable: true\n    format: \"lz4\"\n</code></pre>"},{"location":"deployment/performance/#high-accuracy","title":"High Accuracy","text":"<pre><code># config/high_accuracy.yaml\nmodel:\n  name: \"stt_en_conformer_ctc_large\"\n\ntranscription:\n  batch_size: 4\n  device: \"cuda\"\n  compute_dtype: \"float32\"         # Full precision\n\ndiarization:\n  enable: true\n  embedding:\n    window_length: 2.0             # Longer windows\n\noutput:\n  formats: [\"json\", \"txt\", \"attributed_txt\"]\n  include_word_timestamps: true\n</code></pre>"},{"location":"deployment/performance/#optimization-checklist","title":"Optimization Checklist","text":"<ol> <li>Hardware</li> <li> GPU drivers updated</li> <li> CUDA toolkit installed</li> <li> <p> Sufficient RAM/VRAM</p> </li> <li> <p>Configuration</p> </li> <li> Optimal batch size</li> <li> Appropriate model size</li> <li> <p> Mixed precision enabled</p> </li> <li> <p>Pipeline</p> </li> <li> Preprocessing cached</li> <li> Parallel processing enabled</li> <li> <p> Output compression configured</p> </li> <li> <p>Monitoring</p> </li> <li> Resource usage tracked</li> <li> Bottlenecks identified</li> <li> Performance logged</li> </ol>"},{"location":"deployment/performance/#troubleshooting-performance","title":"Troubleshooting Performance","text":""},{"location":"deployment/performance/#slow-processing","title":"Slow Processing","text":"<ol> <li> <p>Check GPU utilization <pre><code>watch -n 1 nvidia-smi\n</code></pre></p> </li> <li> <p>Increase batch size <pre><code>transcription:\n  batch_size: 16  # From 4\n</code></pre></p> </li> <li> <p>Use smaller model <pre><code>--model stt_en_conformer_ctc_small\n</code></pre></p> </li> </ol>"},{"location":"deployment/performance/#out-of-memory","title":"Out of Memory","text":"<ol> <li> <p>Reduce batch size <pre><code>transcription:\n  batch_size: 1\n</code></pre></p> </li> <li> <p>Enable gradient checkpointing <pre><code>model.gradient_checkpointing_enable()\n</code></pre></p> </li> <li> <p>Clear cache <pre><code>torch.cuda.empty_cache()\n</code></pre></p> </li> </ol>"},{"location":"deployment/performance/#high-latency","title":"High Latency","text":"<ol> <li> <p>Enable model caching <pre><code>@lru_cache(maxsize=1)\ndef get_model():\n    return load_model()\n</code></pre></p> </li> <li> <p>Warm up GPU <pre><code># Process dummy input\nmodel.transcribe(torch.zeros(1, 16000))\n</code></pre></p> </li> <li> <p>Use streaming <pre><code>for chunk in stream_process(audio):\n    yield chunk\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to the Audio Transcription Pipeline! This guide will help you get started.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>CUDA-capable GPU (optional, for testing GPU features)</li> <li>Git for version control</li> <li><code>uv</code> package manager</li> </ul>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Fork and Clone <pre><code># Fork the repository on GitHub, then:\ngit clone https://github.com/YOUR_USERNAME/audio_aigented.git\ncd audio_aigented\n</code></pre></p> </li> <li> <p>Create Virtual Environment <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install Development Dependencies <pre><code>uv pip install -e \".[dev,docs]\"\n</code></pre></p> </li> <li> <p>Set Up Pre-commit Hooks <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#branch-strategy","title":"Branch Strategy","text":"<p>We use a simple branch strategy: - <code>main</code> - Stable release branch - <code>develop</code> - Integration branch for features - <code>feature/*</code> - Feature branches - <code>fix/*</code> - Bug fix branches - <code>docs/*</code> - Documentation updates</p>"},{"location":"development/contributing/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a Branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make Your Changes</p> </li> <li>Write clean, documented code</li> <li>Follow the existing code style</li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Run Tests <pre><code>uv run pytest\nuv run pytest --cov=src --cov-report=term-missing\n</code></pre></p> </li> <li> <p>Check Code Quality <pre><code>uv run ruff check .\nuv run ruff format .\nuv run mypy src/\n</code></pre></p> </li> <li> <p>Commit Changes <pre><code>git add .\ngit commit -m \"feat: add amazing new feature\"\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#commit-message-format","title":"Commit Message Format","text":"<p>We follow Conventional Commits:</p> <ul> <li><code>feat:</code> New feature</li> <li><code>fix:</code> Bug fix</li> <li><code>docs:</code> Documentation changes</li> <li><code>style:</code> Code style changes (formatting, etc)</li> <li><code>refactor:</code> Code refactoring</li> <li><code>test:</code> Test additions or changes</li> <li><code>chore:</code> Build process or auxiliary tool changes</li> </ul> <p>Examples: <pre><code>feat: add support for MP3 input files\nfix: correct speaker assignment in diarization\ndocs: update installation guide for Windows\ntest: add integration tests for pipeline\n</code></pre></p>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with these additions: - Maximum line length: 88 characters (Black default) - Use type hints for function signatures - Use Google-style docstrings</p> <p>Example: <pre><code>def process_audio(\n    file_path: Path,\n    sample_rate: int = 16000,\n    normalize: bool = True\n) -&gt; Tuple[np.ndarray, int]:\n    \"\"\"Process audio file for transcription.\n\n    Args:\n        file_path: Path to the audio file\n        sample_rate: Target sample rate in Hz\n        normalize: Whether to normalize audio levels\n\n    Returns:\n        Tuple of (audio_array, actual_sample_rate)\n\n    Raises:\n        AudioLoadError: If file cannot be loaded\n    \"\"\"\n    # Implementation\n</code></pre></p>"},{"location":"development/contributing/#testing-guidelines","title":"Testing Guidelines","text":"<ol> <li>Write Tests First (TDD encouraged)</li> <li>Test File Naming: <code>test_&lt;module_name&gt;.py</code></li> <li> <p>Test Structure:    <pre><code>def test_feature_description():\n    # Arrange\n    input_data = create_test_data()\n\n    # Act\n    result = function_under_test(input_data)\n\n    # Assert\n    assert result.expected_field == expected_value\n</code></pre></p> </li> <li> <p>Use Fixtures:    <pre><code>@pytest.fixture\ndef sample_audio():\n    return np.random.randn(16000)  # 1 second at 16kHz\n</code></pre></p> </li> <li> <p>Mock External Dependencies:    <pre><code>@patch('audio_aigented.transcription.load_model')\ndef test_transcription(mock_model):\n    mock_model.return_value.transcribe.return_value = \"test\"\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#documentation-standards","title":"Documentation Standards","text":"<ol> <li>Docstrings: All public functions, classes, and modules</li> <li>Type Hints: All function parameters and returns</li> <li>Examples: Include usage examples in docstrings</li> <li>User Guides: Update relevant guides for new features</li> </ol>"},{"location":"development/contributing/#project-structure","title":"Project Structure","text":"<pre><code>audio_aigented/\n\u251c\u2500\u2500 src/audio_aigented/      # Source code\n\u2502   \u251c\u2500\u2500 audio/              # Audio processing\n\u2502   \u251c\u2500\u2500 diarization/        # Speaker diarization\n\u2502   \u251c\u2500\u2500 transcription/      # ASR transcription\n\u2502   \u251c\u2500\u2500 formatting/         # Output formatting\n\u2502   \u251c\u2500\u2500 output/            # File writing\n\u2502   \u2514\u2500\u2500 pipeline.py        # Main pipeline\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 docs/                   # Documentation\n\u251c\u2500\u2500 config/                 # Configuration files\n\u2514\u2500\u2500 examples/              # Usage examples\n</code></pre>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_pipeline.py\n\n# Run with coverage\nuv run pytest --cov=src --cov-report=html\n\n# Run specific test\nuv run pytest tests/test_pipeline.py::test_process_directory\n</code></pre>"},{"location":"development/contributing/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests: Test individual functions/classes</li> <li>Integration Tests: Test component interactions</li> <li>End-to-End Tests: Test complete pipeline</li> <li>Performance Tests: Test speed and resource usage</li> </ol>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code># tests/test_audio_loader.py\nimport pytest\nfrom pathlib import Path\nfrom audio_aigented.audio import AudioLoader\n\nclass TestAudioLoader:\n    def test_load_valid_wav(self, sample_wav_file):\n        loader = AudioLoader()\n        audio, sr = loader.load(sample_wav_file)\n\n        assert audio is not None\n        assert sr == 16000\n        assert len(audio) &gt; 0\n\n    def test_load_missing_file(self):\n        loader = AudioLoader()\n\n        with pytest.raises(FileNotFoundError):\n            loader.load(Path(\"nonexistent.wav\"))\n</code></pre>"},{"location":"development/contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"development/contributing/#1-planning","title":"1. Planning","text":"<p>Before implementing: 1. Open an issue to discuss the feature 2. Get feedback from maintainers 3. Consider backward compatibility</p>"},{"location":"development/contributing/#2-implementation-checklist","title":"2. Implementation Checklist","text":"<ul> <li> Create feature branch</li> <li> Write tests first</li> <li> Implement feature</li> <li> Update documentation</li> <li> Add usage examples</li> <li> Update changelog</li> <li> Run full test suite</li> </ul>"},{"location":"development/contributing/#3-example-adding-new-output-format","title":"3. Example: Adding New Output Format","text":"<pre><code># src/audio_aigented/formatting/csv_formatter.py\nfrom .base import BaseFormatter\n\nclass CSVFormatter(BaseFormatter):\n    \"\"\"Format transcription results as CSV.\"\"\"\n\n    def format(self, result: TranscriptionResult) -&gt; str:\n        output = \"speaker,start,end,text,confidence\\n\"\n\n        for segment in result.transcription.segments:\n            output += f\"{segment.speaker_id},\"\n            output += f\"{segment.start_time},\"\n            output += f\"{segment.end_time},\"\n            output += f'\"{segment.text}\",'\n            output += f\"{segment.confidence}\\n\"\n\n        return output\n\n# Register in pipeline\nFORMATTERS['csv'] = CSVFormatter\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Update Your Branch <pre><code>git fetch upstream\ngit rebase upstream/develop\n</code></pre></p> </li> <li> <p>Push Changes <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create Pull Request</p> </li> <li>Use a clear, descriptive title</li> <li>Reference any related issues</li> <li>Include a description of changes</li> <li> <p>Add screenshots for UI changes</p> </li> <li> <p>PR Template <pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- [ ] Tests pass locally\n- [ ] Added new tests\n- [ ] Coverage maintained/improved\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Self-reviewed code\n- [ ] Updated documentation\n- [ ] No new warnings\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#review-process","title":"Review Process","text":""},{"location":"development/contributing/#what-we-look-for","title":"What We Look For","text":"<ol> <li>Code Quality</li> <li>Clean, readable code</li> <li>Appropriate abstractions</li> <li> <p>No code duplication</p> </li> <li> <p>Testing</p> </li> <li>Adequate test coverage</li> <li>Tests pass in CI</li> <li> <p>Edge cases covered</p> </li> <li> <p>Documentation</p> </li> <li>Clear docstrings</li> <li>Updated user guides</li> <li> <p>Changelog entry</p> </li> <li> <p>Performance</p> </li> <li>No performance regressions</li> <li>Efficient algorithms</li> <li>Resource usage considered</li> </ol>"},{"location":"development/contributing/#responding-to-feedback","title":"Responding to Feedback","text":"<ul> <li>Be open to suggestions</li> <li>Ask for clarification if needed</li> <li>Update PR based on feedback</li> <li>Re-request review when ready</li> </ul>"},{"location":"development/contributing/#release-process","title":"Release Process","text":"<ol> <li>Version Numbering: Semantic Versioning (MAJOR.MINOR.PATCH)</li> <li>Changelog: Update CHANGELOG.md</li> <li>Documentation: Ensure docs are current</li> <li>Testing: Full test suite passes</li> <li>Tag Release: Create Git tag</li> <li>Deploy: Update package repositories</li> </ol>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":""},{"location":"development/contributing/#resources","title":"Resources","text":"<ul> <li>Issue Tracker</li> <li>Discussions</li> <li>Documentation</li> </ul>"},{"location":"development/contributing/#communication","title":"Communication","text":"<ul> <li>Issues: Bug reports and feature requests</li> <li>Discussions: General questions and ideas</li> <li>Pull Requests: Code contributions</li> </ul>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"development/contributing/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Accept constructive criticism</li> <li>Focus on what's best for the community</li> </ul>"},{"location":"development/contributing/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<ul> <li>Harassment or discrimination</li> <li>Trolling or insulting comments</li> <li>Public or private harassment</li> <li>Publishing private information</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - CONTRIBUTORS.md file - Release notes - Project documentation</p> <p>Thank you for contributing! \ud83c\udf89</p>"},{"location":"development/structure/","title":"Project Structure","text":"<p>This document describes the organization of the audio transcription pipeline repository.</p>"},{"location":"development/structure/#directory-layout","title":"Directory Layout","text":"<pre><code>audio_aigented/\n\u251c\u2500\u2500 src/audio_aigented/          # Main package source code\n\u2502   \u251c\u2500\u2500 audio/                   # Audio loading and preprocessing\n\u2502   \u251c\u2500\u2500 cache/                   # Caching functionality\n\u2502   \u251c\u2500\u2500 config/                  # Configuration management\n\u2502   \u251c\u2500\u2500 diarization/            # Speaker diarization\n\u2502   \u251c\u2500\u2500 formatting/             # Output formatting\n\u2502   \u251c\u2500\u2500 models/                 # Data models and schemas\n\u2502   \u251c\u2500\u2500 output/                 # Output writing\n\u2502   \u251c\u2500\u2500 transcription/          # ASR transcription\n\u2502   \u251c\u2500\u2500 utils/                  # Utility functions\n\u2502   \u2514\u2500\u2500 pipeline.py             # Main pipeline orchestration\n\u251c\u2500\u2500 tests/                      # Test suite\n\u2502   \u2514\u2500\u2500 test_*.py              # Unit and integration tests\n\u251c\u2500\u2500 config/                     # Configuration files\n\u2502   \u251c\u2500\u2500 default.yaml           # Default configuration\n\u2502   \u2514\u2500\u2500 diarization_config.yaml # Diarization-specific config\n\u251c\u2500\u2500 docs/                       # Documentation\n\u2502   \u251c\u2500\u2500 DOCKER.md              # Docker setup guide\n\u2502   \u251c\u2500\u2500 IMPLEMENTATION_PLAN.md  # Development roadmap\n\u2502   \u251c\u2500\u2500 MODEL_MANAGEMENT.md    # Model handling strategy\n\u2502   \u2514\u2500\u2500 PROJECT_STRUCTURE.md   # This file\n\u251c\u2500\u2500 examples/                   # Example usage scripts\n\u2502   \u2514\u2500\u2500 basic_usage.py         # Basic API usage example\n\u251c\u2500\u2500 scripts/                    # Shell scripts\n\u2502   \u251c\u2500\u2500 run-transcription.sh   # Main execution script\n\u2502   \u251c\u2500\u2500 docker-test.sh         # Docker test script\n\u2502   \u2514\u2500\u2500 docker-entrypoint.sh   # Docker entrypoint\n\u251c\u2500\u2500 inputs/                     # Input audio files (git-ignored)\n\u251c\u2500\u2500 outputs/                    # Output transcriptions (git-ignored)\n\u251c\u2500\u2500 models/                     # Downloaded models (git-ignored)\n\u251c\u2500\u2500 cache/                      # Runtime cache (git-ignored)\n\u251c\u2500\u2500 main.py                     # CLI entry point\n\u251c\u2500\u2500 pyproject.toml             # Python project configuration\n\u251c\u2500\u2500 Dockerfile                 # Docker image definition\n\u251c\u2500\u2500 docker-compose.yml         # Docker orchestration\n\u251c\u2500\u2500 README.md                  # Project overview\n\u2514\u2500\u2500 CLAUDE.md                  # AI assistant instructions\n</code></pre>"},{"location":"development/structure/#key-components","title":"Key Components","text":""},{"location":"development/structure/#source-code-srcaudio_aigented","title":"Source Code (<code>src/audio_aigented/</code>)","text":"<ul> <li>Modular Design: Each module has a single responsibility</li> <li>Type Safety: All modules use Pydantic models for data validation</li> <li>Pipeline Pattern: Central <code>pipeline.py</code> orchestrates all components</li> </ul>"},{"location":"development/structure/#configuration-config","title":"Configuration (<code>config/</code>)","text":"<ul> <li>YAML-based: Human-readable configuration</li> <li>OmegaConf Integration: Supports environment variable overrides</li> <li>Hierarchical: Separate configs for different components</li> </ul>"},{"location":"development/structure/#tests-tests","title":"Tests (<code>tests/</code>)","text":"<ul> <li>Comprehensive Coverage: Unit tests for each module</li> <li>Integration Tests: End-to-end pipeline testing</li> <li>Fixtures: Reusable test data and mocks</li> </ul>"},{"location":"development/structure/#documentation-docs","title":"Documentation (<code>docs/</code>)","text":"<ul> <li>User Guides: Setup and usage instructions</li> <li>Architecture Docs: Technical design decisions</li> <li>API Reference: Generated from docstrings</li> </ul>"},{"location":"development/structure/#scripts-scripts","title":"Scripts (<code>scripts/</code>)","text":"<ul> <li>Automation: Shell scripts for common tasks</li> <li>Docker Support: Container management scripts</li> <li>CI/CD Ready: Structured for automation</li> </ul>"},{"location":"development/structure/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Local Development:    <pre><code>uv pip install -e \".[dev]\"\nuv run pytest\n</code></pre></p> </li> <li> <p>Docker Development:    <pre><code>docker-compose build\n./scripts/docker-test.sh\n</code></pre></p> </li> <li> <p>Adding Features:</p> </li> <li>Create module in appropriate directory</li> <li>Add tests in <code>tests/</code></li> <li>Update configuration if needed</li> <li>Document in module docstrings</li> </ol>"},{"location":"development/structure/#best-practices","title":"Best Practices","text":"<ol> <li>Code Organization:</li> <li>One class/function per file for major components</li> <li>Group related functionality in modules</li> <li> <p>Use <code>__init__.py</code> for clean imports</p> </li> <li> <p>Testing:</p> </li> <li>Write tests alongside implementation</li> <li>Use fixtures for test data</li> <li> <p>Mock external dependencies</p> </li> <li> <p>Documentation:</p> </li> <li>Docstrings for all public APIs</li> <li>README files in complex directories</li> <li> <p>Examples for common use cases</p> </li> <li> <p>Version Control:</p> </li> <li><code>.gitignore</code> excludes generated/large files</li> <li><code>.gitkeep</code> preserves empty directories</li> <li>Atomic commits with clear messages</li> </ol>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers the testing strategy, tools, and best practices for the Audio Transcription Pipeline.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>We follow these principles: - Test-Driven Development (TDD): Write tests first when possible - Comprehensive Coverage: Aim for &gt;80% code coverage - Fast Feedback: Tests should run quickly - Isolation: Tests should not depend on external services - Clarity: Tests should document behavior</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Unit tests for individual components\n\u2502   \u251c\u2500\u2500 test_audio_loader.py\n\u2502   \u251c\u2500\u2500 test_diarizer.py\n\u2502   \u2514\u2500\u2500 test_transcriber.py\n\u251c\u2500\u2500 integration/             # Integration tests\n\u2502   \u251c\u2500\u2500 test_pipeline.py\n\u2502   \u2514\u2500\u2500 test_output_formats.py\n\u251c\u2500\u2500 fixtures/               # Test data and fixtures\n\u2502   \u251c\u2500\u2500 audio/\n\u2502   \u2514\u2500\u2500 config/\n\u251c\u2500\u2500 conftest.py            # Shared pytest configuration\n\u2514\u2500\u2500 test_e2e.py           # End-to-end tests\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-commands","title":"Basic Commands","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific test file\nuv run pytest tests/test_audio_loader.py\n\n# Run specific test\nuv run pytest tests/test_audio_loader.py::test_load_wav_file\n\n# Run tests matching pattern\nuv run pytest -k \"test_diarization\"\n</code></pre>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Generate coverage report\nuv run pytest --cov=src --cov-report=term-missing\n\n# Generate HTML coverage report\nuv run pytest --cov=src --cov-report=html\n# Open htmlcov/index.html in browser\n\n# Coverage with branch coverage\nuv run pytest --cov=src --cov-branch --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#test-markers","title":"Test Markers","text":"<pre><code># Run only fast tests\nuv run pytest -m \"not slow\"\n\n# Run only GPU tests\nuv run pytest -m \"gpu\"\n\n# Run integration tests\nuv run pytest -m \"integration\"\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests focus on individual functions or classes:</p> <pre><code># tests/unit/test_audio_loader.py\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom audio_aigented.audio.loader import AudioLoader, AudioLoadError\n\nclass TestAudioLoader:\n    \"\"\"Test AudioLoader functionality.\"\"\"\n\n    def test_load_valid_wav(self, sample_wav_path):\n        \"\"\"Test loading a valid WAV file.\"\"\"\n        loader = AudioLoader()\n        audio, sample_rate = loader.load(sample_wav_path)\n\n        assert isinstance(audio, np.ndarray)\n        assert sample_rate == 16000\n        assert len(audio) &gt; 0\n\n    def test_load_missing_file(self):\n        \"\"\"Test error handling for missing files.\"\"\"\n        loader = AudioLoader()\n\n        with pytest.raises(AudioLoadError) as exc_info:\n            loader.load(Path(\"nonexistent.wav\"))\n\n        assert \"not found\" in str(exc_info.value)\n\n    def test_resample_audio(self):\n        \"\"\"Test audio resampling.\"\"\"\n        loader = AudioLoader(target_sample_rate=16000)\n\n        # Create 8kHz audio\n        audio_8k = np.random.randn(8000)\n\n        # Resample to 16kHz\n        audio_16k = loader.resample(audio_8k, 8000, 16000)\n\n        assert len(audio_16k) == 16000\n\n    @pytest.mark.parametrize(\"sample_rate,expected\", [\n        (8000, 16000),\n        (22050, 16000),\n        (44100, 16000),\n        (48000, 16000),\n    ])\n    def test_various_sample_rates(self, sample_rate, expected):\n        \"\"\"Test resampling from various sample rates.\"\"\"\n        loader = AudioLoader(target_sample_rate=expected)\n        audio = np.random.randn(sample_rate)\n\n        resampled = loader.resample(audio, sample_rate, expected)\n\n        assert len(resampled) == expected\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify component interactions:</p> <pre><code># tests/integration/test_pipeline.py\nimport pytest\nfrom pathlib import Path\nfrom audio_aigented.pipeline import TranscriptionPipeline\nfrom audio_aigented.config import PipelineConfig\n\nclass TestPipeline:\n    \"\"\"Test complete pipeline integration.\"\"\"\n\n    def test_process_single_file(self, sample_audio_file, tmp_path):\n        \"\"\"Test processing a single audio file.\"\"\"\n        config = PipelineConfig(\n            output_dir=tmp_path,\n            enable_diarization=True\n        )\n\n        pipeline = TranscriptionPipeline(config)\n        result = pipeline.process_single_file(sample_audio_file)\n\n        assert result.success\n        assert result.transcription.full_text != \"\"\n        assert len(result.transcription.segments) &gt; 0\n\n        # Check output files\n        output_dir = tmp_path / sample_audio_file.stem\n        assert (output_dir / \"transcript.json\").exists()\n        assert (output_dir / \"transcript.txt\").exists()\n\n    @pytest.mark.slow\n    def test_process_directory(self, audio_directory, tmp_path):\n        \"\"\"Test batch processing of directory.\"\"\"\n        config = PipelineConfig(output_dir=tmp_path)\n        pipeline = TranscriptionPipeline(config)\n\n        results = pipeline.process_directory(audio_directory)\n\n        assert len(results) &gt; 0\n        assert all(r.success for r in results)\n\n    def test_pipeline_with_diarization_disabled(self, sample_audio_file):\n        \"\"\"Test pipeline without speaker diarization.\"\"\"\n        config = PipelineConfig(enable_diarization=False)\n        pipeline = TranscriptionPipeline(config)\n\n        result = pipeline.process_single_file(sample_audio_file)\n\n        # All segments should have same speaker\n        speakers = {s.speaker_id for s in result.transcription.segments}\n        assert speakers == {\"SPEAKER_00\"}\n</code></pre>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<p>E2E tests verify the complete system:</p> <pre><code># tests/test_e2e.py\nimport pytest\nimport subprocess\nfrom pathlib import Path\n\n@pytest.mark.e2e\nclass TestEndToEnd:\n    \"\"\"End-to-end tests using CLI.\"\"\"\n\n    def test_cli_basic_transcription(self, sample_wav_file, tmp_path):\n        \"\"\"Test basic CLI transcription.\"\"\"\n        cmd = [\n            \"python\", \"main.py\",\n            \"--input-dir\", str(sample_wav_file.parent),\n            \"--output-dir\", str(tmp_path),\n            \"--device\", \"cpu\"  # Use CPU for CI\n        ]\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n\n        assert result.returncode == 0\n        assert \"Processing complete\" in result.stdout\n\n        # Check outputs\n        output_files = list(tmp_path.glob(\"**/*.json\"))\n        assert len(output_files) &gt; 0\n\n    def test_cli_with_config(self, config_file, audio_dir, tmp_path):\n        \"\"\"Test CLI with custom configuration.\"\"\"\n        cmd = [\n            \"python\", \"main.py\",\n            \"--config\", str(config_file),\n            \"--output-dir\", str(tmp_path)\n        ]\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n\n        assert result.returncode == 0\n</code></pre>"},{"location":"development/testing/#fixtures","title":"Fixtures","text":""},{"location":"development/testing/#common-fixtures","title":"Common Fixtures","text":"<pre><code># tests/conftest.py\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nimport soundfile as sf\n\n@pytest.fixture\ndef sample_audio_data():\n    \"\"\"Generate sample audio data.\"\"\"\n    duration = 5  # seconds\n    sample_rate = 16000\n    frequency = 440  # A4 note\n\n    t = np.linspace(0, duration, duration * sample_rate)\n    audio = np.sin(2 * np.pi * frequency * t)\n\n    return audio, sample_rate\n\n@pytest.fixture\ndef sample_wav_file(tmp_path, sample_audio_data):\n    \"\"\"Create a temporary WAV file.\"\"\"\n    audio, sample_rate = sample_audio_data\n\n    wav_path = tmp_path / \"test_audio.wav\"\n    sf.write(wav_path, audio, sample_rate)\n\n    return wav_path\n\n@pytest.fixture\ndef mock_transcription_result():\n    \"\"\"Mock transcription result.\"\"\"\n    return {\n        \"segments\": [\n            {\n                \"text\": \"Hello world\",\n                \"start_time\": 0.0,\n                \"end_time\": 1.5,\n                \"speaker_id\": \"SPEAKER_00\",\n                \"confidence\": 0.95\n            },\n            {\n                \"text\": \"How are you\",\n                \"start_time\": 2.0,\n                \"end_time\": 3.5,\n                \"speaker_id\": \"SPEAKER_01\",\n                \"confidence\": 0.92\n            }\n        ]\n    }\n\n@pytest.fixture\ndef pipeline_config(tmp_path):\n    \"\"\"Create test pipeline configuration.\"\"\"\n    return {\n        \"input_dir\": str(tmp_path / \"inputs\"),\n        \"output_dir\": str(tmp_path / \"outputs\"),\n        \"model\": {\n            \"name\": \"stt_en_conformer_ctc_small\"\n        },\n        \"device\": \"cpu\",\n        \"enable_diarization\": False\n    }\n</code></pre>"},{"location":"development/testing/#audio-test-data","title":"Audio Test Data","text":"<pre><code># tests/fixtures/audio_generator.py\nimport numpy as np\nimport soundfile as sf\n\ndef create_test_audio(\n    duration: float = 5.0,\n    sample_rate: int = 16000,\n    num_speakers: int = 1\n) -&gt; np.ndarray:\n    \"\"\"Create synthetic audio for testing.\"\"\"\n    samples = int(duration * sample_rate)\n\n    if num_speakers == 1:\n        # Simple sine wave\n        frequency = 440\n        t = np.linspace(0, duration, samples)\n        audio = np.sin(2 * np.pi * frequency * t)\n    else:\n        # Multiple frequencies for different \"speakers\"\n        audio = np.zeros(samples)\n        segment_length = samples // num_speakers\n\n        for i in range(num_speakers):\n            start = i * segment_length\n            end = start + segment_length\n            frequency = 440 * (i + 1)\n            t = np.linspace(0, segment_length / sample_rate, segment_length)\n            audio[start:end] = np.sin(2 * np.pi * frequency * t)\n\n    return audio\n\ndef create_speech_like_audio(\n    duration: float = 5.0,\n    sample_rate: int = 16000\n) -&gt; np.ndarray:\n    \"\"\"Create audio that resembles speech patterns.\"\"\"\n    samples = int(duration * sample_rate)\n\n    # Combine multiple frequencies (formants)\n    f1, f2, f3 = 700, 1220, 2600  # Typical formants\n    t = np.linspace(0, duration, samples)\n\n    audio = (\n        0.5 * np.sin(2 * np.pi * f1 * t) +\n        0.3 * np.sin(2 * np.pi * f2 * t) +\n        0.2 * np.sin(2 * np.pi * f3 * t)\n    )\n\n    # Add envelope to simulate speech patterns\n    envelope = np.concatenate([\n        np.linspace(0, 1, samples // 4),\n        np.ones(samples // 2),\n        np.linspace(1, 0, samples // 4)\n    ])\n\n    return audio * envelope\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":""},{"location":"development/testing/#mock-external-dependencies","title":"Mock External Dependencies","text":"<pre><code># tests/unit/test_transcriber.py\nfrom unittest.mock import Mock, patch\nimport pytest\n\nclass TestTranscriber:\n    @patch('audio_aigented.transcription.asr.EncDecCTCModel')\n    def test_transcribe_with_mock_model(self, mock_model_class):\n        \"\"\"Test transcription with mocked NeMo model.\"\"\"\n        # Setup mock\n        mock_model = Mock()\n        mock_model.transcribe.return_value = [\"Hello world\"]\n        mock_model_class.from_pretrained.return_value = mock_model\n\n        # Test\n        transcriber = Transcriber()\n        result = transcriber.transcribe(np.zeros(16000))\n\n        assert result.text == \"Hello world\"\n        mock_model.transcribe.assert_called_once()\n\n    @patch('torch.cuda.is_available')\n    def test_device_fallback(self, mock_cuda):\n        \"\"\"Test fallback to CPU when GPU unavailable.\"\"\"\n        mock_cuda.return_value = False\n\n        transcriber = Transcriber(device=\"cuda\")\n\n        assert transcriber.device == \"cpu\"\n</code></pre>"},{"location":"development/testing/#mock-file-system","title":"Mock File System","text":"<pre><code># tests/unit/test_output_writer.py\nfrom unittest.mock import mock_open, patch\n\nclass TestOutputWriter:\n    @patch('builtins.open', new_callable=mock_open)\n    def test_write_json(self, mock_file):\n        \"\"\"Test JSON file writing.\"\"\"\n        writer = OutputWriter()\n        data = {\"test\": \"data\"}\n\n        writer.write_json(Path(\"test.json\"), data)\n\n        mock_file.assert_called_once_with(Path(\"test.json\"), 'w')\n        handle = mock_file()\n        written_data = ''.join(call.args[0] for call in handle.write.call_args_list)\n\n        assert json.loads(written_data) == data\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#benchmarking","title":"Benchmarking","text":"<pre><code># tests/performance/test_benchmarks.py\nimport pytest\nimport time\n\nclass TestPerformance:\n    @pytest.mark.benchmark\n    def test_transcription_speed(self, benchmark, sample_audio):\n        \"\"\"Benchmark transcription speed.\"\"\"\n        transcriber = Transcriber()\n\n        def transcribe():\n            return transcriber.transcribe(sample_audio)\n\n        result = benchmark(transcribe)\n\n        # Assert performance requirements\n        assert benchmark.stats['mean'] &lt; 1.0  # Less than 1 second\n\n    def test_memory_usage(self, sample_audio):\n        \"\"\"Test memory usage during transcription.\"\"\"\n        import tracemalloc\n\n        tracemalloc.start()\n\n        transcriber = Transcriber()\n        result = transcriber.transcribe(sample_audio)\n\n        current, peak = tracemalloc.get_traced_memory()\n        tracemalloc.stop()\n\n        # Assert memory usage is reasonable\n        assert peak &lt; 1024 * 1024 * 1024  # Less than 1GB\n</code></pre>"},{"location":"development/testing/#load-testing","title":"Load Testing","text":"<pre><code># tests/performance/test_load.py\nimport concurrent.futures\nimport pytest\n\nclass TestLoad:\n    @pytest.mark.slow\n    def test_concurrent_processing(self, audio_files):\n        \"\"\"Test concurrent file processing.\"\"\"\n        pipeline = TranscriptionPipeline()\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n            futures = [\n                executor.submit(pipeline.process_single_file, f)\n                for f in audio_files\n            ]\n\n            results = [f.result() for f in futures]\n\n        assert all(r.success for r in results)\n        assert len(results) == len(audio_files)\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":""},{"location":"development/testing/#creating-test-data","title":"Creating Test Data","text":"<pre><code># scripts/create_test_data.py\n\"\"\"Script to create test audio files.\"\"\"\n\nimport numpy as np\nimport soundfile as sf\nfrom pathlib import Path\n\ndef create_test_dataset():\n    test_dir = Path(\"tests/fixtures/audio\")\n    test_dir.mkdir(exist_ok=True)\n\n    # Short audio (1 second)\n    short_audio = create_speech_like_audio(1.0)\n    sf.write(test_dir / \"short.wav\", short_audio, 16000)\n\n    # Medium audio (30 seconds)\n    medium_audio = create_speech_like_audio(30.0)\n    sf.write(test_dir / \"medium.wav\", medium_audio, 16000)\n\n    # Multi-speaker simulation\n    multi_speaker = create_test_audio(60.0, num_speakers=3)\n    sf.write(test_dir / \"multi_speaker.wav\", multi_speaker, 16000)\n\n    print(f\"Created test data in {test_dir}\")\n\nif __name__ == \"__main__\":\n    create_test_dataset()\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-configuration","title":"GitHub Actions Configuration","text":"<pre><code># .github/workflows/tests.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.10, 3.11, 3.12]\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        pip install uv\n        uv pip install -e \".[dev]\"\n\n    - name: Run tests\n      run: |\n        uv run pytest --cov=src --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n</code></pre>"},{"location":"development/testing/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"development/testing/#1-test-naming","title":"1. Test Naming","text":"<pre><code># Good test names\ndef test_load_wav_file_returns_audio_and_sample_rate():\n    pass\n\ndef test_missing_file_raises_file_not_found_error():\n    pass\n\ndef test_diarization_identifies_two_speakers():\n    pass\n</code></pre>"},{"location":"development/testing/#2-test-organization","title":"2. Test Organization","text":"<pre><code>class TestAudioLoader:\n    \"\"\"Group related tests in classes.\"\"\"\n\n    class TestLoadingFiles:\n        def test_wav_file(self):\n            pass\n\n        def test_missing_file(self):\n            pass\n\n    class TestPreprocessing:\n        def test_resampling(self):\n            pass\n\n        def test_normalization(self):\n            pass\n</code></pre>"},{"location":"development/testing/#3-assertion-messages","title":"3. Assertion Messages","text":"<pre><code>def test_audio_duration():\n    audio, sr = load_audio(\"test.wav\")\n    expected_duration = 5.0\n    actual_duration = len(audio) / sr\n\n    assert actual_duration == pytest.approx(expected_duration, rel=0.01), \\\n        f\"Expected {expected_duration}s, got {actual_duration}s\"\n</code></pre>"},{"location":"development/testing/#4-parametrized-tests","title":"4. Parametrized Tests","text":"<pre><code>@pytest.mark.parametrize(\"model_name,expected_speed\", [\n    (\"stt_en_conformer_ctc_small\", 15.0),\n    (\"stt_en_conformer_ctc_medium\", 8.0),\n    (\"stt_en_conformer_ctc_large\", 4.0),\n])\ndef test_model_performance(model_name, expected_speed):\n    \"\"\"Test different model speeds.\"\"\"\n    config = {\"model\": {\"name\": model_name}}\n    pipeline = TranscriptionPipeline(config)\n\n    # Test performance\n    actual_speed = measure_speed(pipeline)\n\n    assert actual_speed &gt;= expected_speed * 0.8  # Allow 20% variance\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#using-pytest-debugger","title":"Using pytest debugger","text":"<pre><code># Drop into debugger on failure\nuv run pytest --pdb\n\n# Drop into debugger at start of test\nuv run pytest --trace\n</code></pre>"},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Show print statements\nuv run pytest -s\n\n# Show detailed test progress\nuv run pytest -vv\n\n# Show local variables on failure\nuv run pytest -l\n</code></pre>"},{"location":"development/testing/#test-isolation","title":"Test Isolation","text":"<pre><code># Ensure tests don't affect each other\n@pytest.fixture(autouse=True)\ndef reset_environment():\n    \"\"\"Reset environment after each test.\"\"\"\n    yield\n    # Cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>The Audio Transcription Pipeline uses a flexible YAML-based configuration system with sensible defaults.</p>"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/configuration/#default-configuration","title":"Default Configuration","text":"<p>The default configuration is located at <code>config/default.yaml</code>:</p> <pre><code># Input/Output paths\ninput_dir: \"./inputs\"\noutput_dir: \"./outputs\"\n\n# Audio processing settings\naudio:\n  sample_rate: 16000\n  mono: true\n\n# ASR model configuration  \nmodel:\n  name: \"stt_en_conformer_ctc_large\"\n\n# Transcription settings\ntranscription:\n  device: \"cuda\"  # or \"cpu\"\n  batch_size: 4\n\n# Diarization settings\ndiarization:\n  enable: true\n  embedding:\n    model_path: \"titanet-l\"\n  clustering:\n    parameters:\n      oracle_num_speakers: null  # Auto-detect\n      max_num_speakers: 8\n\n# Processing options\nprocessing:\n  enable_caching: true\n  parallel_workers: 4\n  log_level: \"INFO\"\n\n# Output settings\noutput:\n  formats: [\"json\", \"txt\", \"attributed_txt\"]\n  include_timestamps: true\n  include_confidence: true\n  pretty_json: true\n</code></pre>"},{"location":"getting-started/configuration/#custom-configuration","title":"Custom Configuration","text":"<p>Create custom configurations for different use cases:</p> <pre><code># config/fast_processing.yaml\nmodel:\n  name: \"stt_en_conformer_ctc_small\"\n\ntranscription:\n  batch_size: 8\n\ndiarization:\n  enable: false  # Disable for speed\n\noutput:\n  formats: [\"txt\"]  # Minimal output\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"getting-started/configuration/#audio-settings","title":"Audio Settings","text":"Option Type Default Description <code>audio.sample_rate</code> int 16000 Target sample rate in Hz <code>audio.mono</code> bool true Convert to mono channel <code>audio.max_duration</code> float null Maximum audio duration in seconds"},{"location":"getting-started/configuration/#model-configuration","title":"Model Configuration","text":"Option Type Default Description <code>model.name</code> string \"stt_en_conformer_ctc_large\" NeMo model name <code>model.cache_dir</code> string \"~/.cache/torch/NeMo\" Model cache directory <p>Available models: - <code>stt_en_conformer_ctc_small</code> - Fast, lower accuracy - <code>stt_en_conformer_ctc_medium</code> - Balanced - <code>stt_en_conformer_ctc_large</code> - Slow, high accuracy</p>"},{"location":"getting-started/configuration/#transcription-settings","title":"Transcription Settings","text":"Option Type Default Description <code>transcription.device</code> string \"cuda\" Device for processing (\"cuda\" or \"cpu\") <code>transcription.batch_size</code> int 4 Batch size for processing <code>transcription.num_workers</code> int 0 DataLoader workers"},{"location":"getting-started/configuration/#diarization-settings","title":"Diarization Settings","text":"Option Type Default Description <code>diarization.enable</code> bool true Enable speaker diarization <code>diarization.embedding.model_path</code> string \"titanet-l\" Speaker embedding model <code>diarization.clustering.parameters.oracle_num_speakers</code> int/null null Known number of speakers <code>diarization.clustering.parameters.max_num_speakers</code> int 8 Maximum speakers to detect <code>diarization.clustering.parameters.enhanced_count_thresholding</code> bool true Enhanced speaker detection <code>diarization.clustering.parameters.maj_vote_spk_count</code> bool false Majority vote for speaker count"},{"location":"getting-started/configuration/#processing-options","title":"Processing Options","text":"Option Type Default Description <code>processing.enable_caching</code> bool true Cache intermediate results <code>processing.parallel_workers</code> int 4 Parallel processing workers <code>processing.log_level</code> string \"INFO\" Logging level <code>processing.skip_existing</code> bool true Skip already processed files"},{"location":"getting-started/configuration/#output-settings","title":"Output Settings","text":"Option Type Default Description <code>output.formats</code> list [\"json\", \"txt\", \"attributed_txt\"] Output file formats <code>output.include_timestamps</code> bool true Include timestamps in output <code>output.include_confidence</code> bool true Include confidence scores <code>output.pretty_json</code> bool true Format JSON with indentation <code>output.segment_separator</code> string \"\\n\\n\" Separator between segments"},{"location":"getting-started/configuration/#using-configuration","title":"Using Configuration","text":""},{"location":"getting-started/configuration/#command-line","title":"Command Line","text":"<pre><code># Use custom config file\nuv run python main.py --config config/my_config.yaml\n\n# Override specific options\nuv run python main.py --input-dir /path/to/audio --device cpu\n\n# Combine config file with overrides\nuv run python main.py --config config/base.yaml --disable-diarization\n</code></pre>"},{"location":"getting-started/configuration/#python-api","title":"Python API","text":"<pre><code>from audio_aigented.config import PipelineConfig\nfrom audio_aigented.pipeline import TranscriptionPipeline\n\n# Load from file\nconfig = PipelineConfig.from_yaml(\"config/custom.yaml\")\n\n# Create programmatically\nconfig = PipelineConfig(\n    device=\"cuda\",\n    model_name=\"stt_en_conformer_ctc_medium\",\n    enable_diarization=True,\n    output_formats=[\"json\", \"attributed_txt\"]\n)\n\n# Initialize pipeline\npipeline = TranscriptionPipeline(config=config)\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Override configuration with environment variables:</p> <pre><code># Override device\nexport AUDIO_TRANSCRIPTION_DEVICE=cpu\n\n# Override model\nexport AUDIO_TRANSCRIPTION_MODEL=stt_en_conformer_ctc_small\n\n# Run with overrides\nuv run python main.py\n</code></pre>"},{"location":"getting-started/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/configuration/#high-speed-processing","title":"High-Speed Processing","text":"<pre><code># config/speed.yaml\nmodel:\n  name: \"stt_en_conformer_ctc_small\"\n\ntranscription:\n  batch_size: 16\n\ndiarization:\n  enable: false\n\nprocessing:\n  parallel_workers: 8\n\noutput:\n  formats: [\"txt\"]\n  include_confidence: false\n</code></pre>"},{"location":"getting-started/configuration/#high-accuracy-processing","title":"High-Accuracy Processing","text":"<pre><code># config/accuracy.yaml\nmodel:\n  name: \"stt_en_conformer_ctc_large\"\n\ntranscription:\n  batch_size: 1\n\ndiarization:\n  enable: true\n  clustering:\n    parameters:\n      enhanced_count_thresholding: true\n      maj_vote_spk_count: true\n\noutput:\n  formats: [\"json\", \"txt\", \"attributed_txt\"]\n  include_confidence: true\n  include_timestamps: true\n</code></pre>"},{"location":"getting-started/configuration/#meeting-transcription","title":"Meeting Transcription","text":"<pre><code># config/meetings.yaml\ndiarization:\n  enable: true\n  clustering:\n    parameters:\n      max_num_speakers: 12\n      enhanced_count_thresholding: true\n\noutput:\n  formats: [\"json\", \"attributed_txt\"]\n  segment_separator: \"\\n---\\n\"\n</code></pre>"},{"location":"getting-started/configuration/#podcast-processing","title":"Podcast Processing","text":"<pre><code># config/podcast.yaml\ndiarization:\n  enable: true\n  clustering:\n    parameters:\n      oracle_num_speakers: 2  # Host and guest\n\noutput:\n  formats: [\"txt\", \"attributed_txt\"]\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/configuration/#multi-stage-processing","title":"Multi-Stage Processing","text":"<pre><code># config/multi_stage.yaml\nstages:\n  - name: \"diarization\"\n    config:\n      enable: true\n\n  - name: \"transcription\"\n    config:\n      model:\n        name: \"stt_en_conformer_ctc_large\"\n\n  - name: \"post_processing\"\n    config:\n      punctuation: true\n      capitalization: true\n</code></pre>"},{"location":"getting-started/configuration/#resource-limits","title":"Resource Limits","text":"<pre><code># config/resource_limited.yaml\ntranscription:\n  device: \"cuda\"\n  batch_size: 1\n\nprocessing:\n  max_memory_gb: 8\n  max_gpu_memory_gb: 4\n\ndiarization:\n  clustering:\n    backend: \"cpu\"  # Use CPU for clustering\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Start with Defaults: The default configuration works well for most use cases</li> <li>Profile First: Test with small batches to find optimal settings</li> <li>GPU Memory: Reduce batch size if encountering OOM errors</li> <li>Model Selection: Balance speed vs accuracy for your use case</li> <li>Output Formats: Only generate formats you need to save processing time</li> </ol>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#configuration-not-loading","title":"Configuration Not Loading","text":"<pre><code># Validate configuration\nuv run python -c \"from audio_aigented.config import PipelineConfig; PipelineConfig.from_yaml('config/my_config.yaml')\"\n</code></pre>"},{"location":"getting-started/configuration/#override-priority","title":"Override Priority","text":"<p>Configuration priority (highest to lowest): 1. Command line arguments 2. Environment variables 3. Custom config file 4. Default config file</p>"},{"location":"getting-started/configuration/#common-issues","title":"Common Issues","text":"<p>Invalid YAML syntax <pre><code># Bad - missing quotes\nmodel:\n  name: stt_en_conformer_ctc_large\n\n# Good\nmodel:\n  name: \"stt_en_conformer_ctc_large\"\n</code></pre></p> <p>Type mismatches <pre><code># Bad - string instead of int\ntranscription:\n  batch_size: \"4\"\n\n# Good\ntranscription:\n  batch_size: 4\n</code></pre></p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up the Audio Transcription Pipeline on your system.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#hardware","title":"Hardware","text":"<ul> <li>GPU: NVIDIA GPU with 4GB+ VRAM (8GB+ recommended)</li> <li>Tested on: RTX 3090, RTX 4090, RTX Titan, A100</li> <li>RAM: 16GB minimum (32GB recommended)</li> <li>Storage: ~3GB for models + space for audio files</li> </ul>"},{"location":"getting-started/installation/#software","title":"Software","text":"<ul> <li>Operating System: Linux (Ubuntu 20.04+), Windows 10/11, macOS</li> <li>Python: 3.10 or higher</li> <li>CUDA: 11.8 or higher (for GPU acceleration)</li> <li>NVIDIA Drivers: 520.61.05 or newer</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-using-uv-recommended","title":"Method 1: Using uv (Recommended)","text":"<p>First, install the <code>uv</code> package manager:</p> <pre><code># On macOS and Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Or using pipx\npipx install uv\n</code></pre> <p>Then install the package:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/audio_aigented.git\ncd audio_aigented\n\n# Install the package\nuv pip install -e .\n\n# For development (includes testing and docs tools)\nuv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"getting-started/installation/#method-2-using-pip","title":"Method 2: Using pip","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/audio_aigented.git\ncd audio_aigented\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install the package\npip install -e .\n\n# For development\npip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-using-docker","title":"Method 3: Using Docker","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/audio_aigented.git\ncd audio_aigented\n\n# Build the Docker image\ndocker-compose build\n\n# Run the container\ndocker-compose run --rm audio-transcription\n</code></pre>"},{"location":"getting-started/installation/#nvidia-setup","title":"NVIDIA Setup","text":""},{"location":"getting-started/installation/#installing-cuda-toolkit","title":"Installing CUDA Toolkit","text":"<ol> <li> <p>Check your GPU compatibility:    <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Install CUDA Toolkit 11.8+:    <pre><code># Ubuntu/Debian\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\nsudo apt-get update\nsudo apt-get install cuda-11-8\n</code></pre></p> </li> <li> <p>Add to PATH:    <pre><code>echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#installing-pytorch-with-cuda","title":"Installing PyTorch with CUDA","text":"<p>The package dependencies will automatically install the correct PyTorch version, but you can verify:</p> <pre><code>python -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"getting-started/installation/#model-downloads","title":"Model Downloads","text":"<p>Models are automatically downloaded on first use. To pre-download:</p> <pre><code>from nemo.collections.asr.models import EncDecCTCModel\nfrom nemo.collections.asr.models.classification_models import EncDecClassificationModel\n\n# Download ASR model\nasr_model = EncDecCTCModel.from_pretrained(\"stt_en_conformer_ctc_large\")\n\n# Download VAD model\nvad_model = EncDecClassificationModel.from_pretrained(\"vad_multilingual_marblenet\")\n</code></pre> <p>Models are cached in: - Linux/macOS: <code>~/.cache/torch/NeMo/</code> - Windows: <code>%USERPROFILE%\\.cache\\torch\\NeMo\\</code></p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify everything is working:</p> <pre><code># Check installation\npython -c \"import audio_aigented; print('Package installed successfully')\"\n\n# Run tests\nuv run pytest tests/test_models.py -v\n\n# Process a test file\nuv run python main.py --help\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>ImportError: No module named 'nemo' - Solution: Install NeMo toolkit   <pre><code>uv pip install \"nemo-toolkit[asr]\"\n</code></pre></p> <p>CUDA out of memory - Reduce batch size in configuration - Use a smaller model - Switch to CPU processing</p> <p>No GPU detected - Check NVIDIA drivers: <code>nvidia-smi</code> - Verify CUDA installation: <code>nvcc --version</code> - Ensure PyTorch CUDA version matches system CUDA</p> <p>Permission denied errors - Ensure write permissions to output directory - Check model cache directory permissions</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Continue to Quick Start to process your first audio file</li> <li>See Configuration for customization options</li> <li>Check Docker Setup for containerized deployment</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with the Audio Transcription Pipeline in minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have: - \u2705 Installed the package (Installation Guide) - \u2705 NVIDIA GPU with CUDA support (or CPU for slower processing) - \u2705 Some <code>.wav</code> audio files to transcribe</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-process-your-first-audio-file","title":"1. Process Your First Audio File","text":"<p>Place your audio files in the <code>inputs/</code> directory:</p> <pre><code>mkdir -p inputs\ncp your_audio.wav inputs/\n</code></pre> <p>Run the transcription:</p> <pre><code>uv run python main.py --input-dir ./inputs\n</code></pre> <p>The pipeline will: 1. Load and validate your audio files 2. Perform speaker diarization (identify who speaks when) 3. Transcribe each speaker segment 4. Generate output files in multiple formats</p>"},{"location":"getting-started/quickstart/#2-check-the-results","title":"2. Check the Results","text":"<p>Your transcriptions will be in the <code>outputs/</code> directory:</p> <pre><code>outputs/\n\u2514\u2500\u2500 your_audio/\n    \u251c\u2500\u2500 transcript.json              # Structured data with timestamps\n    \u251c\u2500\u2500 transcript.txt               # Human-readable format\n    \u2514\u2500\u2500 transcript_attributed.txt    # Theater-style with speakers\n</code></pre>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quickstart/#meeting-transcription","title":"Meeting Transcription","text":"<p>For meeting recordings with multiple speakers:</p> <pre><code># Process with speaker identification\nuv run python main.py --input-dir ./meetings --output-dir ./meeting_transcripts\n\n# Review speaker-attributed transcript\ncat outputs/team_meeting/transcript_attributed.txt\n</code></pre> <p>Output example: <pre><code>SPEAKER_00: Good morning everyone, let's start with the status updates.\nSPEAKER_01: I've completed the frontend implementation...\nSPEAKER_02: The backend API is ready for testing...\n</code></pre></p>"},{"location":"getting-started/quickstart/#single-speaker-audio","title":"Single Speaker Audio","text":"<p>For podcasts or single-speaker content:</p> <pre><code># Disable diarization for faster processing\nuv run python main.py --input-dir ./podcasts --disable-diarization\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch Processing","text":"<p>Process multiple files at once:</p> <pre><code># Create input structure\ninputs/\n\u251c\u2500\u2500 interview_001.wav\n\u251c\u2500\u2500 interview_002.wav\n\u2514\u2500\u2500 interview_003.wav\n\n# Process all files\nuv run python main.py --input-dir ./inputs\n\n# Check summary\ncat outputs/processing_summary.txt\n</code></pre>"},{"location":"getting-started/quickstart/#using-different-models","title":"Using Different Models","text":""},{"location":"getting-started/quickstart/#fast-processing-lower-accuracy","title":"Fast Processing (Lower Accuracy)","text":"<pre><code># Create custom config\ncat &gt; config/fast.yaml &lt;&lt; EOF\nmodel:\n  name: \"stt_en_conformer_ctc_small\"\nprocessing:\n  batch_size: 8\nEOF\n\n# Use fast config\nuv run python main.py --input-dir ./inputs --config config/fast.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#high-accuracy-slower","title":"High Accuracy (Slower)","text":"<pre><code># Use large model (default)\nuv run python main.py --input-dir ./inputs --device cuda\n</code></pre>"},{"location":"getting-started/quickstart/#cpu-processing","title":"CPU Processing","text":"<pre><code># When no GPU available\nuv run python main.py --input-dir ./inputs --device cpu\n</code></pre>"},{"location":"getting-started/quickstart/#python-api-quick-start","title":"Python API Quick Start","text":""},{"location":"getting-started/quickstart/#basic-example","title":"Basic Example","text":"<pre><code>from pathlib import Path\nfrom audio_aigented.pipeline import TranscriptionPipeline\n\n# Initialize pipeline\npipeline = TranscriptionPipeline()\n\n# Process a single file\nresult = pipeline.process_single_file(Path(\"meeting.wav\"))\n\n# Print the transcription\nprint(result.transcription.full_text)\n\n# Access speaker segments\nfor segment in result.transcription.segments:\n    print(f\"{segment.speaker_id}: {segment.text}\")\n</code></pre>"},{"location":"getting-started/quickstart/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from audio_aigented.pipeline import TranscriptionPipeline\nfrom audio_aigented.config import PipelineConfig\n\n# Custom configuration\nconfig = PipelineConfig(\n    device=\"cuda\",\n    enable_diarization=True,\n    output_formats=[\"json\", \"attributed_txt\"]\n)\n\n# Initialize with custom config\npipeline = TranscriptionPipeline(config=config)\n\n# Process files\nresults = pipeline.process_directory(Path(\"./inputs\"))\n</code></pre>"},{"location":"getting-started/quickstart/#accessing-results","title":"Accessing Results","text":"<pre><code># Process file\nresult = pipeline.process_single_file(Path(\"audio.wav\"))\n\n# Access structured data\nprint(f\"Duration: {result.audio_info.duration}s\")\nprint(f\"Processing time: {result.processing_info.processing_time}s\")\n\n# Iterate through segments\nfor segment in result.transcription.segments:\n    print(f\"[{segment.start_time:.1f}s - {segment.end_time:.1f}s] \"\n          f\"{segment.speaker_id}: {segment.text} \"\n          f\"(confidence: {segment.confidence:.2f})\")\n</code></pre>"},{"location":"getting-started/quickstart/#docker-quick-start","title":"Docker Quick Start","text":""},{"location":"getting-started/quickstart/#using-docker-compose","title":"Using Docker Compose","text":"<pre><code># Place audio files in inputs/\ncp *.wav inputs/\n\n# Run with Docker\ndocker-compose run --rm audio-transcription\n\n# Results will be in outputs/\n</code></pre>"},{"location":"getting-started/quickstart/#custom-docker-run","title":"Custom Docker Run","text":"<pre><code># Process specific directory\ndocker run --rm --gpus all \\\n  -v /path/to/audio:/data/inputs \\\n  -v /path/to/results:/data/outputs \\\n  audio-aigented\n</code></pre>"},{"location":"getting-started/quickstart/#tips-for-best-results","title":"Tips for Best Results","text":""},{"location":"getting-started/quickstart/#audio-quality","title":"Audio Quality","text":"<ul> <li>Use high-quality recordings (16kHz or higher)</li> <li>Minimize background noise</li> <li>Ensure clear speech</li> </ul>"},{"location":"getting-started/quickstart/#file-preparation","title":"File Preparation","text":"<ul> <li>Convert to WAV format if needed</li> <li>Split very long files (&gt;1 hour) for better performance</li> <li>Name files descriptively for easier organization</li> </ul>"},{"location":"getting-started/quickstart/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Use GPU for 5-10x faster processing</li> <li>Disable diarization for single-speaker content</li> <li>Process files in batches for efficiency</li> </ul>"},{"location":"getting-started/quickstart/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":""},{"location":"getting-started/quickstart/#no-output-files","title":"No Output Files","text":"<pre><code># Check for errors\nuv run python main.py --input-dir ./inputs --log-level DEBUG\n\n# Verify input files\nls -la inputs/*.wav\n</code></pre>"},{"location":"getting-started/quickstart/#slow-processing","title":"Slow Processing","text":"<pre><code># Use smaller model\nuv run python main.py --input-dir ./inputs \\\n  --config config/fast.yaml\n\n# Check GPU usage\nnvidia-smi\n</code></pre>"},{"location":"getting-started/quickstart/#memory-issues","title":"Memory Issues","text":"<pre><code># Reduce batch size\necho \"processing:\n  batch_size: 1\" &gt; config/low_memory.yaml\n\nuv run python main.py --config config/low_memory.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration Options</li> <li>Explore Speaker Diarization</li> <li>Understand Output Formats</li> <li>Set up Docker Deployment</li> </ul>"},{"location":"guide/audio-processing/","title":"Audio Processing","text":"<p>This guide covers how the pipeline handles audio files, from loading to preprocessing.</p>"},{"location":"guide/audio-processing/#supported-formats","title":"Supported Formats","text":""},{"location":"guide/audio-processing/#primary-format","title":"Primary Format","text":"<ul> <li>WAV (<code>.wav</code>) - Uncompressed audio, best quality</li> <li>Sample rates: 8kHz - 48kHz</li> <li>Bit depths: 16-bit, 24-bit, 32-bit</li> <li>Channels: Mono or stereo</li> </ul>"},{"location":"guide/audio-processing/#format-conversion","title":"Format Conversion","text":"<p>For other formats (MP3, M4A, etc.), convert to WAV first:</p> <pre><code># Using ffmpeg\nffmpeg -i input.mp3 -ar 16000 -ac 1 output.wav\n\n# Using sox\nsox input.mp3 -r 16000 -c 1 output.wav\n</code></pre>"},{"location":"guide/audio-processing/#audio-loading-process","title":"Audio Loading Process","text":""},{"location":"guide/audio-processing/#1-file-validation","title":"1. File Validation","text":"<p>The <code>AudioLoader</code> validates files before processing:</p> <pre><code>def validate_audio_file(file_path: Path) -&gt; bool:\n    # Check file exists\n    if not file_path.exists():\n        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n\n    # Check file extension\n    if file_path.suffix.lower() != '.wav':\n        raise ValueError(f\"Unsupported format: {file_path.suffix}\")\n\n    # Check file size\n    if file_path.stat().st_size == 0:\n        raise ValueError(\"Audio file is empty\")\n</code></pre>"},{"location":"guide/audio-processing/#2-audio-loading","title":"2. Audio Loading","text":"<p>Files are loaded using <code>soundfile</code>:</p> <pre><code>import soundfile as sf\n\ndef load_audio(file_path: Path) -&gt; Tuple[np.ndarray, int]:\n    audio, sample_rate = sf.read(file_path)\n    return audio, sample_rate\n</code></pre>"},{"location":"guide/audio-processing/#3-preprocessing","title":"3. Preprocessing","text":""},{"location":"guide/audio-processing/#resampling","title":"Resampling","text":"<p>All audio is resampled to 16kHz for optimal ASR performance:</p> <pre><code>import librosa\n\ndef resample_audio(audio: np.ndarray, orig_sr: int, target_sr: int = 16000):\n    if orig_sr != target_sr:\n        audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=target_sr)\n    return audio\n</code></pre>"},{"location":"guide/audio-processing/#channel-conversion","title":"Channel Conversion","text":"<p>Stereo audio is converted to mono:</p> <pre><code>def to_mono(audio: np.ndarray) -&gt; np.ndarray:\n    if audio.ndim &gt; 1:\n        # Average all channels\n        audio = np.mean(audio, axis=1)\n    return audio\n</code></pre>"},{"location":"guide/audio-processing/#normalization","title":"Normalization","text":"<p>Audio is normalized to prevent clipping:</p> <pre><code>def normalize_audio(audio: np.ndarray) -&gt; np.ndarray:\n    max_val = np.max(np.abs(audio))\n    if max_val &gt; 0:\n        audio = audio / max_val * 0.95\n    return audio\n</code></pre>"},{"location":"guide/audio-processing/#audio-quality-considerations","title":"Audio Quality Considerations","text":""},{"location":"guide/audio-processing/#optimal-input-characteristics","title":"Optimal Input Characteristics","text":"<ul> <li>Sample Rate: 16kHz or higher</li> <li>Bit Depth: 16-bit or higher</li> <li>Dynamic Range: -20dB to -3dB peaks</li> <li>Background Noise: &lt; -40dB</li> </ul>"},{"location":"guide/audio-processing/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"guide/audio-processing/#low-sample-rate","title":"Low Sample Rate","text":"<p>Problem: Audio recorded at 8kHz sounds muffled Solution: Use upsampling with interpolation <pre><code># Upsample from 8kHz to 16kHz\naudio_16k = librosa.resample(audio_8k, orig_sr=8000, target_sr=16000)\n</code></pre></p>"},{"location":"guide/audio-processing/#clippingdistortion","title":"Clipping/Distortion","text":"<p>Problem: Audio peaks exceed 0dB causing distortion Solution: Apply dynamic range compression <pre><code>def reduce_clipping(audio: np.ndarray, threshold: float = 0.95):\n    audio = np.tanh(audio / threshold) * threshold\n    return audio\n</code></pre></p>"},{"location":"guide/audio-processing/#background-noise","title":"Background Noise","text":"<p>Problem: High background noise affects accuracy Solution: Apply noise reduction (optional preprocessing) <pre><code># Using noisereduce library\nimport noisereduce as nr\naudio_clean = nr.reduce_noise(y=audio, sr=sample_rate)\n</code></pre></p>"},{"location":"guide/audio-processing/#batch-processing","title":"Batch Processing","text":""},{"location":"guide/audio-processing/#efficient-file-loading","title":"Efficient File Loading","text":"<p>Process multiple files efficiently:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\ndef load_audio_batch(file_paths: List[Path], max_workers: int = 4):\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        results = executor.map(load_and_preprocess, file_paths)\n    return list(results)\n</code></pre>"},{"location":"guide/audio-processing/#memory-management","title":"Memory Management","text":"<p>For large files, use chunked processing:</p> <pre><code>def process_large_audio(file_path: Path, chunk_size: int = 30):\n    \"\"\"Process audio in 30-second chunks\"\"\"\n    audio, sr = sf.read(file_path)\n    chunk_samples = chunk_size * sr\n\n    for i in range(0, len(audio), chunk_samples):\n        chunk = audio[i:i + chunk_samples]\n        yield chunk\n</code></pre>"},{"location":"guide/audio-processing/#audio-analysis","title":"Audio Analysis","text":""},{"location":"guide/audio-processing/#duration-calculation","title":"Duration Calculation","text":"<pre><code>def get_audio_duration(file_path: Path) -&gt; float:\n    info = sf.info(file_path)\n    return info.duration\n</code></pre>"},{"location":"guide/audio-processing/#audio-statistics","title":"Audio Statistics","text":"<pre><code>def analyze_audio(audio: np.ndarray, sample_rate: int) -&gt; dict:\n    return {\n        \"duration\": len(audio) / sample_rate,\n        \"rms_level\": np.sqrt(np.mean(audio**2)),\n        \"peak_level\": np.max(np.abs(audio)),\n        \"dynamic_range\": 20 * np.log10(np.max(np.abs(audio)) / np.sqrt(np.mean(audio**2))),\n        \"silence_ratio\": np.sum(np.abs(audio) &lt; 0.01) / len(audio)\n    }\n</code></pre>"},{"location":"guide/audio-processing/#configuration-options","title":"Configuration Options","text":""},{"location":"guide/audio-processing/#audio-processing-settings","title":"Audio Processing Settings","text":"<pre><code>audio:\n  sample_rate: 16000          # Target sample rate\n  mono: true                  # Convert to mono\n  normalize: true             # Normalize audio levels\n  max_duration: 3600          # Maximum duration in seconds\n  min_duration: 0.5           # Minimum duration in seconds\n\npreprocessing:\n  remove_silence: false       # Remove leading/trailing silence\n  noise_reduction: false      # Apply noise reduction\n  compression: false          # Apply dynamic range compression\n</code></pre>"},{"location":"guide/audio-processing/#advanced-options","title":"Advanced Options","text":"<pre><code>audio:\n  resampling_method: \"kaiser_best\"  # librosa resampling method\n  normalization_target: -3.0        # Target peak level in dB\n  silence_threshold: -40.0          # Silence detection threshold\n\nquality_checks:\n  min_sample_rate: 8000            # Reject files below this rate\n  max_noise_level: -30.0           # Reject files with high noise\n  check_clipping: true             # Check for clipped samples\n</code></pre>"},{"location":"guide/audio-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/audio-processing/#common-errors","title":"Common Errors","text":""},{"location":"guide/audio-processing/#audio-file-not-found","title":"\"Audio file not found\"","text":"<pre><code># Check file path\nfile_path = Path(\"./inputs/audio.wav\")\nprint(f\"File exists: {file_path.exists()}\")\nprint(f\"Absolute path: {file_path.absolute()}\")\n</code></pre>"},{"location":"guide/audio-processing/#unsupported-audio-format","title":"\"Unsupported audio format\"","text":"<pre><code># Check file format\nimport wave\nwith wave.open(str(file_path), 'rb') as wav_file:\n    print(f\"Channels: {wav_file.getnchannels()}\")\n    print(f\"Sample rate: {wav_file.getframerate()}\")\n    print(f\"Bit depth: {wav_file.getsampwidth() * 8}\")\n</code></pre>"},{"location":"guide/audio-processing/#memory-error-with-large-files","title":"\"Memory error with large files\"","text":"<pre><code># Use memory mapping for large files\nimport numpy as np\naudio = np.memmap(file_path, dtype='float32', mode='r')\n</code></pre>"},{"location":"guide/audio-processing/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Preprocessing Cache: Cache preprocessed audio    <pre><code>cache_path = Path(f\".cache/{file_path.stem}_16k.npy\")\nif cache_path.exists():\n    audio = np.load(cache_path)\nelse:\n    audio = preprocess_audio(file_path)\n    np.save(cache_path, audio)\n</code></pre></p> </li> <li> <p>Parallel Loading: Load files in parallel    <pre><code>from multiprocessing import Pool\n\nwith Pool(processes=4) as pool:\n    audio_data = pool.map(load_audio, file_paths)\n</code></pre></p> </li> <li> <p>Streaming Processing: For real-time applications    <pre><code>def stream_audio(file_path: Path, block_size: int = 1024):\n    with sf.SoundFile(file_path) as f:\n        while True:\n            audio_block = f.read(block_size)\n            if audio_block.size == 0:\n                break\n            yield audio_block\n</code></pre></p> </li> </ol>"},{"location":"guide/audio-processing/#best-practices","title":"Best Practices","text":""},{"location":"guide/audio-processing/#input-preparation","title":"Input Preparation","text":"<ol> <li>Record at 16kHz or higher</li> <li>Use good quality microphones</li> <li>Minimize background noise</li> <li>Avoid compression artifacts</li> </ol>"},{"location":"guide/audio-processing/#file-organization","title":"File Organization","text":"<pre><code>inputs/\n\u251c\u2500\u2500 high_quality/      # 16kHz+, low noise\n\u251c\u2500\u2500 needs_processing/  # Requires preprocessing\n\u2514\u2500\u2500 archived/          # Processed files\n</code></pre>"},{"location":"guide/audio-processing/#quality-assurance","title":"Quality Assurance","text":"<pre><code>def validate_audio_quality(file_path: Path) -&gt; bool:\n    audio, sr = load_audio(file_path)\n    stats = analyze_audio(audio, sr)\n\n    # Check quality criteria\n    if stats[\"duration\"] &lt; 0.5:\n        logger.warning(f\"Audio too short: {stats['duration']}s\")\n        return False\n\n    if stats[\"silence_ratio\"] &gt; 0.8:\n        logger.warning(f\"Too much silence: {stats['silence_ratio']*100}%\")\n        return False\n\n    if stats[\"peak_level\"] &gt; 0.99:\n        logger.warning(\"Audio may be clipped\")\n        return False\n\n    return True\n</code></pre>"},{"location":"guide/diarization/","title":"Speaker Diarization","text":"<p>Speaker diarization is the process of identifying \"who spoke when\" in an audio recording. This guide covers how the pipeline performs speaker diarization using NVIDIA NeMo's neural diarization models.</p>"},{"location":"guide/diarization/#overview","title":"Overview","text":"<p>The diarization process consists of: 1. Voice Activity Detection (VAD) - Identify speech regions 2. Speaker Embedding Extraction - Create voice fingerprints 3. Clustering - Group similar voice segments 4. Segment Refinement - Clean up speaker boundaries</p>"},{"location":"guide/diarization/#how-it-works","title":"How It Works","text":""},{"location":"guide/diarization/#1-voice-activity-detection","title":"1. Voice Activity Detection","text":"<p>First, the pipeline detects which parts of the audio contain speech:</p> <pre><code># VAD identifies speech regions\nvad_segments = vad_model.get_speech_segments(audio)\n# Output: [(0.5, 2.3), (3.1, 5.7), ...]  # (start, end) times\n</code></pre>"},{"location":"guide/diarization/#2-speaker-embeddings","title":"2. Speaker Embeddings","text":"<p>For each speech segment, the pipeline extracts a speaker embedding (voice fingerprint):</p> <pre><code># Extract embeddings using TitanNet\nembeddings = embedding_model.get_embeddings(speech_segments)\n# Output: Array of 192-dimensional vectors\n</code></pre>"},{"location":"guide/diarization/#3-spectral-clustering","title":"3. Spectral Clustering","text":"<p>Embeddings are clustered to identify unique speakers:</p> <pre><code># Cluster embeddings to find speakers\nspeaker_labels = clustering_backend.cluster(embeddings)\n# Output: [0, 0, 1, 0, 1, 2, ...]  # Speaker IDs for each segment\n</code></pre>"},{"location":"guide/diarization/#4-output-format","title":"4. Output Format","text":"<p>Diarization results are saved in RTTM format:</p> <pre><code>SPEAKER audio_file 1 0.500 1.800 &lt;NA&gt; &lt;NA&gt; SPEAKER_00 &lt;NA&gt; &lt;NA&gt;\nSPEAKER audio_file 1 3.100 2.600 &lt;NA&gt; &lt;NA&gt; SPEAKER_01 &lt;NA&gt; &lt;NA&gt;\n</code></pre>"},{"location":"guide/diarization/#configuration","title":"Configuration","text":""},{"location":"guide/diarization/#basic-settings","title":"Basic Settings","text":"<pre><code>diarization:\n  enable: true                    # Enable/disable diarization\n  device: \"cuda\"                  # GPU acceleration\n\n  vad:\n    model_path: \"vad_multilingual_marblenet\"\n    onset: 0.8                    # Speech detection sensitivity\n    offset: 0.6                   # Speech end sensitivity\n\n  embedding:\n    model_path: \"titanet-l\"       # Speaker embedding model\n    window_length: 1.5            # Embedding window size\n    shift_length: 0.75            # Window shift\n\n  clustering:\n    backend: \"SC\"                 # Spectral Clustering\n    parameters:\n      oracle_num_speakers: null   # Auto-detect speakers\n      max_num_speakers: 8         # Maximum speakers\n      enhanced_count_thresholding: true\n</code></pre>"},{"location":"guide/diarization/#advanced-options","title":"Advanced Options","text":"<pre><code>diarization:\n  clustering:\n    parameters:\n      maj_vote_spk_count: true    # Majority voting\n      min_samples_for_nmesc: 6    # Minimum samples for NME-SC\n\n  postprocessing:\n    onset: 0.5                    # Refine segment starts\n    offset: 0.5                   # Refine segment ends\n    min_duration_on: 0.1          # Minimum speech duration\n    min_duration_off: 0.1         # Minimum silence duration\n</code></pre>"},{"location":"guide/diarization/#usage-examples","title":"Usage Examples","text":""},{"location":"guide/diarization/#enabledisable-diarization","title":"Enable/Disable Diarization","text":"<pre><code># With diarization (default)\nuv run python main.py --input-dir ./inputs\n\n# Without diarization (faster)\nuv run python main.py --input-dir ./inputs --disable-diarization\n</code></pre>"},{"location":"guide/diarization/#specify-number-of-speakers","title":"Specify Number of Speakers","text":"<p>When you know the number of speakers:</p> <pre><code># config/two_speakers.yaml\ndiarization:\n  clustering:\n    parameters:\n      oracle_num_speakers: 2\n</code></pre> <pre><code>uv run python main.py --config config/two_speakers.yaml\n</code></pre>"},{"location":"guide/diarization/#python-api","title":"Python API","text":"<pre><code>from audio_aigented.diarization import Diarizer\nfrom audio_aigented.config import DiarizationConfig\n\n# Configure diarizer\nconfig = DiarizationConfig(\n    enable=True,\n    oracle_num_speakers=3  # Known: 3 speakers\n)\n\n# Initialize and run\ndiarizer = Diarizer(config)\nspeaker_segments = diarizer.diarize(audio_path)\n\n# Access results\nfor segment in speaker_segments:\n    print(f\"{segment.speaker}: {segment.start:.1f}s - {segment.end:.1f}s\")\n</code></pre>"},{"location":"guide/diarization/#output-formats","title":"Output Formats","text":""},{"location":"guide/diarization/#rttm-file-format","title":"RTTM File Format","text":"<p>The standard Rich Transcription Time Marked (RTTM) format:</p> <pre><code>SPEAKER file_id 1 start_time duration &lt;NA&gt; &lt;NA&gt; speaker_id &lt;NA&gt; &lt;NA&gt;\n</code></pre> <p>Example: <pre><code>SPEAKER meeting_audio 1 0.000 5.230 &lt;NA&gt; &lt;NA&gt; SPEAKER_00 &lt;NA&gt; &lt;NA&gt;\nSPEAKER meeting_audio 1 5.230 3.120 &lt;NA&gt; &lt;NA&gt; SPEAKER_01 &lt;NA&gt; &lt;NA&gt;\nSPEAKER meeting_audio 1 8.350 2.450 &lt;NA&gt; &lt;NA&gt; SPEAKER_00 &lt;NA&gt; &lt;NA&gt;\n</code></pre></p>"},{"location":"guide/diarization/#json-format","title":"JSON Format","text":"<p>Structured diarization data:</p> <pre><code>{\n  \"speakers\": {\n    \"SPEAKER_00\": {\n      \"segments\": [\n        {\"start\": 0.0, \"end\": 5.23},\n        {\"start\": 8.35, \"end\": 10.8}\n      ],\n      \"total_duration\": 7.68\n    },\n    \"SPEAKER_01\": {\n      \"segments\": [\n        {\"start\": 5.23, \"end\": 8.35}\n      ],\n      \"total_duration\": 3.12\n    }\n  },\n  \"total_speakers\": 2\n}\n</code></pre>"},{"location":"guide/diarization/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guide/diarization/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Diarization is significantly faster on GPU:</p> Component CPU Time GPU Time Speedup VAD 2.5s 0.3s 8.3x Embeddings 45s 4.5s 10x Clustering 1.5s 1.5s 1x Total 49s 6.3s 7.8x <p>Times for 5-minute audio file</p>"},{"location":"guide/diarization/#batch-processing","title":"Batch Processing","text":"<p>Process multiple files efficiently:</p> <pre><code># Batch configuration\nconfig = DiarizationConfig(\n    batch_size=8,  # Process 8 files simultaneously\n    num_workers=4  # Parallel data loading\n)\n</code></pre>"},{"location":"guide/diarization/#memory-management","title":"Memory Management","text":"<p>For long audio files:</p> <pre><code>diarization:\n  max_audio_length: 1800  # Process in 30-minute chunks\n  overlap_duration: 5     # 5-second overlap between chunks\n</code></pre>"},{"location":"guide/diarization/#quality-tuning","title":"Quality Tuning","text":""},{"location":"guide/diarization/#for-meetingsconferences","title":"For Meetings/Conferences","text":"<pre><code># Many speakers, overlapping speech\ndiarization:\n  clustering:\n    parameters:\n      max_num_speakers: 15\n      enhanced_count_thresholding: true\n\n  vad:\n    onset: 0.7   # More sensitive\n    offset: 0.5\n</code></pre>"},{"location":"guide/diarization/#for-interviewspodcasts","title":"For Interviews/Podcasts","text":"<pre><code># Few speakers, clear turn-taking\ndiarization:\n  clustering:\n    parameters:\n      oracle_num_speakers: 2  # Host + Guest\n\n  vad:\n    onset: 0.9   # Less sensitive\n    offset: 0.7\n</code></pre>"},{"location":"guide/diarization/#for-noisy-environments","title":"For Noisy Environments","text":"<pre><code># Background noise, multiple speakers\ndiarization:\n  vad:\n    model_path: \"vad_multilingual_marblenet\"\n    onset: 0.85\n    offset: 0.65\n\n  postprocessing:\n    min_duration_on: 0.2   # Longer minimum speech\n    min_duration_off: 0.3  # Longer silence gaps\n</code></pre>"},{"location":"guide/diarization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/diarization/#common-issues","title":"Common Issues","text":""},{"location":"guide/diarization/#too-many-speakers-detected","title":"Too Many Speakers Detected","text":"<p>Problem: System detects more speakers than actually present</p> <p>Solutions: 1. Set <code>oracle_num_speakers</code> if known 2. Adjust clustering parameters:    <pre><code>clustering:\n  parameters:\n    max_num_speakers: 4\n    enhanced_count_thresholding: true\n</code></pre></p>"},{"location":"guide/diarization/#speaker-confusion","title":"Speaker Confusion","text":"<p>Problem: Same speaker identified as multiple speakers</p> <p>Solutions: 1. Increase embedding window size:    <pre><code>embedding:\n  window_length: 2.0  # Longer windows\n  shift_length: 1.0\n</code></pre></p> <ol> <li>Use majority voting:    <pre><code>clustering:\n  parameters:\n    maj_vote_spk_count: true\n</code></pre></li> </ol>"},{"location":"guide/diarization/#missing-short-utterances","title":"Missing Short Utterances","text":"<p>Problem: Short speech segments not detected</p> <p>Solutions: <pre><code>vad:\n  onset: 0.7    # Lower threshold\n  offset: 0.5\n\npostprocessing:\n  min_duration_on: 0.05  # Shorter minimum\n</code></pre></p>"},{"location":"guide/diarization/#performance-issues","title":"Performance Issues","text":""},{"location":"guide/diarization/#slow-processing","title":"Slow Processing","text":"<ol> <li> <p>Use GPU: Ensure CUDA is available    <pre><code>nvidia-smi  # Check GPU availability\n</code></pre></p> </li> <li> <p>Reduce audio length: Process in chunks    <pre><code>diarization:\n  max_audio_length: 900  # 15-minute chunks\n</code></pre></p> </li> <li> <p>Disable if not needed:    <pre><code>uv run python main.py --disable-diarization\n</code></pre></p> </li> </ol>"},{"location":"guide/diarization/#out-of-memory","title":"Out of Memory","text":"<ol> <li> <p>Reduce batch size:    <pre><code>diarization:\n  batch_size: 1\n</code></pre></p> </li> <li> <p>Use CPU clustering:    <pre><code>clustering:\n  backend: \"SC_cpu\"\n</code></pre></p> </li> </ol>"},{"location":"guide/diarization/#best-practices","title":"Best Practices","text":""},{"location":"guide/diarization/#audio-quality","title":"Audio Quality","text":"<ol> <li>Clear audio: Minimize background noise</li> <li>Good separation: Speakers should not talk over each other</li> <li>Consistent volume: Similar audio levels for all speakers</li> </ol>"},{"location":"guide/diarization/#configuration_1","title":"Configuration","text":"<ol> <li>Start with defaults: Work well for most cases</li> <li>Set oracle_num_speakers: When known, always specify</li> <li>Test on samples: Tune parameters on representative clips</li> </ol>"},{"location":"guide/diarization/#integration","title":"Integration","text":"<pre><code># Example: Post-process diarization results\ndef merge_short_segments(segments, min_gap=0.5):\n    \"\"\"Merge segments from same speaker with small gaps\"\"\"\n    merged = []\n    for segment in segments:\n        if merged and merged[-1].speaker == segment.speaker:\n            if segment.start - merged[-1].end &lt; min_gap:\n                merged[-1].end = segment.end\n                continue\n        merged.append(segment)\n    return merged\n</code></pre>"},{"location":"guide/diarization/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guide/diarization/#custom-embedding-models","title":"Custom Embedding Models","text":"<pre><code># Use custom speaker embedding model\nfrom nemo.collections.asr.models import EncDecSpeakerLabelModel\n\ncustom_model = EncDecSpeakerLabelModel.from_pretrained(\"your_model\")\ndiarizer.embedding_model = custom_model\n</code></pre>"},{"location":"guide/diarization/#real-time-diarization","title":"Real-time Diarization","text":"<pre><code># Streaming diarization (experimental)\nasync def stream_diarization(audio_stream):\n    buffer = []\n    for chunk in audio_stream:\n        buffer.append(chunk)\n        if len(buffer) &gt; window_size:\n            segments = diarizer.process_window(buffer)\n            yield segments\n            buffer = buffer[overlap:]\n</code></pre>"},{"location":"guide/diarization/#multi-stage-diarization","title":"Multi-stage Diarization","text":"<pre><code># Hierarchical clustering for many speakers\ndef hierarchical_diarization(audio_path, max_speakers=20):\n    # Stage 1: Initial clustering\n    initial_segments = diarizer.diarize(\n        audio_path, \n        max_speakers=max_speakers\n    )\n\n    # Stage 2: Refine with sub-clustering\n    refined_segments = []\n    for speaker_group in group_by_speaker(initial_segments):\n        if len(speaker_group) &gt; threshold:\n            sub_segments = diarizer.diarize(\n                speaker_group,\n                max_speakers=5\n            )\n            refined_segments.extend(sub_segments)\n\n    return refined_segments\n</code></pre>"},{"location":"guide/output-formats/","title":"Output Formats","text":"<p>The Audio Transcription Pipeline generates multiple output formats to suit different use cases. This guide covers each format in detail.</p>"},{"location":"guide/output-formats/#output-structure","title":"Output Structure","text":"<p>For each processed audio file, the pipeline creates:</p> <pre><code>outputs/\n\u2514\u2500\u2500 audio_filename/\n    \u251c\u2500\u2500 transcript.json              # Structured data\n    \u251c\u2500\u2500 transcript.txt               # Human-readable report\n    \u2514\u2500\u2500 transcript_attributed.txt    # Theater-style dialog\n</code></pre>"},{"location":"guide/output-formats/#json-format","title":"JSON Format","text":""},{"location":"guide/output-formats/#overview","title":"Overview","text":"<p>The JSON format provides complete structured data including: - Audio metadata - Full transcription with timestamps - Speaker segments - Confidence scores - Processing information</p>"},{"location":"guide/output-formats/#structure","title":"Structure","text":"<pre><code>{\n  \"audio_file\": {\n    \"path\": \"./inputs/meeting.wav\",\n    \"filename\": \"meeting.wav\",\n    \"duration\": 125.34,\n    \"sample_rate\": 16000,\n    \"channels\": 1,\n    \"size_bytes\": 4013088\n  },\n  \"transcription\": {\n    \"full_text\": \"Good morning everyone. Let's begin today's meeting...\",\n    \"segments\": [\n      {\n        \"text\": \"Good morning everyone.\",\n        \"start_time\": 0.5,\n        \"end_time\": 2.3,\n        \"confidence\": 0.95,\n        \"speaker_id\": \"SPEAKER_00\",\n        \"words\": [\n          {\n            \"text\": \"Good\",\n            \"start\": 0.5,\n            \"end\": 0.7,\n            \"confidence\": 0.97\n          },\n          {\n            \"text\": \"morning\",\n            \"start\": 0.8,\n            \"end\": 1.2,\n            \"confidence\": 0.96\n          },\n          {\n            \"text\": \"everyone\",\n            \"start\": 1.3,\n            \"end\": 2.1,\n            \"confidence\": 0.93\n          }\n        ]\n      }\n    ]\n  },\n  \"speakers\": {\n    \"count\": 3,\n    \"details\": {\n      \"SPEAKER_00\": {\n        \"segments\": 15,\n        \"total_duration\": 45.2,\n        \"percentage\": 36.1\n      },\n      \"SPEAKER_01\": {\n        \"segments\": 12,\n        \"total_duration\": 38.7,\n        \"percentage\": 30.9\n      },\n      \"SPEAKER_02\": {\n        \"segments\": 8,\n        \"total_duration\": 41.4,\n        \"percentage\": 33.0\n      }\n    }\n  },\n  \"processing\": {\n    \"timestamp\": \"2024-01-15T10:30:45Z\",\n    \"processing_time\": 12.34,\n    \"real_time_factor\": 10.16,\n    \"model\": {\n      \"name\": \"stt_en_conformer_ctc_large\",\n      \"version\": \"1.20.0\"\n    },\n    \"diarization\": {\n      \"enabled\": true,\n      \"model\": \"titanet-l\"\n    },\n    \"device\": \"cuda\",\n    \"success\": true\n  }\n}\n</code></pre>"},{"location":"guide/output-formats/#usage-examples","title":"Usage Examples","text":"<pre><code>import json\n\n# Load transcription results\nwith open(\"outputs/meeting/transcript.json\") as f:\n    data = json.load(f)\n\n# Access full text\nprint(data[\"transcription\"][\"full_text\"])\n\n# Iterate through segments\nfor segment in data[\"transcription\"][\"segments\"]:\n    print(f\"{segment['speaker_id']}: {segment['text']}\")\n\n# Get speaker statistics\nfor speaker, stats in data[\"speakers\"][\"details\"].items():\n    print(f\"{speaker}: {stats['percentage']:.1f}% of conversation\")\n</code></pre>"},{"location":"guide/output-formats/#text-format","title":"Text Format","text":""},{"location":"guide/output-formats/#overview_1","title":"Overview","text":"<p>The text format provides a human-readable report with: - Summary statistics - Formatted transcription - Speaker transitions - Processing metadata</p>"},{"location":"guide/output-formats/#example-output","title":"Example Output","text":"<pre><code>================================================================================\n                          TRANSCRIPTION REPORT\n================================================================================\n\nFile: meeting.wav\nDuration: 2:05 (125.34 seconds)\nProcessed: 2024-01-15 10:30:45\nProcessing Time: 12.34 seconds (10.2x faster than real-time)\n\n--------------------------------------------------------------------------------\n                           SPEAKER STATISTICS\n--------------------------------------------------------------------------------\n\nTotal Speakers: 3\n\nSPEAKER_00: 45.2 seconds (36.1%) - 15 segments\nSPEAKER_01: 38.7 seconds (30.9%) - 12 segments  \nSPEAKER_02: 41.4 seconds (33.0%) - 8 segments\n\n--------------------------------------------------------------------------------\n                              TRANSCRIPTION\n--------------------------------------------------------------------------------\n\n[00:00:00 - 00:00:02] SPEAKER_00 (confidence: 0.95)\nGood morning everyone.\n\n[00:00:02 - 00:00:05] SPEAKER_01 (confidence: 0.93)\nGood morning. Thanks for joining today's meeting.\n\n[00:00:06 - 00:00:10] SPEAKER_00 (confidence: 0.94)\nLet's start with the agenda. We have three main topics to cover.\n\n[00:00:11 - 00:00:15] SPEAKER_02 (confidence: 0.92)\nBefore we begin, I'd like to add one item to the agenda if that's okay.\n\n================================================================================\n                           END OF TRANSCRIPTION\n================================================================================\n</code></pre>"},{"location":"guide/output-formats/#configuration-options","title":"Configuration Options","text":"<pre><code>output:\n  text:\n    include_timestamps: true\n    include_confidence: true\n    include_speaker_stats: true\n    timestamp_format: \"[%H:%M:%S]\"  # Or \"[%M:%S]\" for shorter\n    confidence_threshold: 0.0        # Hide low confidence segments\n</code></pre>"},{"location":"guide/output-formats/#attributed-text-format","title":"Attributed Text Format","text":""},{"location":"guide/output-formats/#overview_2","title":"Overview","text":"<p>The attributed text format presents conversations in a theater-style dialog format: - Clean, readable layout - Speaker labels on each turn - Natural conversation flow - Perfect for meeting minutes</p>"},{"location":"guide/output-formats/#example-output_1","title":"Example Output","text":"<pre><code>SPEAKER_00: Good morning everyone.\n\nSPEAKER_01: Good morning. Thanks for joining today's meeting.\n\nSPEAKER_00: Let's start with the agenda. We have three main topics to cover.\n\nSPEAKER_02: Before we begin, I'd like to add one item to the agenda if that's okay.\n\nSPEAKER_00: Of course, what would you like to discuss?\n\nSPEAKER_02: I think we should review the Q4 results before moving to next year's planning.\n\nSPEAKER_01: That's a great point. Let's add that as the first item.\n\nSPEAKER_00: Agreed. So our updated agenda is: First, Q4 results review. Second, 2024 planning. Third, resource allocation. And finally, next steps.\n\nSPEAKER_01: Perfect. Should we start with the Q4 review then?\n\nSPEAKER_02: Yes, I have the slides ready to share.\n</code></pre>"},{"location":"guide/output-formats/#use-cases","title":"Use Cases","text":"<ol> <li>Meeting Minutes: Clean format for documentation</li> <li>Interview Transcripts: Easy to follow Q&amp;A</li> <li>Podcast Show Notes: Reader-friendly dialog</li> <li>Legal Depositions: Clear speaker attribution</li> </ol>"},{"location":"guide/output-formats/#customization","title":"Customization","text":"<pre><code>output:\n  attributed:\n    speaker_format: \"{speaker}: \"    # Or \"- {speaker}: \" for bullets\n    paragraph_break: true            # Add breaks between speakers\n    merge_consecutive: true          # Merge same-speaker turns\n    min_segment_words: 3            # Skip very short utterances\n</code></pre>"},{"location":"guide/output-formats/#processing-summary","title":"Processing Summary","text":""},{"location":"guide/output-formats/#overview_3","title":"Overview","text":"<p>A summary file is created for batch processing:</p> <pre><code>outputs/processing_summary.txt\n</code></pre>"},{"location":"guide/output-formats/#example-content","title":"Example Content","text":"<pre><code>================================================================================\n                         BATCH PROCESSING SUMMARY\n================================================================================\n\nProcessing Started: 2024-01-15 10:30:00\nProcessing Completed: 2024-01-15 10:35:45\nTotal Duration: 5 minutes 45 seconds\n\n--------------------------------------------------------------------------------\n                              FILES PROCESSED\n--------------------------------------------------------------------------------\n\nSuccessfully Processed: 8/10 files\n\n\u2713 meeting_2024_01_15.wav\n  - Duration: 2:05\n  - Speakers: 3\n  - Processing: 12.3s\n\n\u2713 interview_john_doe.wav\n  - Duration: 45:32\n  - Speakers: 2\n  - Processing: 3:12s\n\n\u2713 podcast_episode_042.wav\n  - Duration: 1:15:20\n  - Speakers: 4\n  - Processing: 5:45s\n\n[... more files ...]\n\nFailed: 2 files\n\n\u2717 corrupted_audio.wav\n  - Error: Invalid audio format\n\n\u2717 empty_file.wav\n  - Error: File is empty\n\n--------------------------------------------------------------------------------\n                              STATISTICS\n--------------------------------------------------------------------------------\n\nTotal Audio Duration: 4:32:15\nTotal Processing Time: 0:25:32\nAverage Speed: 10.7x real-time\n\nTotal Speakers Detected: 23\nAverage Speakers per File: 2.9\n\nModel Used: stt_en_conformer_ctc_large\nDevice: NVIDIA GeForce RTX 3090\n\n================================================================================\n</code></pre>"},{"location":"guide/output-formats/#custom-output-formats","title":"Custom Output Formats","text":""},{"location":"guide/output-formats/#adding-new-formats","title":"Adding New Formats","text":"<pre><code>from audio_aigented.formatting import BaseFormatter\n\nclass MarkdownFormatter(BaseFormatter):\n    def format(self, result: TranscriptionResult) -&gt; str:\n        output = f\"# Transcription: {result.audio_file.filename}\\n\\n\"\n        output += f\"**Duration:** {result.audio_file.duration:.1f}s\\n\\n\"\n\n        for segment in result.transcription.segments:\n            output += f\"**{segment.speaker_id}** \"\n            output += f\"*({segment.start_time:.1f}s)*: \"\n            output += f\"{segment.text}\\n\\n\"\n\n        return output\n\n# Register formatter\npipeline.add_formatter(\"markdown\", MarkdownFormatter())\n</code></pre>"},{"location":"guide/output-formats/#output-post-processing","title":"Output Post-Processing","text":"<pre><code># Add timestamps to existing format\ndef add_word_timestamps(json_data):\n    output = []\n    for segment in json_data[\"transcription\"][\"segments\"]:\n        for word in segment.get(\"words\", []):\n            output.append(f\"[{word['start']:.2f}] {word['text']}\")\n    return \" \".join(output)\n\n# Generate SRT subtitles\ndef to_srt(json_data):\n    srt_output = []\n    for i, segment in enumerate(json_data[\"transcription\"][\"segments\"], 1):\n        start = format_srt_time(segment[\"start_time\"])\n        end = format_srt_time(segment[\"end_time\"])\n        text = f\"{segment['speaker_id']}: {segment['text']}\"\n\n        srt_output.append(f\"{i}\\n{start} --&gt; {end}\\n{text}\\n\")\n\n    return \"\\n\".join(srt_output)\n</code></pre>"},{"location":"guide/output-formats/#export-options","title":"Export Options","text":""},{"location":"guide/output-formats/#csv-export","title":"CSV Export","text":"<pre><code>import csv\n\ndef export_to_csv(json_file, csv_file):\n    with open(json_file) as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Speaker', 'Start', 'End', 'Text', 'Confidence'])\n\n        for segment in data['transcription']['segments']:\n            writer.writerow([\n                segment['speaker_id'],\n                segment['start_time'],\n                segment['end_time'],\n                segment['text'],\n                segment['confidence']\n            ])\n</code></pre>"},{"location":"guide/output-formats/#database-export","title":"Database Export","text":"<pre><code>import sqlite3\n\ndef export_to_db(json_file, db_file):\n    with open(json_file) as f:\n        data = json.load(f)\n\n    conn = sqlite3.connect(db_file)\n    c = conn.cursor()\n\n    # Create table\n    c.execute('''CREATE TABLE IF NOT EXISTS transcripts\n                 (id INTEGER PRIMARY KEY,\n                  filename TEXT,\n                  speaker TEXT,\n                  start_time REAL,\n                  end_time REAL,\n                  text TEXT,\n                  confidence REAL)''')\n\n    # Insert segments\n    for segment in data['transcription']['segments']:\n        c.execute('''INSERT INTO transcripts \n                     (filename, speaker, start_time, end_time, text, confidence)\n                     VALUES (?, ?, ?, ?, ?, ?)''',\n                  (data['audio_file']['filename'],\n                   segment['speaker_id'],\n                   segment['start_time'],\n                   segment['end_time'],\n                   segment['text'],\n                   segment['confidence']))\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"guide/output-formats/#best-practices","title":"Best Practices","text":""},{"location":"guide/output-formats/#choosing-output-formats","title":"Choosing Output Formats","text":"<ol> <li>JSON: For programmatic access and further processing</li> <li>TXT: For human review and reports</li> <li>Attributed: For meeting minutes and documentation</li> </ol>"},{"location":"guide/output-formats/#file-organization","title":"File Organization","text":"<pre><code># Organize outputs by date\noutputs/\n\u251c\u2500\u2500 2024-01-15/\n\u2502   \u251c\u2500\u2500 morning_meeting/\n\u2502   \u251c\u2500\u2500 client_call/\n\u2502   \u2514\u2500\u2500 team_standup/\n\u2514\u2500\u2500 2024-01-16/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/output-formats/#compression","title":"Compression","text":"<pre><code># Compress large output files\nimport gzip\nimport shutil\n\ndef compress_outputs(output_dir):\n    for json_file in output_dir.glob(\"**/*.json\"):\n        with open(json_file, 'rb') as f_in:\n            with gzip.open(f\"{json_file}.gz\", 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        json_file.unlink()  # Remove original\n</code></pre>"},{"location":"guide/output-formats/#validation","title":"Validation","text":"<pre><code># Validate output completeness\ndef validate_output(output_dir):\n    required_files = ['transcript.json', 'transcript.txt', \n                     'transcript_attributed.txt']\n\n    for required in required_files:\n        if not (output_dir / required).exists():\n            raise FileNotFoundError(f\"Missing {required}\")\n\n    # Validate JSON structure\n    with open(output_dir / 'transcript.json') as f:\n        data = json.load(f)\n        assert 'transcription' in data\n        assert 'segments' in data['transcription']\n\n    return True\n</code></pre>"},{"location":"guide/overview/","title":"Pipeline Overview","text":"<p>The Audio Transcription Pipeline is designed as a modular, extensible system for converting speech to text with speaker attribution.</p>"},{"location":"guide/overview/#architecture","title":"Architecture","text":"<p>The pipeline follows a sequential processing model with five main stages:</p> <pre><code>graph LR\n    A[Audio Files] --&gt; B[Audio Loader]\n    B --&gt; C{Diarization Enabled?}\n    C --&gt;|Yes| D[Speaker Diarization]\n    C --&gt;|No| E[Full Audio]\n    D --&gt; F[Speaker Segments]\n    E --&gt; F\n    F --&gt; G[ASR Transcription]\n    G --&gt; H[Output Formatter]\n    H --&gt; I[File Writer]\n    I --&gt; J[Results]</code></pre>"},{"location":"guide/overview/#processing-flow","title":"Processing Flow","text":""},{"location":"guide/overview/#1-audio-loading","title":"1. Audio Loading","text":"<ul> <li>Purpose: Load and prepare audio files for processing</li> <li>Module: <code>audio_aigented.audio.loader</code></li> <li>Functionality:</li> <li>Validates WAV file format</li> <li>Resamples to target rate (16kHz)</li> <li>Converts to mono if needed</li> <li>Extracts audio metadata</li> </ul>"},{"location":"guide/overview/#2-speaker-diarization-optional","title":"2. Speaker Diarization (Optional)","text":"<ul> <li>Purpose: Identify different speakers and their speaking segments</li> <li>Module: <code>audio_aigented.diarization.diarizer</code></li> <li>Functionality:</li> <li>Detects voice activity</li> <li>Extracts speaker embeddings</li> <li>Clusters speakers</li> <li>Outputs RTTM format segments</li> </ul>"},{"location":"guide/overview/#3-asr-transcription","title":"3. ASR Transcription","text":"<ul> <li>Purpose: Convert speech segments to text</li> <li>Module: <code>audio_aigented.transcription.asr</code></li> <li>Functionality:</li> <li>Processes audio segments</li> <li>Uses NVIDIA NeMo models</li> <li>Generates confidence scores</li> <li>Preserves timing information</li> </ul>"},{"location":"guide/overview/#4-output-formatting","title":"4. Output Formatting","text":"<ul> <li>Purpose: Structure transcription data for different use cases</li> <li>Module: <code>audio_aigented.formatting.formatter</code></li> <li>Functionality:</li> <li>Aligns transcriptions with speakers</li> <li>Formats multiple output types</li> <li>Calculates statistics</li> </ul>"},{"location":"guide/overview/#5-file-writing","title":"5. File Writing","text":"<ul> <li>Purpose: Save results to disk</li> <li>Module: <code>audio_aigented.output.writer</code></li> <li>Functionality:</li> <li>Creates output directories</li> <li>Writes JSON, TXT, and attributed formats</li> <li>Generates processing summaries</li> </ul>"},{"location":"guide/overview/#key-components","title":"Key Components","text":""},{"location":"guide/overview/#pipeline-orchestrator","title":"Pipeline Orchestrator","text":"<p>The <code>TranscriptionPipeline</code> class coordinates all stages:</p> <pre><code>class TranscriptionPipeline:\n    def __init__(self, config: PipelineConfig):\n        self.audio_loader = AudioLoader(config)\n        self.diarizer = Diarizer(config) if config.enable_diarization else None\n        self.transcriber = Transcriber(config)\n        self.formatter = OutputFormatter(config)\n        self.writer = OutputWriter(config)\n</code></pre>"},{"location":"guide/overview/#data-models","title":"Data Models","text":"<p>All data is validated using Pydantic models:</p> <pre><code>@dataclass\nclass TranscriptionSegment:\n    text: str\n    start_time: float\n    end_time: float\n    confidence: float\n    speaker_id: str\n\n@dataclass\nclass TranscriptionResult:\n    audio_file: AudioInfo\n    transcription: TranscriptionData\n    processing_info: ProcessingInfo\n</code></pre>"},{"location":"guide/overview/#configuration-system","title":"Configuration System","text":"<p>Flexible YAML-based configuration with OmegaConf:</p> <pre><code>config = OmegaConf.load(\"config/default.yaml\")\nconfig = OmegaConf.merge(config, custom_config)\n</code></pre>"},{"location":"guide/overview/#processing-modes","title":"Processing Modes","text":""},{"location":"guide/overview/#with-speaker-diarization-default","title":"With Speaker Diarization (Default)","text":"<ol> <li>Audio \u2192 Diarization \u2192 Speaker segments</li> <li>Each segment \u2192 ASR \u2192 Transcribed text</li> <li>Results merged with speaker attribution</li> </ol> <p>Best for: Meetings, interviews, multi-speaker content</p>"},{"location":"guide/overview/#without-speaker-diarization","title":"Without Speaker Diarization","text":"<ol> <li>Audio \u2192 Full file as single segment</li> <li>Complete audio \u2192 ASR \u2192 Transcribed text</li> <li>All text attributed to SPEAKER_00</li> </ol> <p>Best for: Single speaker content, faster processing</p>"},{"location":"guide/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"guide/overview/#gpu-processing","title":"GPU Processing","text":"<ul> <li>Diarization: ~10x real-time</li> <li>Transcription: 4-15x real-time (model dependent)</li> <li>Total: 3-8x real-time for complete pipeline</li> </ul>"},{"location":"guide/overview/#cpu-processing","title":"CPU Processing","text":"<ul> <li>Diarization: ~2x real-time</li> <li>Transcription: 0.4-1.5x real-time</li> <li>Total: 0.3-1x real-time</li> </ul>"},{"location":"guide/overview/#memory-usage","title":"Memory Usage","text":"<ul> <li>Base: ~2GB RAM</li> <li>Per file: ~100MB + audio size</li> <li>GPU: 2-6GB VRAM (model dependent)</li> </ul>"},{"location":"guide/overview/#error-handling","title":"Error Handling","text":"<p>The pipeline implements comprehensive error handling:</p>"},{"location":"guide/overview/#graceful-degradation","title":"Graceful Degradation","text":"<ul> <li>Falls back to CPU if GPU unavailable</li> <li>Continues processing other files on single file failure</li> <li>Provides detailed error messages</li> </ul>"},{"location":"guide/overview/#recovery-mechanisms","title":"Recovery Mechanisms","text":"<ul> <li>Automatic retry for transient failures</li> <li>Caching to resume interrupted processing</li> <li>Validation at each stage</li> </ul>"},{"location":"guide/overview/#logging","title":"Logging","text":"<ul> <li>Structured logging with levels</li> <li>Per-file processing logs</li> <li>Summary statistics</li> </ul>"},{"location":"guide/overview/#extensibility","title":"Extensibility","text":""},{"location":"guide/overview/#adding-new-models","title":"Adding New Models","text":"<pre><code># Custom ASR model\nclass CustomASRModel(BaseASRModel):\n    def transcribe(self, audio: np.ndarray) -&gt; List[Segment]:\n        # Implementation\n        pass\n\n# Register in pipeline\npipeline.transcriber = CustomASRModel()\n</code></pre>"},{"location":"guide/overview/#custom-output-formats","title":"Custom Output Formats","text":"<pre><code># Custom formatter\nclass CustomFormatter(BaseFormatter):\n    def format(self, result: TranscriptionResult) -&gt; str:\n        # Custom formatting logic\n        pass\n\n# Add to pipeline\npipeline.formatters.append(CustomFormatter())\n</code></pre>"},{"location":"guide/overview/#processing-hooks","title":"Processing Hooks","text":"<pre><code># Pre/post processing hooks\npipeline.add_preprocessor(noise_reduction)\npipeline.add_postprocessor(punctuation_restoration)\n</code></pre>"},{"location":"guide/overview/#best-practices","title":"Best Practices","text":""},{"location":"guide/overview/#input-preparation","title":"Input Preparation","text":"<ol> <li>Use high-quality audio (16kHz+ sample rate)</li> <li>Minimize background noise</li> <li>Ensure clear speech</li> <li>Split very long files</li> </ol>"},{"location":"guide/overview/#configuration-tuning","title":"Configuration Tuning","text":"<ol> <li>Start with defaults</li> <li>Profile on representative samples</li> <li>Adjust based on quality/speed needs</li> <li>Monitor resource usage</li> </ol>"},{"location":"guide/overview/#production-deployment","title":"Production Deployment","text":"<ol> <li>Use Docker for consistency</li> <li>Set up monitoring</li> <li>Implement queuing for large batches</li> <li>Configure appropriate timeouts</li> </ol>"},{"location":"guide/overview/#integration-examples","title":"Integration Examples","text":""},{"location":"guide/overview/#web-service","title":"Web Service","text":"<pre><code>from fastapi import FastAPI, UploadFile\nfrom audio_aigented.pipeline import TranscriptionPipeline\n\napp = FastAPI()\npipeline = TranscriptionPipeline()\n\n@app.post(\"/transcribe\")\nasync def transcribe(file: UploadFile):\n    result = pipeline.process_file(file)\n    return result.to_dict()\n</code></pre>"},{"location":"guide/overview/#batch-processing","title":"Batch Processing","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor\nfrom audio_aigented.pipeline import TranscriptionPipeline\n\ndef process_batch(files: List[Path], workers: int = 4):\n    pipeline = TranscriptionPipeline()\n\n    with ProcessPoolExecutor(max_workers=workers) as executor:\n        results = executor.map(pipeline.process_single_file, files)\n\n    return list(results)\n</code></pre>"},{"location":"guide/overview/#stream-processing","title":"Stream Processing","text":"<pre><code>import asyncio\nfrom audio_aigented.pipeline import TranscriptionPipeline\n\nasync def process_stream(audio_stream):\n    pipeline = TranscriptionPipeline()\n\n    async for chunk in audio_stream:\n        result = await pipeline.process_chunk(chunk)\n        yield result\n</code></pre>"},{"location":"guide/transcription/","title":"Transcription","text":"<p>This guide covers the automatic speech recognition (ASR) component of the pipeline, which converts audio to text using NVIDIA NeMo models.</p>"},{"location":"guide/transcription/#overview","title":"Overview","text":"<p>The transcription module: - Uses state-of-the-art Conformer CTC models - Processes audio segments from diarization - Generates timestamped text with confidence scores - Supports GPU acceleration for fast processing</p>"},{"location":"guide/transcription/#asr-models","title":"ASR Models","text":""},{"location":"guide/transcription/#available-models","title":"Available Models","text":"Model Speed Accuracy Use Case <code>stt_en_conformer_ctc_small</code> Fast (15x real-time) Good Quick drafts, real-time <code>stt_en_conformer_ctc_medium</code> Medium (8x real-time) Better Balanced performance <code>stt_en_conformer_ctc_large</code> Slow (4x real-time) Best High accuracy needs"},{"location":"guide/transcription/#model-architecture","title":"Model Architecture","text":"<p>The Conformer models combine: - Convolutional layers: Capture local patterns - Self-attention: Model long-range dependencies - Feed-forward networks: Process representations - CTC decoder: Convert to text</p>"},{"location":"guide/transcription/#how-transcription-works","title":"How Transcription Works","text":""},{"location":"guide/transcription/#1-segment-processing","title":"1. Segment Processing","text":"<p>With diarization enabled: <pre><code># Process each speaker segment\nfor segment in diarization_segments:\n    audio_segment = extract_audio(segment.start, segment.end)\n    text = asr_model.transcribe(audio_segment)\n    results.append({\n        \"speaker\": segment.speaker,\n        \"text\": text,\n        \"start\": segment.start,\n        \"end\": segment.end\n    })\n</code></pre></p>"},{"location":"guide/transcription/#2-direct-transcription","title":"2. Direct Transcription","text":"<p>Without diarization: <pre><code># Process entire audio as one segment\nfull_text = asr_model.transcribe(audio_data)\nresults = [{\n    \"speaker\": \"SPEAKER_00\",\n    \"text\": full_text,\n    \"start\": 0.0,\n    \"end\": audio_duration\n}]\n</code></pre></p>"},{"location":"guide/transcription/#3-confidence-scoring","title":"3. Confidence Scoring","text":"<p>Each word includes confidence scores: <pre><code># Word-level confidence\ntranscription = asr_model.transcribe_with_confidence(audio)\nfor word in transcription.words:\n    print(f\"{word.text}: {word.confidence:.2f}\")\n</code></pre></p>"},{"location":"guide/transcription/#configuration","title":"Configuration","text":""},{"location":"guide/transcription/#basic-settings","title":"Basic Settings","text":"<pre><code>transcription:\n  model:\n    name: \"stt_en_conformer_ctc_large\"\n\n  device: \"cuda\"              # GPU acceleration\n  batch_size: 4               # Parallel processing\n\n  decoding:\n    beam_size: 10             # Beam search width\n    lm_weight: 0.0            # Language model weight\n    word_insertion_penalty: 0.0\n</code></pre>"},{"location":"guide/transcription/#advanced-options","title":"Advanced Options","text":"<pre><code>transcription:\n  preprocessing:\n    normalize_audio: true     # Normalize input levels\n    remove_noise: false       # Noise reduction\n\n  model:\n    compute_dtype: \"float16\"  # Mixed precision for speed\n\n  postprocessing:\n    punctuation: false        # Add punctuation (separate model)\n    capitalize: false         # Capitalize sentences\n    word_timestamps: true     # Include word timings\n</code></pre>"},{"location":"guide/transcription/#usage-examples","title":"Usage Examples","text":""},{"location":"guide/transcription/#command-line","title":"Command Line","text":"<pre><code># Default large model\nuv run python main.py --input-dir ./inputs\n\n# Fast processing with small model\nuv run python main.py --input-dir ./inputs \\\n  --model stt_en_conformer_ctc_small\n\n# CPU processing\nuv run python main.py --input-dir ./inputs --device cpu\n</code></pre>"},{"location":"guide/transcription/#python-api","title":"Python API","text":"<pre><code>from audio_aigented.transcription import Transcriber\nfrom audio_aigented.config import TranscriptionConfig\n\n# Configure transcriber\nconfig = TranscriptionConfig(\n    model_name=\"stt_en_conformer_ctc_medium\",\n    device=\"cuda\",\n    batch_size=8\n)\n\n# Initialize\ntranscriber = Transcriber(config)\n\n# Transcribe audio\nresult = transcriber.transcribe(audio_path)\nprint(result.text)\n\n# With timestamps\nfor segment in result.segments:\n    print(f\"[{segment.start:.1f}s - {segment.end:.1f}s] {segment.text}\")\n</code></pre>"},{"location":"guide/transcription/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple files efficiently\naudio_files = [\"file1.wav\", \"file2.wav\", \"file3.wav\"]\nresults = transcriber.transcribe_batch(audio_files)\n\nfor file, result in zip(audio_files, results):\n    print(f\"{file}: {result.text}\")\n</code></pre>"},{"location":"guide/transcription/#output-format","title":"Output Format","text":""},{"location":"guide/transcription/#segment-structure","title":"Segment Structure","text":"<pre><code>{\n  \"text\": \"Hello, how are you doing today?\",\n  \"start_time\": 0.5,\n  \"end_time\": 2.3,\n  \"confidence\": 0.95,\n  \"speaker_id\": \"SPEAKER_00\",\n  \"words\": [\n    {\"text\": \"Hello\", \"start\": 0.5, \"end\": 0.8, \"confidence\": 0.98},\n    {\"text\": \"how\", \"start\": 0.9, \"end\": 1.0, \"confidence\": 0.96},\n    {\"text\": \"are\", \"start\": 1.0, \"end\": 1.1, \"confidence\": 0.97},\n    {\"text\": \"you\", \"start\": 1.1, \"end\": 1.2, \"confidence\": 0.95},\n    {\"text\": \"doing\", \"start\": 1.3, \"end\": 1.5, \"confidence\": 0.94},\n    {\"text\": \"today\", \"start\": 1.6, \"end\": 1.9, \"confidence\": 0.93}\n  ]\n}\n</code></pre>"},{"location":"guide/transcription/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guide/transcription/#gpu-utilization","title":"GPU Utilization","text":"<p>Monitor GPU usage: <pre><code># During transcription\nwatch -n 1 nvidia-smi\n\n# Check memory usage\nnvidia-smi --query-gpu=memory.used,memory.total --format=csv\n</code></pre></p>"},{"location":"guide/transcription/#speed-vs-accuracy-trade-offs","title":"Speed vs Accuracy Trade-offs","text":"Setting Impact on Speed Impact on Accuracy Smaller model +++ -- Larger batch size ++ 0 FP16 precision ++ - Beam size reduction + - Disable word timestamps + 0"},{"location":"guide/transcription/#memory-management","title":"Memory Management","text":"<pre><code># For limited GPU memory\ntranscription:\n  batch_size: 1              # Process one at a time\n  model:\n    compute_dtype: \"float16\" # Use mixed precision\n\n  memory:\n    max_audio_length: 300    # Process in 5-minute chunks\n</code></pre>"},{"location":"guide/transcription/#quality-improvements","title":"Quality Improvements","text":""},{"location":"guide/transcription/#pre-processing","title":"Pre-processing","text":"<pre><code># Enhance audio before transcription\ndef preprocess_for_asr(audio, sample_rate):\n    # Normalize\n    audio = audio / np.max(np.abs(audio))\n\n    # High-pass filter for speech clarity\n    from scipy import signal\n    b, a = signal.butter(4, 80/(sample_rate/2), 'high')\n    audio = signal.filtfilt(b, a, audio)\n\n    return audio\n</code></pre>"},{"location":"guide/transcription/#post-processing","title":"Post-processing","text":"<pre><code># Clean up transcription output\ndef postprocess_text(text):\n    # Remove filler words\n    fillers = [\"um\", \"uh\", \"er\", \"ah\"]\n    words = text.split()\n    words = [w for w in words if w.lower() not in fillers]\n\n    # Fix common errors\n    replacements = {\n        \"gonna\": \"going to\",\n        \"wanna\": \"want to\",\n        \"kinda\": \"kind of\"\n    }\n\n    text = \" \".join(words)\n    for old, new in replacements.items():\n        text = text.replace(old, new)\n\n    return text\n</code></pre>"},{"location":"guide/transcription/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/transcription/#common-issues","title":"Common Issues","text":""},{"location":"guide/transcription/#low-accuracy","title":"Low Accuracy","text":"<p>Causes &amp; Solutions:</p> <ol> <li> <p>Poor audio quality <pre><code># Check audio statistics\nstats = analyze_audio(audio_path)\nif stats[\"snr\"] &lt; 10:  # Low signal-to-noise ratio\n    print(\"Warning: Audio quality may affect accuracy\")\n</code></pre></p> </li> <li> <p>Wrong model for content</p> </li> <li>Technical content \u2192 Use large model</li> <li>Casual speech \u2192 Medium model often sufficient</li> <li> <p>Non-native speakers \u2192 Consider specialized models</p> </li> <li> <p>Preprocessing needed <pre><code>transcription:\n  preprocessing:\n    normalize_audio: true\n    remove_noise: true\n</code></pre></p> </li> </ol>"},{"location":"guide/transcription/#slow-processing","title":"Slow Processing","text":"<p>Solutions:</p> <ol> <li> <p>Use smaller model <pre><code>uv run python main.py --model stt_en_conformer_ctc_small\n</code></pre></p> </li> <li> <p>Increase batch size <pre><code>transcription:\n  batch_size: 16  # If GPU memory allows\n</code></pre></p> </li> <li> <p>Enable mixed precision <pre><code>transcription:\n  model:\n    compute_dtype: \"float16\"\n</code></pre></p> </li> </ol>"},{"location":"guide/transcription/#out-of-memory","title":"Out of Memory","text":"<p>Solutions:</p> <ol> <li> <p>Reduce batch size <pre><code>transcription:\n  batch_size: 1\n</code></pre></p> </li> <li> <p>Process in chunks <pre><code># Split long audio\ndef transcribe_long_audio(audio_path, chunk_duration=300):\n    chunks = split_audio(audio_path, chunk_duration)\n    results = []\n    for chunk in chunks:\n        result = transcriber.transcribe(chunk)\n        results.append(result)\n    return merge_results(results)\n</code></pre></p> </li> </ol>"},{"location":"guide/transcription/#best-practices","title":"Best Practices","text":""},{"location":"guide/transcription/#model-selection","title":"Model Selection","text":"<ol> <li>Start with medium model - Good balance</li> <li>Use large for:</li> <li>Legal/medical transcription</li> <li>High-stakes content</li> <li>Final production</li> <li>Use small for:</li> <li>Drafts and previews</li> <li>Real-time processing</li> <li>High volume, low stakes</li> </ol>"},{"location":"guide/transcription/#audio-preparation","title":"Audio Preparation","text":"<ol> <li>Optimal recording:</li> <li>16kHz sample rate minimum</li> <li>Quiet environment</li> <li> <p>Good microphone placement</p> </li> <li> <p>Pre-screening:    <pre><code>def screen_audio(audio_path):\n    duration = get_duration(audio_path)\n    if duration &lt; 0.5:\n        return False, \"Too short\"\n    if duration &gt; 3600:\n        return False, \"Too long\"\n\n    stats = analyze_audio(audio_path)\n    if stats[\"silence_ratio\"] &gt; 0.9:\n        return False, \"Mostly silence\"\n\n    return True, \"OK\"\n</code></pre></p> </li> </ol>"},{"location":"guide/transcription/#production-deployment","title":"Production Deployment","text":"<ol> <li> <p>Model caching:    <pre><code># Load model once, reuse\nclass TranscriptionService:\n    def __init__(self):\n        self.model = load_model()\n\n    def transcribe(self, audio):\n        return self.model.transcribe(audio)\n</code></pre></p> </li> <li> <p>Queue management:    <pre><code># Process queue of files\nfrom queue import Queue\nfrom threading import Thread\n\ndef worker(queue, transcriber):\n    while True:\n        audio_path = queue.get()\n        result = transcriber.transcribe(audio_path)\n        save_result(result)\n        queue.task_done()\n</code></pre></p> </li> </ol>"},{"location":"guide/transcription/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guide/transcription/#custom-vocabulary","title":"Custom Vocabulary","text":"<pre><code># Add domain-specific terms\ncustom_vocab = [\"COVID-19\", \"mRNA\", \"blockchain\"]\ntranscriber.add_vocabulary(custom_vocab)\n</code></pre>"},{"location":"guide/transcription/#streaming-transcription","title":"Streaming Transcription","text":"<pre><code># Real-time transcription\nasync def stream_transcribe(audio_stream):\n    buffer_size = 16000  # 1 second at 16kHz\n    buffer = []\n\n    async for chunk in audio_stream:\n        buffer.extend(chunk)\n\n        if len(buffer) &gt;= buffer_size:\n            # Process buffer\n            text = transcriber.transcribe(buffer)\n            yield text\n\n            # Keep overlap for context\n            buffer = buffer[-buffer_size//4:]\n</code></pre>"},{"location":"guide/transcription/#multi-language-support","title":"Multi-language Support","text":"<pre><code># Detect language and select model\ndef transcribe_multilingual(audio_path):\n    language = detect_language(audio_path)\n\n    model_map = {\n        \"en\": \"stt_en_conformer_ctc_large\",\n        \"es\": \"stt_es_conformer_ctc_large\",\n        \"fr\": \"stt_fr_conformer_ctc_large\"\n    }\n\n    model_name = model_map.get(language, \"stt_en_conformer_ctc_large\")\n    transcriber = Transcriber(model_name=model_name)\n\n    return transcriber.transcribe(audio_path)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>audio_aigented<ul> <li>audio<ul> <li>loader</li> </ul> </li> <li>cache<ul> <li>manager</li> </ul> </li> <li>config<ul> <li>manager</li> </ul> </li> <li>diarization<ul> <li>config_builder</li> <li>diarizer</li> <li>rttm_parser</li> </ul> </li> <li>formatting<ul> <li>formatter</li> </ul> </li> <li>models<ul> <li>schemas</li> </ul> </li> <li>output<ul> <li>writer</li> </ul> </li> <li>pipeline</li> <li>transcription<ul> <li>asr</li> </ul> </li> <li>utils<ul> <li>decorators</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/audio_aigented/__init__/","title":"audio_aigented","text":""},{"location":"reference/audio_aigented/__init__/#audio_aigented","title":"audio_aigented","text":"<p>Audio Transcription Pipeline using NVIDIA NeMo</p> <p>A modular, GPU-accelerated audio processing pipeline for speech recognition.</p>"},{"location":"reference/audio_aigented/__init__/#audio_aigented-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline","title":"TranscriptionPipeline","text":"<pre><code>TranscriptionPipeline(\n    config: ProcessingConfig | None = None,\n    config_path: Path | None = None,\n)\n</code></pre> <p>Main orchestrator for the audio transcription pipeline.</p> <p>Coordinates audio loading, ASR processing, formatting, and output writing with comprehensive error handling and progress tracking.</p> <p>Initialize the transcription pipeline.</p> PARAMETER DESCRIPTION <code>config</code> <p>Optional ProcessingConfig instance</p> <p> TYPE: <code>ProcessingConfig | None</code> DEFAULT: <code>None</code> </p> <code>config_path</code> <p>Optional path to configuration file</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def __init__(self, config: ProcessingConfig | None = None, config_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Initialize the transcription pipeline.\n\n    Args:\n        config: Optional ProcessingConfig instance\n        config_path: Optional path to configuration file\n    \"\"\"\n    # Load configuration\n    if config is None:\n        config_manager = ConfigManager(config_path)\n        self.config = config_manager.load_config()\n    else:\n        self.config = config\n\n    # Initialize components\n    self.audio_loader = AudioLoader(self.config)\n    self.asr_transcriber = ASRTranscriber(self.config)\n    self.file_writer = FileWriter(self.config)\n\n    # Initialize cache manager\n    cache_enabled = self.config.processing.get(\"enable_caching\", True)\n    self.cache_manager = CacheManager(self.config.cache_dir, enabled=cache_enabled)\n\n    # Initialize diarizer if enabled\n    self.diarizer = None\n    if self.config.processing.get(\"enable_diarization\", True):\n        try:\n            from .diarization.diarizer import NeMoDiarizer\n            self.diarizer = NeMoDiarizer()\n            logger.info(\"Speaker diarization enabled\")\n        except Exception as e:\n            logger.warning(f\"Failed to initialize diarizer: {e}\")\n            logger.info(\"Continuing without speaker diarization\")\n            self.config.processing[\"enable_diarization\"] = False\n    else:\n        logger.info(\"Speaker diarization disabled\")\n\n    # Pipeline state\n    self.status = PipelineStatus()\n\n    # Setup logging\n    self._setup_logging()\n\n    logger.info(\"TranscriptionPipeline initialized successfully\")\n</code></pre>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.cache_stats","title":"cache_stats  <code>property</code>","text":"<pre><code>cache_stats: dict[str, int]\n</code></pre> <p>Get cache statistics.</p>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.pipeline_status","title":"pipeline_status  <code>property</code>","text":"<pre><code>pipeline_status: PipelineStatus\n</code></pre> <p>Get current pipeline processing status.</p>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.process_directory","title":"process_directory","text":"<pre><code>process_directory(\n    input_dir: Path | None = None,\n) -&gt; list[TranscriptionResult]\n</code></pre> <p>Process all audio files in a directory.</p> PARAMETER DESCRIPTION <code>input_dir</code> <p>Optional input directory. If None, uses config default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[TranscriptionResult]</code> <p>List of TranscriptionResult instances</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def process_directory(self, input_dir: Path | None = None) -&gt; list[TranscriptionResult]:\n    \"\"\"\n    Process all audio files in a directory.\n\n    Args:\n        input_dir: Optional input directory. If None, uses config default.\n\n    Returns:\n        List of TranscriptionResult instances\n    \"\"\"\n    search_dir = input_dir or self.config.input_dir\n    logger.info(f\"Starting directory processing: {search_dir}\")\n\n    # Discover audio files\n    audio_file_paths = self.audio_loader.discover_audio_files(search_dir)\n\n    if not audio_file_paths:\n        logger.warning(\"No audio files found to process\")\n        return []\n\n    # Process all files\n    return self.process_files(audio_file_paths)\n</code></pre>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.process_files","title":"process_files","text":"<pre><code>process_files(\n    audio_file_paths: list[Path],\n) -&gt; list[TranscriptionResult]\n</code></pre> <p>Process a list of audio files.</p> PARAMETER DESCRIPTION <code>audio_file_paths</code> <p>List of audio file paths to process</p> <p> TYPE: <code>list[Path]</code> </p> RETURNS DESCRIPTION <code>list[TranscriptionResult]</code> <p>List of TranscriptionResult instances</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def process_files(self, audio_file_paths: list[Path]) -&gt; list[TranscriptionResult]:\n    \"\"\"\n    Process a list of audio files.\n\n    Args:\n        audio_file_paths: List of audio file paths to process\n\n    Returns:\n        List of TranscriptionResult instances\n    \"\"\"\n    logger.info(f\"Processing {len(audio_file_paths)} audio files\")\n\n    # Initialize pipeline status\n    self.status = PipelineStatus(total_files=len(audio_file_paths))\n\n    results = []\n\n    # Process each file\n    for file_path in tqdm(audio_file_paths, desc=\"Processing audio files\"):\n        try:\n            self.status.current_file = file_path.name\n\n            # Check cache first\n            cached_result = self.cache_manager.get_cached_result(file_path)\n            if cached_result:\n                results.append(cached_result)\n                self.status.completed_files += 1\n                continue\n\n            result = self.process_single_file(file_path)\n            results.append(result)\n\n            # Cache the result\n            self.cache_manager.cache_result(file_path, result)\n\n            self.status.completed_files += 1\n\n        except Exception as e:\n            logger.error(f\"Failed to process {file_path}: {e}\")\n            self.status.failed_files += 1\n\n            # Create empty result for failed processing\n            try:\n                audio_file = AudioFile(path=file_path)\n                empty_result = TranscriptionResult(\n                    audio_file=audio_file,\n                    segments=[],\n                    full_text=\"\",\n                    processing_time=0.0,\n                    metadata={\"error\": str(e)}\n                )\n                results.append(empty_result)\n            except Exception:\n                # Skip if we can't even create an empty result\n                pass\n\n    # Write all results\n    self.file_writer.write_batch_results(results)\n\n    # Create summary report\n    if results:\n        self.file_writer.create_summary_report(results)\n\n    # Log final statistics\n    self._log_final_statistics(results)\n\n    return results\n</code></pre>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.process_single_file","title":"process_single_file","text":"<pre><code>process_single_file(file_path: Path) -&gt; TranscriptionResult\n</code></pre> <p>Process a single audio file through the complete pipeline.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>TranscriptionResult</code> <p>TranscriptionResult instance</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def process_single_file(self, file_path: Path) -&gt; TranscriptionResult:\n    \"\"\"\n    Process a single audio file through the complete pipeline.\n\n    Args:\n        file_path: Path to the audio file\n\n    Returns:\n        TranscriptionResult instance\n    \"\"\"\n    logger.info(f\"Processing file: {file_path.name}\")\n    start_time = time.time()\n\n    try:\n        # Stage 1: Load audio file\n        audio_file = self.audio_loader.load_audio_file(file_path)\n\n        # Validate audio file\n        if not self.audio_loader.validate_audio_file(audio_file):\n            raise ValueError(f\"Audio file validation failed: {file_path}\")\n\n        # Stage 2: Load audio data\n        audio_data, sample_rate = self.audio_loader.load_audio_data(audio_file)\n\n        # Stage 3: Perform diarization FIRST (if enabled)\n        speaker_segments = None\n        if self.config.processing.get(\"enable_diarization\", True) and self.diarizer is not None:\n            try:\n                logger.info(f\"Performing speaker diarization for {file_path.name}\")\n                speaker_segments = self.diarizer.diarize(audio_file)\n\n                if speaker_segments:\n                    # Log detailed speaker segment information\n                    unique_speakers = set(speaker for _, _, speaker in speaker_segments)\n                    logger.info(f\"Diarization found {len(speaker_segments)} segments with {len(unique_speakers)} unique speakers: {unique_speakers}\")\n\n                    # Log first few speaker segments for debugging\n                    for i, (start, end, speaker) in enumerate(speaker_segments[:5]):\n                        logger.debug(f\"Speaker segment {i}: {speaker} [{start:.2f}s - {end:.2f}s]\")\n                else:\n                    logger.warning(f\"No speaker segments detected for {file_path.name}\")\n\n            except Exception as e:\n                logger.warning(f\"Diarization failed for {file_path.name}: {e}\")\n                logger.info(\"Continuing without speaker identification\")\n                speaker_segments = None\n        else:\n            logger.debug(\"Diarization disabled - will assign single speaker\")\n\n        # Stage 4: Transcribe audio with speaker information\n        if speaker_segments:\n            # Transcribe with diarization - process each speaker segment\n            logger.info(f\"Transcribing {len(speaker_segments)} speaker segments\")\n            result = self._transcribe_with_speakers(audio_file, audio_data, speaker_segments)\n        else:\n            # Transcribe without diarization - assume single speaker\n            logger.info(\"Transcribing as single speaker\")\n            result = self.asr_transcriber.transcribe_audio_file(audio_file, audio_data)\n\n            # Assign default speaker to all segments\n            for segment in result.segments:\n                segment.speaker_id = \"SPEAKER_00\"\n\n            result.metadata[\"num_speakers\"] = 1\n            result.metadata[\"speaker_segments\"] = [\n                {\"start\": 0.0, \"end\": audio_file.duration or 0.0, \"speaker\": \"SPEAKER_00\"}\n            ]\n\n        # Stage 5: Write output\n        created_files = self.file_writer.write_transcription_result(result)\n\n        # Add file paths to metadata\n        result.metadata[\"output_files\"] = [str(f) for f in created_files]\n\n        total_time = time.time() - start_time\n        logger.info(f\"Successfully processed {file_path.name} in {total_time:.2f}s\")\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Pipeline failed for {file_path}: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache() -&gt; int\n</code></pre> <p>Clear all cached transcription results.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of cache files cleared</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def clear_cache(self) -&gt; int:\n    \"\"\"\n    Clear all cached transcription results.\n\n    Returns:\n        Number of cache files cleared\n    \"\"\"\n    return self.cache_manager.clear_cache()\n</code></pre>"},{"location":"reference/audio_aigented/__init__/#audio_aigented.TranscriptionPipeline.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; list[str]\n</code></pre> <p>Get list of supported audio formats.</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def get_supported_formats(self) -&gt; list[str]:\n    \"\"\"Get list of supported audio formats.\"\"\"\n    return [\"wav\"]  # Currently only supporting WAV files\n</code></pre>"},{"location":"reference/audio_aigented/pipeline/","title":"audio_aigented.pipeline","text":""},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline","title":"audio_aigented.pipeline","text":"<p>Main transcription pipeline orchestration.</p> <p>This module coordinates all components of the audio transcription system, providing a unified interface for processing audio files through the complete ASR pipeline with error handling, logging, and progress tracking.</p>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline","title":"TranscriptionPipeline","text":"<pre><code>TranscriptionPipeline(\n    config: ProcessingConfig | None = None,\n    config_path: Path | None = None,\n)\n</code></pre> <p>Main orchestrator for the audio transcription pipeline.</p> <p>Coordinates audio loading, ASR processing, formatting, and output writing with comprehensive error handling and progress tracking.</p> <p>Initialize the transcription pipeline.</p> PARAMETER DESCRIPTION <code>config</code> <p>Optional ProcessingConfig instance</p> <p> TYPE: <code>ProcessingConfig | None</code> DEFAULT: <code>None</code> </p> <code>config_path</code> <p>Optional path to configuration file</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def __init__(self, config: ProcessingConfig | None = None, config_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Initialize the transcription pipeline.\n\n    Args:\n        config: Optional ProcessingConfig instance\n        config_path: Optional path to configuration file\n    \"\"\"\n    # Load configuration\n    if config is None:\n        config_manager = ConfigManager(config_path)\n        self.config = config_manager.load_config()\n    else:\n        self.config = config\n\n    # Initialize components\n    self.audio_loader = AudioLoader(self.config)\n    self.asr_transcriber = ASRTranscriber(self.config)\n    self.file_writer = FileWriter(self.config)\n\n    # Initialize cache manager\n    cache_enabled = self.config.processing.get(\"enable_caching\", True)\n    self.cache_manager = CacheManager(self.config.cache_dir, enabled=cache_enabled)\n\n    # Initialize diarizer if enabled\n    self.diarizer = None\n    if self.config.processing.get(\"enable_diarization\", True):\n        try:\n            from .diarization.diarizer import NeMoDiarizer\n            self.diarizer = NeMoDiarizer()\n            logger.info(\"Speaker diarization enabled\")\n        except Exception as e:\n            logger.warning(f\"Failed to initialize diarizer: {e}\")\n            logger.info(\"Continuing without speaker diarization\")\n            self.config.processing[\"enable_diarization\"] = False\n    else:\n        logger.info(\"Speaker diarization disabled\")\n\n    # Pipeline state\n    self.status = PipelineStatus()\n\n    # Setup logging\n    self._setup_logging()\n\n    logger.info(\"TranscriptionPipeline initialized successfully\")\n</code></pre>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.cache_stats","title":"cache_stats  <code>property</code>","text":"<pre><code>cache_stats: dict[str, int]\n</code></pre> <p>Get cache statistics.</p>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.pipeline_status","title":"pipeline_status  <code>property</code>","text":"<pre><code>pipeline_status: PipelineStatus\n</code></pre> <p>Get current pipeline processing status.</p>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.process_directory","title":"process_directory","text":"<pre><code>process_directory(\n    input_dir: Path | None = None,\n) -&gt; list[TranscriptionResult]\n</code></pre> <p>Process all audio files in a directory.</p> PARAMETER DESCRIPTION <code>input_dir</code> <p>Optional input directory. If None, uses config default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[TranscriptionResult]</code> <p>List of TranscriptionResult instances</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def process_directory(self, input_dir: Path | None = None) -&gt; list[TranscriptionResult]:\n    \"\"\"\n    Process all audio files in a directory.\n\n    Args:\n        input_dir: Optional input directory. If None, uses config default.\n\n    Returns:\n        List of TranscriptionResult instances\n    \"\"\"\n    search_dir = input_dir or self.config.input_dir\n    logger.info(f\"Starting directory processing: {search_dir}\")\n\n    # Discover audio files\n    audio_file_paths = self.audio_loader.discover_audio_files(search_dir)\n\n    if not audio_file_paths:\n        logger.warning(\"No audio files found to process\")\n        return []\n\n    # Process all files\n    return self.process_files(audio_file_paths)\n</code></pre>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.process_files","title":"process_files","text":"<pre><code>process_files(\n    audio_file_paths: list[Path],\n) -&gt; list[TranscriptionResult]\n</code></pre> <p>Process a list of audio files.</p> PARAMETER DESCRIPTION <code>audio_file_paths</code> <p>List of audio file paths to process</p> <p> TYPE: <code>list[Path]</code> </p> RETURNS DESCRIPTION <code>list[TranscriptionResult]</code> <p>List of TranscriptionResult instances</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def process_files(self, audio_file_paths: list[Path]) -&gt; list[TranscriptionResult]:\n    \"\"\"\n    Process a list of audio files.\n\n    Args:\n        audio_file_paths: List of audio file paths to process\n\n    Returns:\n        List of TranscriptionResult instances\n    \"\"\"\n    logger.info(f\"Processing {len(audio_file_paths)} audio files\")\n\n    # Initialize pipeline status\n    self.status = PipelineStatus(total_files=len(audio_file_paths))\n\n    results = []\n\n    # Process each file\n    for file_path in tqdm(audio_file_paths, desc=\"Processing audio files\"):\n        try:\n            self.status.current_file = file_path.name\n\n            # Check cache first\n            cached_result = self.cache_manager.get_cached_result(file_path)\n            if cached_result:\n                results.append(cached_result)\n                self.status.completed_files += 1\n                continue\n\n            result = self.process_single_file(file_path)\n            results.append(result)\n\n            # Cache the result\n            self.cache_manager.cache_result(file_path, result)\n\n            self.status.completed_files += 1\n\n        except Exception as e:\n            logger.error(f\"Failed to process {file_path}: {e}\")\n            self.status.failed_files += 1\n\n            # Create empty result for failed processing\n            try:\n                audio_file = AudioFile(path=file_path)\n                empty_result = TranscriptionResult(\n                    audio_file=audio_file,\n                    segments=[],\n                    full_text=\"\",\n                    processing_time=0.0,\n                    metadata={\"error\": str(e)}\n                )\n                results.append(empty_result)\n            except Exception:\n                # Skip if we can't even create an empty result\n                pass\n\n    # Write all results\n    self.file_writer.write_batch_results(results)\n\n    # Create summary report\n    if results:\n        self.file_writer.create_summary_report(results)\n\n    # Log final statistics\n    self._log_final_statistics(results)\n\n    return results\n</code></pre>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.process_single_file","title":"process_single_file","text":"<pre><code>process_single_file(file_path: Path) -&gt; TranscriptionResult\n</code></pre> <p>Process a single audio file through the complete pipeline.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>TranscriptionResult</code> <p>TranscriptionResult instance</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def process_single_file(self, file_path: Path) -&gt; TranscriptionResult:\n    \"\"\"\n    Process a single audio file through the complete pipeline.\n\n    Args:\n        file_path: Path to the audio file\n\n    Returns:\n        TranscriptionResult instance\n    \"\"\"\n    logger.info(f\"Processing file: {file_path.name}\")\n    start_time = time.time()\n\n    try:\n        # Stage 1: Load audio file\n        audio_file = self.audio_loader.load_audio_file(file_path)\n\n        # Validate audio file\n        if not self.audio_loader.validate_audio_file(audio_file):\n            raise ValueError(f\"Audio file validation failed: {file_path}\")\n\n        # Stage 2: Load audio data\n        audio_data, sample_rate = self.audio_loader.load_audio_data(audio_file)\n\n        # Stage 3: Perform diarization FIRST (if enabled)\n        speaker_segments = None\n        if self.config.processing.get(\"enable_diarization\", True) and self.diarizer is not None:\n            try:\n                logger.info(f\"Performing speaker diarization for {file_path.name}\")\n                speaker_segments = self.diarizer.diarize(audio_file)\n\n                if speaker_segments:\n                    # Log detailed speaker segment information\n                    unique_speakers = set(speaker for _, _, speaker in speaker_segments)\n                    logger.info(f\"Diarization found {len(speaker_segments)} segments with {len(unique_speakers)} unique speakers: {unique_speakers}\")\n\n                    # Log first few speaker segments for debugging\n                    for i, (start, end, speaker) in enumerate(speaker_segments[:5]):\n                        logger.debug(f\"Speaker segment {i}: {speaker} [{start:.2f}s - {end:.2f}s]\")\n                else:\n                    logger.warning(f\"No speaker segments detected for {file_path.name}\")\n\n            except Exception as e:\n                logger.warning(f\"Diarization failed for {file_path.name}: {e}\")\n                logger.info(\"Continuing without speaker identification\")\n                speaker_segments = None\n        else:\n            logger.debug(\"Diarization disabled - will assign single speaker\")\n\n        # Stage 4: Transcribe audio with speaker information\n        if speaker_segments:\n            # Transcribe with diarization - process each speaker segment\n            logger.info(f\"Transcribing {len(speaker_segments)} speaker segments\")\n            result = self._transcribe_with_speakers(audio_file, audio_data, speaker_segments)\n        else:\n            # Transcribe without diarization - assume single speaker\n            logger.info(\"Transcribing as single speaker\")\n            result = self.asr_transcriber.transcribe_audio_file(audio_file, audio_data)\n\n            # Assign default speaker to all segments\n            for segment in result.segments:\n                segment.speaker_id = \"SPEAKER_00\"\n\n            result.metadata[\"num_speakers\"] = 1\n            result.metadata[\"speaker_segments\"] = [\n                {\"start\": 0.0, \"end\": audio_file.duration or 0.0, \"speaker\": \"SPEAKER_00\"}\n            ]\n\n        # Stage 5: Write output\n        created_files = self.file_writer.write_transcription_result(result)\n\n        # Add file paths to metadata\n        result.metadata[\"output_files\"] = [str(f) for f in created_files]\n\n        total_time = time.time() - start_time\n        logger.info(f\"Successfully processed {file_path.name} in {total_time:.2f}s\")\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Pipeline failed for {file_path}: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache() -&gt; int\n</code></pre> <p>Clear all cached transcription results.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of cache files cleared</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def clear_cache(self) -&gt; int:\n    \"\"\"\n    Clear all cached transcription results.\n\n    Returns:\n        Number of cache files cleared\n    \"\"\"\n    return self.cache_manager.clear_cache()\n</code></pre>"},{"location":"reference/audio_aigented/pipeline/#audio_aigented.pipeline.TranscriptionPipeline.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; list[str]\n</code></pre> <p>Get list of supported audio formats.</p> Source code in <code>src/audio_aigented/pipeline.py</code> <pre><code>def get_supported_formats(self) -&gt; list[str]:\n    \"\"\"Get list of supported audio formats.\"\"\"\n    return [\"wav\"]  # Currently only supporting WAV files\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/","title":"audio_aigented.audio","text":""},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio","title":"audio_aigented.audio","text":"<p>Audio loading and preprocessing module.</p>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader","title":"AudioLoader","text":"<pre><code>AudioLoader(config: ProcessingConfig)\n</code></pre> <p>Handles loading and preprocessing of audio files for ASR processing.</p> <p>Provides methods to load, validate, and prepare audio files with proper resampling and format conversion for NVIDIA NeMo compatibility.</p> <p>Initialize the AudioLoader.</p> PARAMETER DESCRIPTION <code>config</code> <p>Processing configuration containing audio settings</p> <p> TYPE: <code>ProcessingConfig</code> </p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def __init__(self, config: ProcessingConfig) -&gt; None:\n    \"\"\"\n    Initialize the AudioLoader.\n\n    Args:\n        config: Processing configuration containing audio settings\n    \"\"\"\n    self.config = config\n    self.target_sample_rate = config.audio[\"sample_rate\"]\n    # Reduce max duration to 10 seconds for better GPU memory management\n    self.max_duration = config.audio.get(\"max_duration\", 10.0)\n\n    logger.info(f\"AudioLoader initialized with sample rate: {self.target_sample_rate}\")\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader.discover_audio_files","title":"discover_audio_files","text":"<pre><code>discover_audio_files(\n    input_dir: Path | None = None,\n) -&gt; list[Path]\n</code></pre> <p>Discover all .wav audio files in the input directory.</p> PARAMETER DESCRIPTION <code>input_dir</code> <p>Optional input directory. If None, uses config default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Path]</code> <p>List of Path objects for discovered audio files</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def discover_audio_files(self, input_dir: Path | None = None) -&gt; list[Path]:\n    \"\"\"\n    Discover all .wav audio files in the input directory.\n\n    Args:\n        input_dir: Optional input directory. If None, uses config default.\n\n    Returns:\n        List of Path objects for discovered audio files\n    \"\"\"\n    search_dir = input_dir or self.config.input_dir\n\n    if not search_dir.exists():\n        logger.warning(f\"Input directory does not exist: {search_dir}\")\n        return []\n\n    # Find all .wav files\n    audio_files = list(search_dir.glob(\"*.wav\"))\n\n    if not audio_files:\n        logger.warning(f\"No .wav files found in {search_dir}\")\n\n    logger.info(f\"Discovered {len(audio_files)} audio files in {search_dir}\")\n    return audio_files\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader.load_audio_file","title":"load_audio_file","text":"<pre><code>load_audio_file(file_path: Path) -&gt; AudioFile\n</code></pre> <p>Load and validate an audio file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>AudioFile</code> <p>AudioFile instance with metadata</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If file format is not supported</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def load_audio_file(self, file_path: Path) -&gt; AudioFile:\n    \"\"\"\n    Load and validate an audio file.\n\n    Args:\n        file_path: Path to the audio file\n\n    Returns:\n        AudioFile instance with metadata\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If file format is not supported\n    \"\"\"\n    if not file_path.exists():\n        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n\n    try:\n        # Get audio info without loading the full file\n        info = sf.info(str(file_path))\n\n        audio_file = AudioFile(\n            path=file_path,\n            sample_rate=info.samplerate,\n            duration=info.duration,\n            channels=info.channels,\n            format=info.format.lower()\n        )\n\n        logger.debug(f\"Loaded audio file info: {file_path.name} \"\n                    f\"({info.duration:.2f}s, {info.samplerate}Hz, {info.channels}ch)\")\n\n        return audio_file\n\n    except Exception as e:\n        logger.error(f\"Failed to load audio file {file_path}: {e}\")\n        raise ValueError(f\"Unsupported audio file format: {file_path}\")\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader.load_audio_data","title":"load_audio_data","text":"<pre><code>load_audio_data(\n    audio_file: AudioFile,\n) -&gt; tuple[np.ndarray, int]\n</code></pre> <p>Load audio data and resample if necessary.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>AudioFile instance</p> <p> TYPE: <code>AudioFile</code> </p> RETURNS DESCRIPTION <code>tuple[ndarray, int]</code> <p>Tuple of (audio_data, sample_rate)</p> RAISES DESCRIPTION <code>ValueError</code> <p>If audio loading fails</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def load_audio_data(self, audio_file: AudioFile) -&gt; tuple[np.ndarray, int]:\n    \"\"\"\n    Load audio data and resample if necessary.\n\n    Args:\n        audio_file: AudioFile instance\n\n    Returns:\n        Tuple of (audio_data, sample_rate)\n\n    Raises:\n        ValueError: If audio loading fails\n    \"\"\"\n    try:\n        # Load audio data\n        audio_data, original_sr = librosa.load(\n            str(audio_file.path),\n            sr=None,  # Keep original sample rate initially\n            mono=True  # Convert to mono\n        )\n\n        # Resample if necessary\n        if original_sr != self.target_sample_rate:\n            logger.debug(f\"Resampling {audio_file.path.name} from \"\n                       f\"{original_sr}Hz to {self.target_sample_rate}Hz\")\n            audio_data = librosa.resample(\n                audio_data,\n                orig_sr=original_sr,\n                target_sr=self.target_sample_rate\n            )\n\n        # Normalize audio data\n        audio_data = self._normalize_audio(audio_data)\n\n        return audio_data, self.target_sample_rate\n\n    except Exception as e:\n        logger.error(f\"Failed to load audio data from {audio_file.path}: {e}\")\n        raise ValueError(f\"Failed to load audio data: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader.segment_audio","title":"segment_audio","text":"<pre><code>segment_audio(\n    audio_data: ndarray, sample_rate: int\n) -&gt; list[np.ndarray]\n</code></pre> <p>Segment long audio into chunks for processing.</p> PARAMETER DESCRIPTION <code>audio_data</code> <p>Audio data array</p> <p> TYPE: <code>ndarray</code> </p> <code>sample_rate</code> <p>Sample rate of the audio</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>list[ndarray]</code> <p>List of audio segments</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def segment_audio(self, audio_data: np.ndarray, sample_rate: int) -&gt; list[np.ndarray]:\n    \"\"\"\n    Segment long audio into chunks for processing.\n\n    Args:\n        audio_data: Audio data array\n        sample_rate: Sample rate of the audio\n\n    Returns:\n        List of audio segments\n    \"\"\"\n    duration = len(audio_data) / sample_rate\n\n    # If audio is shorter than max duration, return as single segment\n    if duration &lt;= self.max_duration:\n        return [audio_data]\n\n    # Calculate segment parameters\n    segment_samples = int(self.max_duration * sample_rate)\n    overlap_samples = int(0.1 * sample_rate)  # 100ms overlap\n\n    segments = []\n    start = 0\n\n    while start &lt; len(audio_data):\n        end = min(start + segment_samples, len(audio_data))\n        segment = audio_data[start:end]\n\n        # Only add segment if it's long enough\n        if len(segment) &gt; overlap_samples:\n            segments.append(segment)\n\n        start = end - overlap_samples\n\n        # Break if we've reached the end\n        if end &gt;= len(audio_data):\n            break\n\n    logger.debug(f\"Segmented audio into {len(segments)} chunks\")\n    return segments\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader.process_audio_files","title":"process_audio_files","text":"<pre><code>process_audio_files(\n    audio_files: list[Path],\n) -&gt; list[AudioFile]\n</code></pre> <p>Process multiple audio files and return AudioFile instances.</p> PARAMETER DESCRIPTION <code>audio_files</code> <p>List of audio file paths</p> <p> TYPE: <code>list[Path]</code> </p> RETURNS DESCRIPTION <code>list[AudioFile]</code> <p>List of validated AudioFile instances</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def process_audio_files(self, audio_files: list[Path]) -&gt; list[AudioFile]:\n    \"\"\"\n    Process multiple audio files and return AudioFile instances.\n\n    Args:\n        audio_files: List of audio file paths\n\n    Returns:\n        List of validated AudioFile instances\n    \"\"\"\n    processed_files = []\n\n    for file_path in tqdm(audio_files, desc=\"Loading audio files\"):\n        try:\n            audio_file = self.load_audio_file(file_path)\n            processed_files.append(audio_file)\n\n        except Exception as e:\n            logger.error(f\"Skipping {file_path}: {e}\")\n            continue\n\n    logger.info(f\"Successfully processed {len(processed_files)} audio files\")\n    return processed_files\n</code></pre>"},{"location":"reference/audio_aigented/audio/__init__/#audio_aigented.audio.AudioLoader.validate_audio_file","title":"validate_audio_file","text":"<pre><code>validate_audio_file(audio_file: AudioFile) -&gt; bool\n</code></pre> <p>Validate audio file for processing compatibility.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>AudioFile instance to validate</p> <p> TYPE: <code>AudioFile</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if file is valid for processing</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def validate_audio_file(self, audio_file: AudioFile) -&gt; bool:\n    \"\"\"\n    Validate audio file for processing compatibility.\n\n    Args:\n        audio_file: AudioFile instance to validate\n\n    Returns:\n        True if file is valid for processing\n    \"\"\"\n    try:\n        # Check file exists\n        if not audio_file.path.exists():\n            logger.error(f\"File does not exist: {audio_file.path}\")\n            return False\n\n        # Check duration\n        if audio_file.duration is None or audio_file.duration &lt;= 0:\n            logger.error(f\"Invalid duration: {audio_file.duration}\")\n            return False\n\n        # Check sample rate\n        if audio_file.sample_rate is None or audio_file.sample_rate &lt;= 0:\n            logger.error(f\"Invalid sample rate: {audio_file.sample_rate}\")\n            return False\n\n        # Check if file is too short (minimum 0.1 seconds)\n        if audio_file.duration &lt; 0.1:\n            logger.warning(f\"Audio file is very short: {audio_file.duration}s\")\n\n        return True\n\n    except Exception as e:\n        logger.error(f\"Validation failed for {audio_file.path}: {e}\")\n        return False\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/","title":"audio_aigented.audio.loader","text":""},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader","title":"audio_aigented.audio.loader","text":"<p>Audio file loading and preprocessing module.</p> <p>This module handles loading audio files, validation, resampling, and preparation for ASR processing with NVIDIA NeMo compatibility.</p>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader","title":"AudioLoader","text":"<pre><code>AudioLoader(config: ProcessingConfig)\n</code></pre> <p>Handles loading and preprocessing of audio files for ASR processing.</p> <p>Provides methods to load, validate, and prepare audio files with proper resampling and format conversion for NVIDIA NeMo compatibility.</p> <p>Initialize the AudioLoader.</p> PARAMETER DESCRIPTION <code>config</code> <p>Processing configuration containing audio settings</p> <p> TYPE: <code>ProcessingConfig</code> </p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def __init__(self, config: ProcessingConfig) -&gt; None:\n    \"\"\"\n    Initialize the AudioLoader.\n\n    Args:\n        config: Processing configuration containing audio settings\n    \"\"\"\n    self.config = config\n    self.target_sample_rate = config.audio[\"sample_rate\"]\n    # Reduce max duration to 10 seconds for better GPU memory management\n    self.max_duration = config.audio.get(\"max_duration\", 10.0)\n\n    logger.info(f\"AudioLoader initialized with sample rate: {self.target_sample_rate}\")\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader.discover_audio_files","title":"discover_audio_files","text":"<pre><code>discover_audio_files(\n    input_dir: Path | None = None,\n) -&gt; list[Path]\n</code></pre> <p>Discover all .wav audio files in the input directory.</p> PARAMETER DESCRIPTION <code>input_dir</code> <p>Optional input directory. If None, uses config default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Path]</code> <p>List of Path objects for discovered audio files</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def discover_audio_files(self, input_dir: Path | None = None) -&gt; list[Path]:\n    \"\"\"\n    Discover all .wav audio files in the input directory.\n\n    Args:\n        input_dir: Optional input directory. If None, uses config default.\n\n    Returns:\n        List of Path objects for discovered audio files\n    \"\"\"\n    search_dir = input_dir or self.config.input_dir\n\n    if not search_dir.exists():\n        logger.warning(f\"Input directory does not exist: {search_dir}\")\n        return []\n\n    # Find all .wav files\n    audio_files = list(search_dir.glob(\"*.wav\"))\n\n    if not audio_files:\n        logger.warning(f\"No .wav files found in {search_dir}\")\n\n    logger.info(f\"Discovered {len(audio_files)} audio files in {search_dir}\")\n    return audio_files\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader.load_audio_file","title":"load_audio_file","text":"<pre><code>load_audio_file(file_path: Path) -&gt; AudioFile\n</code></pre> <p>Load and validate an audio file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>AudioFile</code> <p>AudioFile instance with metadata</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If file format is not supported</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def load_audio_file(self, file_path: Path) -&gt; AudioFile:\n    \"\"\"\n    Load and validate an audio file.\n\n    Args:\n        file_path: Path to the audio file\n\n    Returns:\n        AudioFile instance with metadata\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If file format is not supported\n    \"\"\"\n    if not file_path.exists():\n        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n\n    try:\n        # Get audio info without loading the full file\n        info = sf.info(str(file_path))\n\n        audio_file = AudioFile(\n            path=file_path,\n            sample_rate=info.samplerate,\n            duration=info.duration,\n            channels=info.channels,\n            format=info.format.lower()\n        )\n\n        logger.debug(f\"Loaded audio file info: {file_path.name} \"\n                    f\"({info.duration:.2f}s, {info.samplerate}Hz, {info.channels}ch)\")\n\n        return audio_file\n\n    except Exception as e:\n        logger.error(f\"Failed to load audio file {file_path}: {e}\")\n        raise ValueError(f\"Unsupported audio file format: {file_path}\")\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader.load_audio_data","title":"load_audio_data","text":"<pre><code>load_audio_data(\n    audio_file: AudioFile,\n) -&gt; tuple[np.ndarray, int]\n</code></pre> <p>Load audio data and resample if necessary.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>AudioFile instance</p> <p> TYPE: <code>AudioFile</code> </p> RETURNS DESCRIPTION <code>tuple[ndarray, int]</code> <p>Tuple of (audio_data, sample_rate)</p> RAISES DESCRIPTION <code>ValueError</code> <p>If audio loading fails</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def load_audio_data(self, audio_file: AudioFile) -&gt; tuple[np.ndarray, int]:\n    \"\"\"\n    Load audio data and resample if necessary.\n\n    Args:\n        audio_file: AudioFile instance\n\n    Returns:\n        Tuple of (audio_data, sample_rate)\n\n    Raises:\n        ValueError: If audio loading fails\n    \"\"\"\n    try:\n        # Load audio data\n        audio_data, original_sr = librosa.load(\n            str(audio_file.path),\n            sr=None,  # Keep original sample rate initially\n            mono=True  # Convert to mono\n        )\n\n        # Resample if necessary\n        if original_sr != self.target_sample_rate:\n            logger.debug(f\"Resampling {audio_file.path.name} from \"\n                       f\"{original_sr}Hz to {self.target_sample_rate}Hz\")\n            audio_data = librosa.resample(\n                audio_data,\n                orig_sr=original_sr,\n                target_sr=self.target_sample_rate\n            )\n\n        # Normalize audio data\n        audio_data = self._normalize_audio(audio_data)\n\n        return audio_data, self.target_sample_rate\n\n    except Exception as e:\n        logger.error(f\"Failed to load audio data from {audio_file.path}: {e}\")\n        raise ValueError(f\"Failed to load audio data: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader.segment_audio","title":"segment_audio","text":"<pre><code>segment_audio(\n    audio_data: ndarray, sample_rate: int\n) -&gt; list[np.ndarray]\n</code></pre> <p>Segment long audio into chunks for processing.</p> PARAMETER DESCRIPTION <code>audio_data</code> <p>Audio data array</p> <p> TYPE: <code>ndarray</code> </p> <code>sample_rate</code> <p>Sample rate of the audio</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>list[ndarray]</code> <p>List of audio segments</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def segment_audio(self, audio_data: np.ndarray, sample_rate: int) -&gt; list[np.ndarray]:\n    \"\"\"\n    Segment long audio into chunks for processing.\n\n    Args:\n        audio_data: Audio data array\n        sample_rate: Sample rate of the audio\n\n    Returns:\n        List of audio segments\n    \"\"\"\n    duration = len(audio_data) / sample_rate\n\n    # If audio is shorter than max duration, return as single segment\n    if duration &lt;= self.max_duration:\n        return [audio_data]\n\n    # Calculate segment parameters\n    segment_samples = int(self.max_duration * sample_rate)\n    overlap_samples = int(0.1 * sample_rate)  # 100ms overlap\n\n    segments = []\n    start = 0\n\n    while start &lt; len(audio_data):\n        end = min(start + segment_samples, len(audio_data))\n        segment = audio_data[start:end]\n\n        # Only add segment if it's long enough\n        if len(segment) &gt; overlap_samples:\n            segments.append(segment)\n\n        start = end - overlap_samples\n\n        # Break if we've reached the end\n        if end &gt;= len(audio_data):\n            break\n\n    logger.debug(f\"Segmented audio into {len(segments)} chunks\")\n    return segments\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader.process_audio_files","title":"process_audio_files","text":"<pre><code>process_audio_files(\n    audio_files: list[Path],\n) -&gt; list[AudioFile]\n</code></pre> <p>Process multiple audio files and return AudioFile instances.</p> PARAMETER DESCRIPTION <code>audio_files</code> <p>List of audio file paths</p> <p> TYPE: <code>list[Path]</code> </p> RETURNS DESCRIPTION <code>list[AudioFile]</code> <p>List of validated AudioFile instances</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def process_audio_files(self, audio_files: list[Path]) -&gt; list[AudioFile]:\n    \"\"\"\n    Process multiple audio files and return AudioFile instances.\n\n    Args:\n        audio_files: List of audio file paths\n\n    Returns:\n        List of validated AudioFile instances\n    \"\"\"\n    processed_files = []\n\n    for file_path in tqdm(audio_files, desc=\"Loading audio files\"):\n        try:\n            audio_file = self.load_audio_file(file_path)\n            processed_files.append(audio_file)\n\n        except Exception as e:\n            logger.error(f\"Skipping {file_path}: {e}\")\n            continue\n\n    logger.info(f\"Successfully processed {len(processed_files)} audio files\")\n    return processed_files\n</code></pre>"},{"location":"reference/audio_aigented/audio/loader/#audio_aigented.audio.loader.AudioLoader.validate_audio_file","title":"validate_audio_file","text":"<pre><code>validate_audio_file(audio_file: AudioFile) -&gt; bool\n</code></pre> <p>Validate audio file for processing compatibility.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>AudioFile instance to validate</p> <p> TYPE: <code>AudioFile</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if file is valid for processing</p> Source code in <code>src/audio_aigented/audio/loader.py</code> <pre><code>def validate_audio_file(self, audio_file: AudioFile) -&gt; bool:\n    \"\"\"\n    Validate audio file for processing compatibility.\n\n    Args:\n        audio_file: AudioFile instance to validate\n\n    Returns:\n        True if file is valid for processing\n    \"\"\"\n    try:\n        # Check file exists\n        if not audio_file.path.exists():\n            logger.error(f\"File does not exist: {audio_file.path}\")\n            return False\n\n        # Check duration\n        if audio_file.duration is None or audio_file.duration &lt;= 0:\n            logger.error(f\"Invalid duration: {audio_file.duration}\")\n            return False\n\n        # Check sample rate\n        if audio_file.sample_rate is None or audio_file.sample_rate &lt;= 0:\n            logger.error(f\"Invalid sample rate: {audio_file.sample_rate}\")\n            return False\n\n        # Check if file is too short (minimum 0.1 seconds)\n        if audio_file.duration &lt; 0.1:\n            logger.warning(f\"Audio file is very short: {audio_file.duration}s\")\n\n        return True\n\n    except Exception as e:\n        logger.error(f\"Validation failed for {audio_file.path}: {e}\")\n        return False\n</code></pre>"},{"location":"reference/audio_aigented/cache/__init__/","title":"audio_aigented.cache","text":""},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache","title":"audio_aigented.cache","text":"<p>Cache management module.</p>"},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager","title":"CacheManager","text":"<pre><code>CacheManager(cache_dir: Path, enabled: bool = True)\n</code></pre> <p>Manages caching of transcription results.</p> <p>Provides methods to check, save, and load cached transcription results based on file metadata to avoid unnecessary reprocessing.</p> <p>Initialize the cache manager.</p> PARAMETER DESCRIPTION <code>cache_dir</code> <p>Directory for storing cache files</p> <p> TYPE: <code>Path</code> </p> <code>enabled</code> <p>Whether caching is enabled</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def __init__(self, cache_dir: Path, enabled: bool = True) -&gt; None:\n    \"\"\"\n    Initialize the cache manager.\n\n    Args:\n        cache_dir: Directory for storing cache files\n        enabled: Whether caching is enabled\n    \"\"\"\n    self.cache_dir = cache_dir\n    self.enabled = enabled\n\n    if self.enabled:\n        # Ensure cache directory exists\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Cache manager initialized with directory: {self.cache_dir}\")\n    else:\n        logger.info(\"Cache manager initialized (caching disabled)\")\n</code></pre>"},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager.cache_size","title":"cache_size  <code>property</code>","text":"<pre><code>cache_size: int\n</code></pre> <p>Get the number of cached results.</p>"},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager.cache_size_bytes","title":"cache_size_bytes  <code>property</code>","text":"<pre><code>cache_size_bytes: int\n</code></pre> <p>Get the total size of cache in bytes.</p>"},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager.get_cached_result","title":"get_cached_result","text":"<pre><code>get_cached_result(\n    file_path: Path,\n) -&gt; Optional[TranscriptionResult]\n</code></pre> <p>Retrieve cached transcription result if available.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>Optional[TranscriptionResult]</code> <p>Cached TranscriptionResult if available and valid, None otherwise</p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def get_cached_result(self, file_path: Path) -&gt; Optional[TranscriptionResult]:\n    \"\"\"\n    Retrieve cached transcription result if available.\n\n    Args:\n        file_path: Path to the audio file\n\n    Returns:\n        Cached TranscriptionResult if available and valid, None otherwise\n    \"\"\"\n    if not self.enabled:\n        return None\n\n    cache_key = self._generate_cache_key(file_path)\n    cache_path = self.cache_dir / f\"{cache_key}.json\"\n\n    if not cache_path.exists():\n        logger.debug(f\"No cache found for {file_path.name}\")\n        return None\n\n    try:\n        # Check if source file has been modified since cache was created\n        if self._is_cache_stale(file_path, cache_path):\n            logger.info(f\"Cache is stale for {file_path.name}, will reprocess\")\n            cache_path.unlink()  # Remove stale cache\n            return None\n\n        # Load cached result\n        result = self._load_from_cache(cache_path, file_path)\n        if result:\n            logger.info(f\"Loaded cached result for {file_path.name}\")\n            return result\n\n    except Exception as e:\n        logger.warning(f\"Failed to load cache for {file_path.name}: {e}\")\n        # Remove corrupted cache file\n        try:\n            cache_path.unlink()\n        except Exception:\n            pass\n\n    return None\n</code></pre>"},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager.cache_result","title":"cache_result","text":"<pre><code>cache_result(\n    file_path: Path, result: TranscriptionResult\n) -&gt; None\n</code></pre> <p>Save transcription result to cache.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> <code>result</code> <p>TranscriptionResult to cache</p> <p> TYPE: <code>TranscriptionResult</code> </p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def cache_result(self, file_path: Path, result: TranscriptionResult) -&gt; None:\n    \"\"\"\n    Save transcription result to cache.\n\n    Args:\n        file_path: Path to the audio file\n        result: TranscriptionResult to cache\n    \"\"\"\n    if not self.enabled:\n        return\n\n    cache_key = self._generate_cache_key(file_path)\n    cache_path = self.cache_dir / f\"{cache_key}.json\"\n\n    try:\n        self._save_to_cache(result, cache_path)\n        logger.debug(f\"Cached result for {file_path.name}\")\n    except Exception as e:\n        logger.warning(f\"Failed to cache result for {file_path.name}: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/cache/__init__/#audio_aigented.cache.CacheManager.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache() -&gt; int\n</code></pre> <p>Clear all cached results.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of cache files removed</p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def clear_cache(self) -&gt; int:\n    \"\"\"\n    Clear all cached results.\n\n    Returns:\n        Number of cache files removed\n    \"\"\"\n    if not self.enabled:\n        return 0\n\n    count = 0\n    try:\n        for cache_file in self.cache_dir.glob(\"*.json\"):\n            cache_file.unlink()\n            count += 1\n\n        logger.info(f\"Cleared {count} cache files\")\n        return count\n\n    except Exception as e:\n        logger.error(f\"Failed to clear cache: {e}\")\n        return count\n</code></pre>"},{"location":"reference/audio_aigented/cache/manager/","title":"audio_aigented.cache.manager","text":""},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager","title":"audio_aigented.cache.manager","text":"<p>Cache management for transcription results.</p> <p>This module provides caching functionality to avoid reprocessing audio files that have already been transcribed.</p>"},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager","title":"CacheManager","text":"<pre><code>CacheManager(cache_dir: Path, enabled: bool = True)\n</code></pre> <p>Manages caching of transcription results.</p> <p>Provides methods to check, save, and load cached transcription results based on file metadata to avoid unnecessary reprocessing.</p> <p>Initialize the cache manager.</p> PARAMETER DESCRIPTION <code>cache_dir</code> <p>Directory for storing cache files</p> <p> TYPE: <code>Path</code> </p> <code>enabled</code> <p>Whether caching is enabled</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def __init__(self, cache_dir: Path, enabled: bool = True) -&gt; None:\n    \"\"\"\n    Initialize the cache manager.\n\n    Args:\n        cache_dir: Directory for storing cache files\n        enabled: Whether caching is enabled\n    \"\"\"\n    self.cache_dir = cache_dir\n    self.enabled = enabled\n\n    if self.enabled:\n        # Ensure cache directory exists\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Cache manager initialized with directory: {self.cache_dir}\")\n    else:\n        logger.info(\"Cache manager initialized (caching disabled)\")\n</code></pre>"},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager.cache_size","title":"cache_size  <code>property</code>","text":"<pre><code>cache_size: int\n</code></pre> <p>Get the number of cached results.</p>"},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager.cache_size_bytes","title":"cache_size_bytes  <code>property</code>","text":"<pre><code>cache_size_bytes: int\n</code></pre> <p>Get the total size of cache in bytes.</p>"},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager.get_cached_result","title":"get_cached_result","text":"<pre><code>get_cached_result(\n    file_path: Path,\n) -&gt; Optional[TranscriptionResult]\n</code></pre> <p>Retrieve cached transcription result if available.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>Optional[TranscriptionResult]</code> <p>Cached TranscriptionResult if available and valid, None otherwise</p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def get_cached_result(self, file_path: Path) -&gt; Optional[TranscriptionResult]:\n    \"\"\"\n    Retrieve cached transcription result if available.\n\n    Args:\n        file_path: Path to the audio file\n\n    Returns:\n        Cached TranscriptionResult if available and valid, None otherwise\n    \"\"\"\n    if not self.enabled:\n        return None\n\n    cache_key = self._generate_cache_key(file_path)\n    cache_path = self.cache_dir / f\"{cache_key}.json\"\n\n    if not cache_path.exists():\n        logger.debug(f\"No cache found for {file_path.name}\")\n        return None\n\n    try:\n        # Check if source file has been modified since cache was created\n        if self._is_cache_stale(file_path, cache_path):\n            logger.info(f\"Cache is stale for {file_path.name}, will reprocess\")\n            cache_path.unlink()  # Remove stale cache\n            return None\n\n        # Load cached result\n        result = self._load_from_cache(cache_path, file_path)\n        if result:\n            logger.info(f\"Loaded cached result for {file_path.name}\")\n            return result\n\n    except Exception as e:\n        logger.warning(f\"Failed to load cache for {file_path.name}: {e}\")\n        # Remove corrupted cache file\n        try:\n            cache_path.unlink()\n        except Exception:\n            pass\n\n    return None\n</code></pre>"},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager.cache_result","title":"cache_result","text":"<pre><code>cache_result(\n    file_path: Path, result: TranscriptionResult\n) -&gt; None\n</code></pre> <p>Save transcription result to cache.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> <code>result</code> <p>TranscriptionResult to cache</p> <p> TYPE: <code>TranscriptionResult</code> </p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def cache_result(self, file_path: Path, result: TranscriptionResult) -&gt; None:\n    \"\"\"\n    Save transcription result to cache.\n\n    Args:\n        file_path: Path to the audio file\n        result: TranscriptionResult to cache\n    \"\"\"\n    if not self.enabled:\n        return\n\n    cache_key = self._generate_cache_key(file_path)\n    cache_path = self.cache_dir / f\"{cache_key}.json\"\n\n    try:\n        self._save_to_cache(result, cache_path)\n        logger.debug(f\"Cached result for {file_path.name}\")\n    except Exception as e:\n        logger.warning(f\"Failed to cache result for {file_path.name}: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/cache/manager/#audio_aigented.cache.manager.CacheManager.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache() -&gt; int\n</code></pre> <p>Clear all cached results.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of cache files removed</p> Source code in <code>src/audio_aigented/cache/manager.py</code> <pre><code>def clear_cache(self) -&gt; int:\n    \"\"\"\n    Clear all cached results.\n\n    Returns:\n        Number of cache files removed\n    \"\"\"\n    if not self.enabled:\n        return 0\n\n    count = 0\n    try:\n        for cache_file in self.cache_dir.glob(\"*.json\"):\n            cache_file.unlink()\n            count += 1\n\n        logger.info(f\"Cleared {count} cache files\")\n        return count\n\n    except Exception as e:\n        logger.error(f\"Failed to clear cache: {e}\")\n        return count\n</code></pre>"},{"location":"reference/audio_aigented/config/__init__/","title":"audio_aigented.config","text":""},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config","title":"audio_aigented.config","text":"<p>Configuration management module.</p>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager","title":"ConfigManager","text":"<pre><code>ConfigManager(config_path: Path | None = None)\n</code></pre> <p>Manages application configuration using OmegaConf and Pydantic validation.</p> <p>Provides methods to load, validate, and access configuration settings from YAML files with proper type checking and defaults.</p> <p>Initialize the configuration manager.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to configuration file. If None, uses default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def __init__(self, config_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Initialize the configuration manager.\n\n    Args:\n        config_path: Optional path to configuration file. If None, uses default.\n    \"\"\"\n    self._config_path = config_path or Path(\"config/default.yaml\")\n    self._config: DictConfig | None = None\n    self._validated_config: ProcessingConfig | None = None\n</code></pre>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager.config_path","title":"config_path  <code>property</code>","text":"<pre><code>config_path: Path\n</code></pre> <p>Get the current configuration file path.</p>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager.is_loaded","title":"is_loaded  <code>property</code>","text":"<pre><code>is_loaded: bool\n</code></pre> <p>Check if configuration has been loaded.</p>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager.load_config","title":"load_config","text":"<pre><code>load_config(\n    config_path: Path | None = None,\n) -&gt; ProcessingConfig\n</code></pre> <p>Load and validate configuration from YAML file.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to configuration file</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Validated ProcessingConfig instance</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If config file doesn't exist</p> <code>ValidationError</code> <p>If config validation fails</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def load_config(self, config_path: Path | None = None) -&gt; ProcessingConfig:\n    \"\"\"\n    Load and validate configuration from YAML file.\n\n    Args:\n        config_path: Optional path to configuration file\n\n    Returns:\n        Validated ProcessingConfig instance\n\n    Raises:\n        FileNotFoundError: If config file doesn't exist\n        ValidationError: If config validation fails\n    \"\"\"\n    if config_path:\n        self._config_path = config_path\n\n    # Load default configuration first\n    default_config = self._get_default_config()\n\n    # Load user configuration if file exists\n    user_config = {}\n    if self._config_path.exists():\n        logger.info(f\"Loading configuration from {self._config_path}\")\n        user_config = OmegaConf.load(self._config_path)\n    else:\n        logger.warning(f\"Config file not found: {self._config_path}. Using defaults.\")\n\n    # Merge configurations (user overrides defaults)\n    self._config = OmegaConf.merge(default_config, user_config)\n\n    # Convert to regular dict for Pydantic validation\n    config_dict = OmegaConf.to_container(self._config, resolve=True)\n\n    try:\n        # Validate with Pydantic\n        self._validated_config = ProcessingConfig(**config_dict)\n        logger.info(\"Configuration loaded and validated successfully\")\n        return self._validated_config\n\n    except ValidationError as e:\n        logger.error(f\"Configuration validation failed: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; ProcessingConfig\n</code></pre> <p>Get the current validated configuration.</p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Current ProcessingConfig instance</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If configuration hasn't been loaded yet</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def get_config(self) -&gt; ProcessingConfig:\n    \"\"\"\n    Get the current validated configuration.\n\n    Returns:\n        Current ProcessingConfig instance\n\n    Raises:\n        RuntimeError: If configuration hasn't been loaded yet\n    \"\"\"\n    if self._validated_config is None:\n        raise RuntimeError(\"Configuration not loaded. Call load_config() first.\")\n    return self._validated_config\n</code></pre>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager.save_config","title":"save_config","text":"<pre><code>save_config(\n    config: ProcessingConfig,\n    output_path: Path | None = None,\n) -&gt; None\n</code></pre> <p>Save configuration to YAML file.</p> PARAMETER DESCRIPTION <code>config</code> <p>ProcessingConfig instance to save</p> <p> TYPE: <code>ProcessingConfig</code> </p> <code>output_path</code> <p>Optional output path. If None, uses current config path.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def save_config(self, config: ProcessingConfig, output_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Save configuration to YAML file.\n\n    Args:\n        config: ProcessingConfig instance to save\n        output_path: Optional output path. If None, uses current config path.\n    \"\"\"\n    save_path = output_path or self._config_path\n\n    # Convert Pydantic model to dict\n    config_dict = config.model_dump()\n\n    # Convert Path objects to strings for YAML serialization\n    config_dict = self._paths_to_strings(config_dict)\n\n    # Create OmegaConf and save\n    omega_config = OmegaConf.create(config_dict)\n\n    # Ensure directory exists\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(save_path, 'w') as f:\n        OmegaConf.save(omega_config, f)\n\n    logger.info(f\"Configuration saved to {save_path}\")\n</code></pre>"},{"location":"reference/audio_aigented/config/__init__/#audio_aigented.config.ConfigManager.update_config","title":"update_config","text":"<pre><code>update_config(updates: dict[str, Any]) -&gt; ProcessingConfig\n</code></pre> <p>Update current configuration with new values.</p> PARAMETER DESCRIPTION <code>updates</code> <p>Dictionary of configuration updates</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Updated ProcessingConfig instance</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def update_config(self, updates: dict[str, Any]) -&gt; ProcessingConfig:\n    \"\"\"\n    Update current configuration with new values.\n\n    Args:\n        updates: Dictionary of configuration updates\n\n    Returns:\n        Updated ProcessingConfig instance\n    \"\"\"\n    if self._validated_config is None:\n        raise RuntimeError(\"Configuration not loaded. Call load_config() first.\")\n\n    # Get current config as dict\n    current_dict = self._validated_config.model_dump()\n\n    # Apply updates using OmegaConf merge\n    current_omega = OmegaConf.create(current_dict)\n    updates_omega = OmegaConf.create(updates)\n    merged = OmegaConf.merge(current_omega, updates_omega)\n\n    # Validate updated configuration\n    updated_dict = OmegaConf.to_container(merged, resolve=True)\n    self._validated_config = ProcessingConfig(**updated_dict)\n\n    logger.info(\"Configuration updated successfully\")\n    return self._validated_config\n</code></pre>"},{"location":"reference/audio_aigented/config/manager/","title":"audio_aigented.config.manager","text":""},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager","title":"audio_aigented.config.manager","text":"<p>Configuration management using OmegaConf for YAML-based settings.</p> <p>This module provides centralized configuration management with validation, defaults, and easy access to settings throughout the application.</p>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager","title":"ConfigManager","text":"<pre><code>ConfigManager(config_path: Path | None = None)\n</code></pre> <p>Manages application configuration using OmegaConf and Pydantic validation.</p> <p>Provides methods to load, validate, and access configuration settings from YAML files with proper type checking and defaults.</p> <p>Initialize the configuration manager.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to configuration file. If None, uses default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def __init__(self, config_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Initialize the configuration manager.\n\n    Args:\n        config_path: Optional path to configuration file. If None, uses default.\n    \"\"\"\n    self._config_path = config_path or Path(\"config/default.yaml\")\n    self._config: DictConfig | None = None\n    self._validated_config: ProcessingConfig | None = None\n</code></pre>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager.config_path","title":"config_path  <code>property</code>","text":"<pre><code>config_path: Path\n</code></pre> <p>Get the current configuration file path.</p>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager.is_loaded","title":"is_loaded  <code>property</code>","text":"<pre><code>is_loaded: bool\n</code></pre> <p>Check if configuration has been loaded.</p>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager.load_config","title":"load_config","text":"<pre><code>load_config(\n    config_path: Path | None = None,\n) -&gt; ProcessingConfig\n</code></pre> <p>Load and validate configuration from YAML file.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to configuration file</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Validated ProcessingConfig instance</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If config file doesn't exist</p> <code>ValidationError</code> <p>If config validation fails</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def load_config(self, config_path: Path | None = None) -&gt; ProcessingConfig:\n    \"\"\"\n    Load and validate configuration from YAML file.\n\n    Args:\n        config_path: Optional path to configuration file\n\n    Returns:\n        Validated ProcessingConfig instance\n\n    Raises:\n        FileNotFoundError: If config file doesn't exist\n        ValidationError: If config validation fails\n    \"\"\"\n    if config_path:\n        self._config_path = config_path\n\n    # Load default configuration first\n    default_config = self._get_default_config()\n\n    # Load user configuration if file exists\n    user_config = {}\n    if self._config_path.exists():\n        logger.info(f\"Loading configuration from {self._config_path}\")\n        user_config = OmegaConf.load(self._config_path)\n    else:\n        logger.warning(f\"Config file not found: {self._config_path}. Using defaults.\")\n\n    # Merge configurations (user overrides defaults)\n    self._config = OmegaConf.merge(default_config, user_config)\n\n    # Convert to regular dict for Pydantic validation\n    config_dict = OmegaConf.to_container(self._config, resolve=True)\n\n    try:\n        # Validate with Pydantic\n        self._validated_config = ProcessingConfig(**config_dict)\n        logger.info(\"Configuration loaded and validated successfully\")\n        return self._validated_config\n\n    except ValidationError as e:\n        logger.error(f\"Configuration validation failed: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; ProcessingConfig\n</code></pre> <p>Get the current validated configuration.</p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Current ProcessingConfig instance</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If configuration hasn't been loaded yet</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def get_config(self) -&gt; ProcessingConfig:\n    \"\"\"\n    Get the current validated configuration.\n\n    Returns:\n        Current ProcessingConfig instance\n\n    Raises:\n        RuntimeError: If configuration hasn't been loaded yet\n    \"\"\"\n    if self._validated_config is None:\n        raise RuntimeError(\"Configuration not loaded. Call load_config() first.\")\n    return self._validated_config\n</code></pre>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager.save_config","title":"save_config","text":"<pre><code>save_config(\n    config: ProcessingConfig,\n    output_path: Path | None = None,\n) -&gt; None\n</code></pre> <p>Save configuration to YAML file.</p> PARAMETER DESCRIPTION <code>config</code> <p>ProcessingConfig instance to save</p> <p> TYPE: <code>ProcessingConfig</code> </p> <code>output_path</code> <p>Optional output path. If None, uses current config path.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def save_config(self, config: ProcessingConfig, output_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Save configuration to YAML file.\n\n    Args:\n        config: ProcessingConfig instance to save\n        output_path: Optional output path. If None, uses current config path.\n    \"\"\"\n    save_path = output_path or self._config_path\n\n    # Convert Pydantic model to dict\n    config_dict = config.model_dump()\n\n    # Convert Path objects to strings for YAML serialization\n    config_dict = self._paths_to_strings(config_dict)\n\n    # Create OmegaConf and save\n    omega_config = OmegaConf.create(config_dict)\n\n    # Ensure directory exists\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(save_path, 'w') as f:\n        OmegaConf.save(omega_config, f)\n\n    logger.info(f\"Configuration saved to {save_path}\")\n</code></pre>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.ConfigManager.update_config","title":"update_config","text":"<pre><code>update_config(updates: dict[str, Any]) -&gt; ProcessingConfig\n</code></pre> <p>Update current configuration with new values.</p> PARAMETER DESCRIPTION <code>updates</code> <p>Dictionary of configuration updates</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Updated ProcessingConfig instance</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def update_config(self, updates: dict[str, Any]) -&gt; ProcessingConfig:\n    \"\"\"\n    Update current configuration with new values.\n\n    Args:\n        updates: Dictionary of configuration updates\n\n    Returns:\n        Updated ProcessingConfig instance\n    \"\"\"\n    if self._validated_config is None:\n        raise RuntimeError(\"Configuration not loaded. Call load_config() first.\")\n\n    # Get current config as dict\n    current_dict = self._validated_config.model_dump()\n\n    # Apply updates using OmegaConf merge\n    current_omega = OmegaConf.create(current_dict)\n    updates_omega = OmegaConf.create(updates)\n    merged = OmegaConf.merge(current_omega, updates_omega)\n\n    # Validate updated configuration\n    updated_dict = OmegaConf.to_container(merged, resolve=True)\n    self._validated_config = ProcessingConfig(**updated_dict)\n\n    logger.info(\"Configuration updated successfully\")\n    return self._validated_config\n</code></pre>"},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/config/manager/#audio_aigented.config.manager.load_config","title":"load_config","text":"<pre><code>load_config(\n    config_path: Path | None = None,\n) -&gt; ProcessingConfig\n</code></pre> <p>Convenience function to load configuration.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to configuration file</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ProcessingConfig</code> <p>Validated ProcessingConfig instance</p> Source code in <code>src/audio_aigented/config/manager.py</code> <pre><code>def load_config(config_path: Path | None = None) -&gt; ProcessingConfig:\n    \"\"\"\n    Convenience function to load configuration.\n\n    Args:\n        config_path: Optional path to configuration file\n\n    Returns:\n        Validated ProcessingConfig instance\n    \"\"\"\n    manager = ConfigManager(config_path)\n    return manager.load_config()\n</code></pre>"},{"location":"reference/audio_aigented/diarization/__init__/","title":"audio_aigented.diarization","text":""},{"location":"reference/audio_aigented/diarization/__init__/#audio_aigented.diarization","title":"audio_aigented.diarization","text":"<p>Speaker diarization module.</p>"},{"location":"reference/audio_aigented/diarization/__init__/#audio_aigented.diarization-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/diarization/__init__/#audio_aigented.diarization.NeMoDiarizer","title":"NeMoDiarizer","text":"<pre><code>NeMoDiarizer(\n    config_path: Path | None = None,\n    collar: float = 0.25,\n    merge_segments: bool = True,\n    min_segment_duration: float = 0.1,\n)\n</code></pre> <p>Speaker diarization using NVIDIA NeMo.</p> <p>Initialize the NeMo diarizer.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to diarization config file.         Defaults to config/diarization_config.yaml</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>collar</code> <p>Time collar in seconds for segment boundaries</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.25</code> </p> <code>merge_segments</code> <p>Whether to merge adjacent segments from same speaker</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_segment_duration</code> <p>Minimum segment duration to keep</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> Source code in <code>src/audio_aigented/diarization/diarizer.py</code> <pre><code>def __init__(self, config_path: Path | None = None, \n             collar: float = 0.25,\n             merge_segments: bool = True,\n             min_segment_duration: float = 0.1):\n    \"\"\"\n    Initialize the NeMo diarizer.\n\n    Args:\n        config_path: Optional path to diarization config file.\n                    Defaults to config/diarization_config.yaml\n        collar: Time collar in seconds for segment boundaries\n        merge_segments: Whether to merge adjacent segments from same speaker\n        min_segment_duration: Minimum segment duration to keep\n    \"\"\"\n    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    logger.info(f\"Initializing NeMo Diarizer on device: {self.device}\")\n\n    # Set default config path\n    if config_path is None:\n        project_root = Path(__file__).parent.parent.parent.parent\n        config_path = project_root / \"config\" / \"diarization_config.yaml\"\n\n    self.config_path = config_path\n    self.collar = collar\n    self.merge_segments = merge_segments\n    self.min_segment_duration = min_segment_duration\n\n    # Initialize helper components\n    self.config_builder = DiarizationConfigBuilder(config_path)\n    self.rttm_parser = RTTMParser(collar=collar)\n\n    logger.debug(f\"Diarization config path: {self.config_path}\")\n</code></pre>"},{"location":"reference/audio_aigented/diarization/__init__/#audio_aigented.diarization.NeMoDiarizer-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/diarization/__init__/#audio_aigented.diarization.NeMoDiarizer.diarize","title":"diarize","text":"<pre><code>diarize(\n    audio_file: AudioFile,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Perform speaker diarization on the given audio file.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>The audio file to diarize.</p> <p> TYPE: <code>AudioFile</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>A list of tuples, where each tuple contains the start time, end time,</p> <code>List[Tuple[float, float, str]]</code> <p>and speaker ID for each segment, sorted by start time.</p> Source code in <code>src/audio_aigented/diarization/diarizer.py</code> <pre><code>@retry_on_error(max_attempts=3, delay=1.0, exceptions=(RuntimeError, ValueError))\ndef diarize(self, audio_file: AudioFile) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Perform speaker diarization on the given audio file.\n\n    Args:\n        audio_file: The audio file to diarize.\n\n    Returns:\n        A list of tuples, where each tuple contains the start time, end time,\n        and speaker ID for each segment, sorted by start time.\n    \"\"\"\n    try:\n        logger.info(f\"Starting diarization for {audio_file.path.name}\")\n\n        # Validate audio file\n        if not audio_file.path.exists():\n            logger.error(f\"Audio file not found: {audio_file.path}\")\n            return []\n\n        # Validate audio file is actually a WAV file\n        if audio_file.path.suffix.lower() not in ['.wav', '.wave']:\n            logger.warning(f\"Audio file may not be in WAV format: {audio_file.path}\")\n            # Continue anyway as NeMo might handle other formats\n\n        # Create temporary output directory for diarization results\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Prepare diarization\n            manifest_path, cfg = self._prepare_diarization(\n                audio_file, Path(temp_dir)\n            )\n\n            # Initialize and run diarizer\n            diarizer = self._initialize_diarizer(cfg)\n\n            # Run diarization\n            logger.debug(f\"Running diarization on {audio_file.path}\")\n            diarizer.diarize()\n\n            # Parse results\n            speaker_segments = self._parse_results(\n                Path(temp_dir), audio_file\n            )\n\n            # Post-process segments if needed\n            if speaker_segments:\n                if self.merge_segments:\n                    speaker_segments = self.rttm_parser.merge_adjacent_segments(\n                        speaker_segments\n                    )\n\n                if self.min_segment_duration &gt; 0:\n                    speaker_segments = self.rttm_parser.filter_short_segments(\n                        speaker_segments, self.min_segment_duration\n                    )\n\n            return speaker_segments\n\n    except Exception as e:\n        logger.error(f\"Diarization failed for {audio_file.path.name}: {e}\")\n        logger.error(f\"Error type: {type(e).__name__}\")\n        logger.error(f\"Error details: {str(e)}\")\n\n        # Provide specific guidance for common errors\n        if \"ConfigAttributeError\" in str(type(e).__name__) or \"Key\" in str(e) and \"is not in struct\" in str(e):\n            logger.error(\"This appears to be a configuration error. Please check that all required \"\n                       \"parameters are present in the diarization config file.\")\n            if \"smoothing\" in str(e):\n                logger.error(\"The 'smoothing' parameter should be set to 'false' or a valid smoothing method.\")\n\n        import traceback\n        logger.debug(f\"Full traceback:\\n{traceback.format_exc()}\")\n        return []\n</code></pre>"},{"location":"reference/audio_aigented/diarization/__init__/#audio_aigented.diarization.NeMoDiarizer.get_speaker_mapping","title":"get_speaker_mapping","text":"<pre><code>get_speaker_mapping(\n    speaker_segments: List[Tuple[float, float, str]],\n    speaker_mapping: Optional[Dict[str, str]] = None,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Apply custom speaker name mapping to diarization results.</p> PARAMETER DESCRIPTION <code>speaker_segments</code> <p>List of (start, end, speaker_id) tuples</p> <p> TYPE: <code>List[Tuple[float, float, str]]</code> </p> <code>speaker_mapping</code> <p>Dict mapping speaker IDs to custom names</p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>List of segments with mapped speaker names</p> Source code in <code>src/audio_aigented/diarization/diarizer.py</code> <pre><code>def get_speaker_mapping(self, speaker_segments: List[Tuple[float, float, str]],\n                      speaker_mapping: Optional[Dict[str, str]] = None) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Apply custom speaker name mapping to diarization results.\n\n    Args:\n        speaker_segments: List of (start, end, speaker_id) tuples\n        speaker_mapping: Dict mapping speaker IDs to custom names\n\n    Returns:\n        List of segments with mapped speaker names\n    \"\"\"\n    if not speaker_mapping:\n        return speaker_segments\n\n    mapped_segments = []\n    for start, end, speaker_id in speaker_segments:\n        mapped_name = speaker_mapping.get(speaker_id, speaker_id)\n        mapped_segments.append((start, end, mapped_name))\n\n    return mapped_segments\n</code></pre>"},{"location":"reference/audio_aigented/diarization/config_builder/","title":"audio_aigented.diarization.config_builder","text":""},{"location":"reference/audio_aigented/diarization/config_builder/#audio_aigented.diarization.config_builder","title":"audio_aigented.diarization.config_builder","text":"<p>Configuration builder for NVIDIA NeMo diarization.</p> <p>This module provides a clean interface for building and validating diarization configurations with sensible defaults.</p>"},{"location":"reference/audio_aigented/diarization/config_builder/#audio_aigented.diarization.config_builder-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/diarization/config_builder/#audio_aigented.diarization.config_builder.DiarizationConfigBuilder","title":"DiarizationConfigBuilder","text":"<pre><code>DiarizationConfigBuilder(\n    base_config_path: Path | None = None,\n)\n</code></pre> <p>Builder for creating validated NeMo diarization configurations.</p> <p>Handles the complexity of NeMo configuration requirements and provides sensible defaults for common use cases.</p> <p>Initialize the configuration builder.</p> PARAMETER DESCRIPTION <code>base_config_path</code> <p>Path to base configuration YAML file</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/audio_aigented/diarization/config_builder.py</code> <pre><code>def __init__(self, base_config_path: Path | None = None) -&gt; None:\n    \"\"\"\n    Initialize the configuration builder.\n\n    Args:\n        base_config_path: Path to base configuration YAML file\n    \"\"\"\n    self.base_config_path = base_config_path\n    self.device = \"cuda\" if self._cuda_available() else \"cpu\"\n</code></pre>"},{"location":"reference/audio_aigented/diarization/config_builder/#audio_aigented.diarization.config_builder.DiarizationConfigBuilder-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/diarization/config_builder/#audio_aigented.diarization.config_builder.DiarizationConfigBuilder.build_config","title":"build_config","text":"<pre><code>build_config(\n    manifest_path: Path,\n    output_dir: Path,\n    audio_file_path: Path,\n    duration: float,\n) -&gt; DictConfig\n</code></pre> <p>Build a complete diarization configuration.</p> PARAMETER DESCRIPTION <code>manifest_path</code> <p>Path to manifest JSON file</p> <p> TYPE: <code>Path</code> </p> <code>output_dir</code> <p>Directory for diarization outputs</p> <p> TYPE: <code>Path</code> </p> <code>audio_file_path</code> <p>Path to the audio file being processed</p> <p> TYPE: <code>Path</code> </p> <code>duration</code> <p>Duration of the audio file in seconds</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>DictConfig</code> <p>Complete OmegaConf configuration for diarization</p> Source code in <code>src/audio_aigented/diarization/config_builder.py</code> <pre><code>def build_config(self, manifest_path: Path, output_dir: Path, \n                audio_file_path: Path, duration: float) -&gt; DictConfig:\n    \"\"\"\n    Build a complete diarization configuration.\n\n    Args:\n        manifest_path: Path to manifest JSON file\n        output_dir: Directory for diarization outputs\n        audio_file_path: Path to the audio file being processed\n        duration: Duration of the audio file in seconds\n\n    Returns:\n        Complete OmegaConf configuration for diarization\n    \"\"\"\n    # Load base config if provided\n    if self.base_config_path and self.base_config_path.exists():\n        base_cfg = OmegaConf.load(self.base_config_path)\n        cfg_dict = OmegaConf.to_container(base_cfg, resolve=True)\n    else:\n        # Use minimal default configuration\n        cfg_dict = self._get_default_config()\n\n    # Update with runtime parameters\n    cfg_dict['diarizer']['manifest_filepath'] = str(manifest_path)\n    cfg_dict['diarizer']['out_dir'] = str(output_dir)\n\n    # Add device configuration\n    cfg_dict = self._add_device_config(cfg_dict)\n\n    # Validate and fix common issues\n    cfg_dict = self._validate_and_fix_config(cfg_dict)\n\n    # Create OmegaConf\n    cfg = OmegaConf.create(cfg_dict)\n\n    logger.debug(f\"Built diarization config with keys: {list(cfg.keys())}\")\n    return cfg\n</code></pre>"},{"location":"reference/audio_aigented/diarization/config_builder/#audio_aigented.diarization.config_builder.DiarizationConfigBuilder.create_manifest_entry","title":"create_manifest_entry","text":"<pre><code>create_manifest_entry(\n    audio_path: Path, duration: float\n) -&gt; Dict[str, Any]\n</code></pre> <p>Create a manifest entry for the audio file.</p> PARAMETER DESCRIPTION <code>audio_path</code> <p>Path to audio file</p> <p> TYPE: <code>Path</code> </p> <code>duration</code> <p>Duration in seconds</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Manifest entry dictionary</p> Source code in <code>src/audio_aigented/diarization/config_builder.py</code> <pre><code>def create_manifest_entry(self, audio_path: Path, duration: float) -&gt; Dict[str, Any]:\n    \"\"\"\n    Create a manifest entry for the audio file.\n\n    Args:\n        audio_path: Path to audio file\n        duration: Duration in seconds\n\n    Returns:\n        Manifest entry dictionary\n    \"\"\"\n    return {\n        \"audio_filepath\": str(audio_path),\n        \"duration\": duration if duration else 100.0,  # Default fallback\n        \"label\": \"infer\",\n        \"text\": \"-\",\n        \"offset\": 0.0\n    }\n</code></pre>"},{"location":"reference/audio_aigented/diarization/diarizer/","title":"audio_aigented.diarization.diarizer","text":""},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer","title":"audio_aigented.diarization.diarizer","text":"<p>Speaker diarization using NVIDIA NeMo.</p>"},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer.NeMoDiarizer","title":"NeMoDiarizer","text":"<pre><code>NeMoDiarizer(\n    config_path: Path | None = None,\n    collar: float = 0.25,\n    merge_segments: bool = True,\n    min_segment_duration: float = 0.1,\n)\n</code></pre> <p>Speaker diarization using NVIDIA NeMo.</p> <p>Initialize the NeMo diarizer.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Optional path to diarization config file.         Defaults to config/diarization_config.yaml</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>collar</code> <p>Time collar in seconds for segment boundaries</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.25</code> </p> <code>merge_segments</code> <p>Whether to merge adjacent segments from same speaker</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_segment_duration</code> <p>Minimum segment duration to keep</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> Source code in <code>src/audio_aigented/diarization/diarizer.py</code> <pre><code>def __init__(self, config_path: Path | None = None, \n             collar: float = 0.25,\n             merge_segments: bool = True,\n             min_segment_duration: float = 0.1):\n    \"\"\"\n    Initialize the NeMo diarizer.\n\n    Args:\n        config_path: Optional path to diarization config file.\n                    Defaults to config/diarization_config.yaml\n        collar: Time collar in seconds for segment boundaries\n        merge_segments: Whether to merge adjacent segments from same speaker\n        min_segment_duration: Minimum segment duration to keep\n    \"\"\"\n    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    logger.info(f\"Initializing NeMo Diarizer on device: {self.device}\")\n\n    # Set default config path\n    if config_path is None:\n        project_root = Path(__file__).parent.parent.parent.parent\n        config_path = project_root / \"config\" / \"diarization_config.yaml\"\n\n    self.config_path = config_path\n    self.collar = collar\n    self.merge_segments = merge_segments\n    self.min_segment_duration = min_segment_duration\n\n    # Initialize helper components\n    self.config_builder = DiarizationConfigBuilder(config_path)\n    self.rttm_parser = RTTMParser(collar=collar)\n\n    logger.debug(f\"Diarization config path: {self.config_path}\")\n</code></pre>"},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer.NeMoDiarizer-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer.NeMoDiarizer.diarize","title":"diarize","text":"<pre><code>diarize(\n    audio_file: AudioFile,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Perform speaker diarization on the given audio file.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>The audio file to diarize.</p> <p> TYPE: <code>AudioFile</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>A list of tuples, where each tuple contains the start time, end time,</p> <code>List[Tuple[float, float, str]]</code> <p>and speaker ID for each segment, sorted by start time.</p> Source code in <code>src/audio_aigented/diarization/diarizer.py</code> <pre><code>@retry_on_error(max_attempts=3, delay=1.0, exceptions=(RuntimeError, ValueError))\ndef diarize(self, audio_file: AudioFile) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Perform speaker diarization on the given audio file.\n\n    Args:\n        audio_file: The audio file to diarize.\n\n    Returns:\n        A list of tuples, where each tuple contains the start time, end time,\n        and speaker ID for each segment, sorted by start time.\n    \"\"\"\n    try:\n        logger.info(f\"Starting diarization for {audio_file.path.name}\")\n\n        # Validate audio file\n        if not audio_file.path.exists():\n            logger.error(f\"Audio file not found: {audio_file.path}\")\n            return []\n\n        # Validate audio file is actually a WAV file\n        if audio_file.path.suffix.lower() not in ['.wav', '.wave']:\n            logger.warning(f\"Audio file may not be in WAV format: {audio_file.path}\")\n            # Continue anyway as NeMo might handle other formats\n\n        # Create temporary output directory for diarization results\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Prepare diarization\n            manifest_path, cfg = self._prepare_diarization(\n                audio_file, Path(temp_dir)\n            )\n\n            # Initialize and run diarizer\n            diarizer = self._initialize_diarizer(cfg)\n\n            # Run diarization\n            logger.debug(f\"Running diarization on {audio_file.path}\")\n            diarizer.diarize()\n\n            # Parse results\n            speaker_segments = self._parse_results(\n                Path(temp_dir), audio_file\n            )\n\n            # Post-process segments if needed\n            if speaker_segments:\n                if self.merge_segments:\n                    speaker_segments = self.rttm_parser.merge_adjacent_segments(\n                        speaker_segments\n                    )\n\n                if self.min_segment_duration &gt; 0:\n                    speaker_segments = self.rttm_parser.filter_short_segments(\n                        speaker_segments, self.min_segment_duration\n                    )\n\n            return speaker_segments\n\n    except Exception as e:\n        logger.error(f\"Diarization failed for {audio_file.path.name}: {e}\")\n        logger.error(f\"Error type: {type(e).__name__}\")\n        logger.error(f\"Error details: {str(e)}\")\n\n        # Provide specific guidance for common errors\n        if \"ConfigAttributeError\" in str(type(e).__name__) or \"Key\" in str(e) and \"is not in struct\" in str(e):\n            logger.error(\"This appears to be a configuration error. Please check that all required \"\n                       \"parameters are present in the diarization config file.\")\n            if \"smoothing\" in str(e):\n                logger.error(\"The 'smoothing' parameter should be set to 'false' or a valid smoothing method.\")\n\n        import traceback\n        logger.debug(f\"Full traceback:\\n{traceback.format_exc()}\")\n        return []\n</code></pre>"},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer.NeMoDiarizer.get_speaker_mapping","title":"get_speaker_mapping","text":"<pre><code>get_speaker_mapping(\n    speaker_segments: List[Tuple[float, float, str]],\n    speaker_mapping: Optional[Dict[str, str]] = None,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Apply custom speaker name mapping to diarization results.</p> PARAMETER DESCRIPTION <code>speaker_segments</code> <p>List of (start, end, speaker_id) tuples</p> <p> TYPE: <code>List[Tuple[float, float, str]]</code> </p> <code>speaker_mapping</code> <p>Dict mapping speaker IDs to custom names</p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>List of segments with mapped speaker names</p> Source code in <code>src/audio_aigented/diarization/diarizer.py</code> <pre><code>def get_speaker_mapping(self, speaker_segments: List[Tuple[float, float, str]],\n                      speaker_mapping: Optional[Dict[str, str]] = None) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Apply custom speaker name mapping to diarization results.\n\n    Args:\n        speaker_segments: List of (start, end, speaker_id) tuples\n        speaker_mapping: Dict mapping speaker IDs to custom names\n\n    Returns:\n        List of segments with mapped speaker names\n    \"\"\"\n    if not speaker_mapping:\n        return speaker_segments\n\n    mapped_segments = []\n    for start, end, speaker_id in speaker_segments:\n        mapped_name = speaker_mapping.get(speaker_id, speaker_id)\n        mapped_segments.append((start, end, mapped_name))\n\n    return mapped_segments\n</code></pre>"},{"location":"reference/audio_aigented/diarization/diarizer/#audio_aigented.diarization.diarizer-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/diarization/rttm_parser/","title":"audio_aigented.diarization.rttm_parser","text":""},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser","title":"audio_aigented.diarization.rttm_parser","text":"<p>RTTM (Rich Transcription Time Marked) file parser.</p> <p>This module handles parsing of RTTM format files produced by speaker diarization systems, with robust handling of various edge cases.</p>"},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser.RTTMParser","title":"RTTMParser","text":"<pre><code>RTTMParser(collar: float = 0.25)\n</code></pre> <p>Parser for RTTM (Rich Transcription Time Marked) format files.</p> <p>Handles parsing of speaker diarization output with support for filenames containing spaces and various RTTM format variations.</p> <p>Initialize the RTTM parser.</p> PARAMETER DESCRIPTION <code>collar</code> <p>Time collar in seconds for segment boundaries</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.25</code> </p> Source code in <code>src/audio_aigented/diarization/rttm_parser.py</code> <pre><code>def __init__(self, collar: float = 0.25) -&gt; None:\n    \"\"\"\n    Initialize the RTTM parser.\n\n    Args:\n        collar: Time collar in seconds for segment boundaries\n    \"\"\"\n    self.collar = collar\n</code></pre>"},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser.RTTMParser-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser.RTTMParser.parse_rttm_file","title":"parse_rttm_file","text":"<pre><code>parse_rttm_file(\n    rttm_path: Path,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Parse an RTTM file and extract speaker segments.</p> PARAMETER DESCRIPTION <code>rttm_path</code> <p>Path to the RTTM file</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>List of (start_time, end_time, speaker_id) tuples, sorted by start time</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If RTTM file doesn't exist</p> <code>ValueError</code> <p>If RTTM file is malformed</p> Source code in <code>src/audio_aigented/diarization/rttm_parser.py</code> <pre><code>def parse_rttm_file(self, rttm_path: Path) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Parse an RTTM file and extract speaker segments.\n\n    Args:\n        rttm_path: Path to the RTTM file\n\n    Returns:\n        List of (start_time, end_time, speaker_id) tuples, sorted by start time\n\n    Raises:\n        FileNotFoundError: If RTTM file doesn't exist\n        ValueError: If RTTM file is malformed\n    \"\"\"\n    if not rttm_path.exists():\n        raise FileNotFoundError(f\"RTTM file not found: {rttm_path}\")\n\n    logger.info(f\"Parsing RTTM file: {rttm_path}\")\n\n    segments = []\n\n    try:\n        with open(rttm_path, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n\n        logger.debug(f\"RTTM file has {len(lines)} lines\")\n\n        for line_num, line in enumerate(lines, 1):\n            line = line.strip()\n            if not line:  # Skip empty lines\n                continue\n\n            # Log first few lines for debugging\n            if line_num &lt;= 3:\n                logger.debug(f\"RTTM line {line_num}: {line}\")\n\n            segment = self._parse_rttm_line(line, line_num)\n            if segment:\n                segments.append(segment)\n\n    except Exception as e:\n        logger.error(f\"Failed to parse RTTM file: {e}\")\n        raise ValueError(f\"Failed to parse RTTM file: {e}\")\n\n    # Sort segments by start time\n    segments.sort(key=lambda x: x[0])\n\n    # Log summary\n    if segments:\n        unique_speakers = set(speaker for _, _, speaker in segments)\n        logger.info(f\"Parsed {len(segments)} segments with {len(unique_speakers)} speakers: {unique_speakers}\")\n    else:\n        logger.warning(\"No valid segments found in RTTM file\")\n\n    return segments\n</code></pre>"},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser.RTTMParser.merge_adjacent_segments","title":"merge_adjacent_segments","text":"<pre><code>merge_adjacent_segments(\n    segments: List[Tuple[float, float, str]],\n    max_gap: float = 0.5,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Merge adjacent segments from the same speaker.</p> PARAMETER DESCRIPTION <code>segments</code> <p>List of (start, end, speaker_id) tuples</p> <p> TYPE: <code>List[Tuple[float, float, str]]</code> </p> <code>max_gap</code> <p>Maximum gap in seconds to merge segments</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>Merged segments list</p> Source code in <code>src/audio_aigented/diarization/rttm_parser.py</code> <pre><code>def merge_adjacent_segments(self, segments: List[Tuple[float, float, str]], \n                           max_gap: float = 0.5) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Merge adjacent segments from the same speaker.\n\n    Args:\n        segments: List of (start, end, speaker_id) tuples\n        max_gap: Maximum gap in seconds to merge segments\n\n    Returns:\n        Merged segments list\n    \"\"\"\n    if not segments:\n        return segments\n\n    # Sort by start time\n    segments = sorted(segments, key=lambda x: x[0])\n\n    merged = []\n    current_start, current_end, current_speaker = segments[0]\n\n    for start, end, speaker in segments[1:]:\n        # Check if we can merge with current segment\n        if speaker == current_speaker and start - current_end &lt;= max_gap:\n            # Extend current segment\n            current_end = max(current_end, end)\n        else:\n            # Save current segment and start new one\n            merged.append((current_start, current_end, current_speaker))\n            current_start, current_end, current_speaker = start, end, speaker\n\n    # Don't forget the last segment\n    merged.append((current_start, current_end, current_speaker))\n\n    logger.debug(f\"Merged {len(segments)} segments into {len(merged)} segments\")\n    return merged\n</code></pre>"},{"location":"reference/audio_aigented/diarization/rttm_parser/#audio_aigented.diarization.rttm_parser.RTTMParser.filter_short_segments","title":"filter_short_segments","text":"<pre><code>filter_short_segments(\n    segments: List[Tuple[float, float, str]],\n    min_duration: float = 0.1,\n) -&gt; List[Tuple[float, float, str]]\n</code></pre> <p>Filter out segments shorter than minimum duration.</p> PARAMETER DESCRIPTION <code>segments</code> <p>List of (start, end, speaker_id) tuples</p> <p> TYPE: <code>List[Tuple[float, float, str]]</code> </p> <code>min_duration</code> <p>Minimum segment duration in seconds</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> RETURNS DESCRIPTION <code>List[Tuple[float, float, str]]</code> <p>Filtered segments list</p> Source code in <code>src/audio_aigented/diarization/rttm_parser.py</code> <pre><code>def filter_short_segments(self, segments: List[Tuple[float, float, str]], \n                         min_duration: float = 0.1) -&gt; List[Tuple[float, float, str]]:\n    \"\"\"\n    Filter out segments shorter than minimum duration.\n\n    Args:\n        segments: List of (start, end, speaker_id) tuples\n        min_duration: Minimum segment duration in seconds\n\n    Returns:\n        Filtered segments list\n    \"\"\"\n    filtered = []\n\n    for start, end, speaker in segments:\n        duration = end - start\n        if duration &gt;= min_duration:\n            filtered.append((start, end, speaker))\n        else:\n            logger.debug(f\"Filtered out short segment: {speaker} [{start:.2f}-{end:.2f}] ({duration:.2f}s)\")\n\n    if len(filtered) &lt; len(segments):\n        logger.info(f\"Filtered {len(segments) - len(filtered)} short segments\")\n\n    return filtered\n</code></pre>"},{"location":"reference/audio_aigented/formatting/__init__/","title":"audio_aigented.formatting","text":""},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting","title":"audio_aigented.formatting","text":"<p>Output formatting and structuring module.</p>"},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting.OutputFormatter","title":"OutputFormatter","text":"<pre><code>OutputFormatter(\n    include_timestamps: bool = True,\n    include_confidence: bool = True,\n    pretty_json: bool = True,\n)\n</code></pre> <p>Formats transcription results for output in various formats.</p> <p>Handles conversion to JSON and text formats with configurable options for timestamps, confidence scores, and formatting.</p> <p>Initialize the output formatter.</p> PARAMETER DESCRIPTION <code>include_timestamps</code> <p>Include timing information in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>include_confidence</code> <p>Include confidence scores in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>pretty_json</code> <p>Format JSON with indentation</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def __init__(self, include_timestamps: bool = True, include_confidence: bool = True, pretty_json: bool = True) -&gt; None:\n    \"\"\"\n    Initialize the output formatter.\n\n    Args:\n        include_timestamps: Include timing information in output\n        include_confidence: Include confidence scores in output\n        pretty_json: Format JSON with indentation\n    \"\"\"\n    self.include_timestamps = include_timestamps\n    self.include_confidence = include_confidence\n    self.pretty_json = pretty_json\n</code></pre>"},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting.OutputFormatter-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting.OutputFormatter.format_as_json","title":"format_as_json","text":"<pre><code>format_as_json(result: TranscriptionResult) -&gt; str\n</code></pre> <p>Format transcription result as JSON.</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to format</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>str</code> <p>JSON string representation</p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def format_as_json(self, result: TranscriptionResult) -&gt; str:\n    \"\"\"\n    Format transcription result as JSON.\n\n    Args:\n        result: TranscriptionResult to format\n\n    Returns:\n        JSON string representation\n    \"\"\"\n    # Create output dictionary\n    output = {\n        \"audio_file\": {\n            \"path\": str(result.audio_file.path),\n            \"duration\": result.audio_file.duration,\n            \"sample_rate\": result.audio_file.sample_rate,\n            \"channels\": result.audio_file.channels,\n            \"format\": result.audio_file.format\n        },\n        \"transcription\": {\n            \"full_text\": result.full_text,\n            \"segments\": self._format_segments_for_json(result.segments)\n        },\n        \"processing\": {\n            \"processing_time\": result.processing_time,\n            \"timestamp\": result.timestamp.isoformat(),\n            \"model_info\": result.model_info\n        },\n        \"metadata\": result.metadata\n    }\n\n    # Format JSON\n    if self.pretty_json:\n        return json.dumps(output, indent=2, ensure_ascii=False)\n    else:\n        return json.dumps(output, ensure_ascii=False)\n</code></pre>"},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting.OutputFormatter.format_as_text","title":"format_as_text","text":"<pre><code>format_as_text(result: TranscriptionResult) -&gt; str\n</code></pre> <p>Format transcription result as human-readable text.</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to format</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Formatted text string</p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def format_as_text(self, result: TranscriptionResult) -&gt; str:\n    \"\"\"\n    Format transcription result as human-readable text.\n\n    Args:\n        result: TranscriptionResult to format\n\n    Returns:\n        Formatted text string\n    \"\"\"\n    lines = []\n\n    # DIAGNOSTIC: Log result object state\n    logger.info(f\"DIAGNOSTIC - Formatting result for: {result.audio_file.path.name}\")\n    logger.info(f\"DIAGNOSTIC - Audio file duration: {result.audio_file.duration} (type: {type(result.audio_file.duration)})\")\n    logger.info(f\"DIAGNOSTIC - Processing time: {result.processing_time} (type: {type(result.processing_time)})\")\n    logger.info(f\"DIAGNOSTIC - Full text length: {len(result.full_text) if result.full_text else 'None'}\")\n    logger.info(f\"DIAGNOSTIC - Segments count: {len(result.segments)}\")\n\n    # Header\n    lines.append(\"=\" * 60)\n    lines.append(\"AUDIO TRANSCRIPTION REPORT\")\n    lines.append(\"=\" * 60)\n    lines.append(f\"File: {result.audio_file.path.name}\")\n\n    # Safe formatting with None checks\n    duration = result.audio_file.duration\n    if duration is not None:\n        lines.append(f\"Duration: {duration:.2f} seconds\")\n    else:\n        lines.append(\"Duration: Unknown\")\n\n    lines.append(f\"Processed: {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n    if result.processing_time:\n        speed_ratio = result.metadata.get('processing_speed_ratio', 0)\n        if speed_ratio is not None:\n            lines.append(f\"Processing Time: {result.processing_time:.2f}s (Speed: {speed_ratio:.2f}x)\")\n        else:\n            lines.append(f\"Processing Time: {result.processing_time:.2f}s\")\n\n    lines.append(\"-\" * 60)\n\n    # Full transcription\n    lines.append(\"FULL TRANSCRIPTION:\")\n    lines.append(\"\")\n    lines.append(result.full_text)\n    lines.append(\"\")\n\n    # Detailed segments (if timestamps enabled)\n    if self.include_timestamps and result.segments:\n        lines.append(\"-\" * 60)\n        lines.append(\"DETAILED SEGMENTS:\")\n        lines.append(\"\")\n\n        for i, segment in enumerate(result.segments, 1):\n            timestamp_str = f\"[{self._format_timestamp(segment.start_time)} - {self._format_timestamp(segment.end_time)}]\"\n            confidence_str = f\" (confidence: {segment.confidence:.2f})\" if self.include_confidence and segment.confidence else \"\"\n\n            lines.append(f\"{i:2d}. {timestamp_str}{confidence_str}\")\n            lines.append(f\"    {segment.text}\")\n            lines.append(\"\")\n\n    # Statistics\n    if result.segments:\n        lines.append(\"-\" * 60)\n        lines.append(\"STATISTICS:\")\n        avg_confidence = result.metadata.get('average_confidence')\n        if avg_confidence and self.include_confidence:\n            lines.append(f\"Average Confidence: {avg_confidence:.2f}\")\n        lines.append(f\"Total Segments: {len(result.segments)}\")\n\n    lines.append(\"=\" * 60)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/audio_aigented/formatting/__init__/#audio_aigented.formatting.OutputFormatter.format_as_attributed_text","title":"format_as_attributed_text","text":"<pre><code>format_as_attributed_text(\n    result: TranscriptionResult,\n) -&gt; str\n</code></pre> <p>Format transcription result as attributed text (theater play style).</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to format</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Formatted attributed text string with speaker labels</p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def format_as_attributed_text(self, result: TranscriptionResult) -&gt; str:\n    \"\"\"\n    Format transcription result as attributed text (theater play style).\n\n    Args:\n        result: TranscriptionResult to format\n\n    Returns:\n        Formatted attributed text string with speaker labels\n    \"\"\"\n    lines = []\n\n    # Only proceed if we have segments with speaker information\n    if not result.segments:\n        # Fallback to full text without attribution if no segments\n        lines.append(\"UNKNOWN_SPEAKER: \" + result.full_text)\n        return \"\\n\".join(lines)\n\n    # Group consecutive segments by speaker to avoid repetitive labels\n    current_speaker = None\n    current_text_parts = []\n\n    for segment in result.segments:\n        speaker_id = segment.speaker_id or \"UNKNOWN_SPEAKER\"\n\n        if speaker_id != current_speaker:\n            # Flush previous speaker's text if any\n            if current_speaker is not None and current_text_parts:\n                combined_text = \" \".join(current_text_parts).strip()\n                if combined_text:  # Only add non-empty text\n                    lines.append(f\"{current_speaker}: {combined_text}\")\n\n            # Start new speaker\n            current_speaker = speaker_id\n            current_text_parts = [segment.text.strip()]\n        else:\n            # Continue with same speaker\n            current_text_parts.append(segment.text.strip())\n\n    # Don't forget the last speaker's text\n    if current_speaker is not None and current_text_parts:\n        combined_text = \" \".join(current_text_parts).strip()\n        if combined_text:\n            lines.append(f\"{current_speaker}: {combined_text}\")\n\n    # If no valid segments were processed, use full text\n    if not lines and result.full_text.strip():\n        lines.append(\"UNKNOWN_SPEAKER: \" + result.full_text.strip())\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/audio_aigented/formatting/formatter/","title":"audio_aigented.formatting.formatter","text":""},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter","title":"audio_aigented.formatting.formatter","text":"<p>Output formatting for transcription results.</p> <p>This module handles formatting transcription results into structured JSON and human-readable text formats with proper timestamps and metadata.</p>"},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter.OutputFormatter","title":"OutputFormatter","text":"<pre><code>OutputFormatter(\n    include_timestamps: bool = True,\n    include_confidence: bool = True,\n    pretty_json: bool = True,\n)\n</code></pre> <p>Formats transcription results for output in various formats.</p> <p>Handles conversion to JSON and text formats with configurable options for timestamps, confidence scores, and formatting.</p> <p>Initialize the output formatter.</p> PARAMETER DESCRIPTION <code>include_timestamps</code> <p>Include timing information in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>include_confidence</code> <p>Include confidence scores in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>pretty_json</code> <p>Format JSON with indentation</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def __init__(self, include_timestamps: bool = True, include_confidence: bool = True, pretty_json: bool = True) -&gt; None:\n    \"\"\"\n    Initialize the output formatter.\n\n    Args:\n        include_timestamps: Include timing information in output\n        include_confidence: Include confidence scores in output\n        pretty_json: Format JSON with indentation\n    \"\"\"\n    self.include_timestamps = include_timestamps\n    self.include_confidence = include_confidence\n    self.pretty_json = pretty_json\n</code></pre>"},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter.OutputFormatter-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter.OutputFormatter.format_as_json","title":"format_as_json","text":"<pre><code>format_as_json(result: TranscriptionResult) -&gt; str\n</code></pre> <p>Format transcription result as JSON.</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to format</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>str</code> <p>JSON string representation</p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def format_as_json(self, result: TranscriptionResult) -&gt; str:\n    \"\"\"\n    Format transcription result as JSON.\n\n    Args:\n        result: TranscriptionResult to format\n\n    Returns:\n        JSON string representation\n    \"\"\"\n    # Create output dictionary\n    output = {\n        \"audio_file\": {\n            \"path\": str(result.audio_file.path),\n            \"duration\": result.audio_file.duration,\n            \"sample_rate\": result.audio_file.sample_rate,\n            \"channels\": result.audio_file.channels,\n            \"format\": result.audio_file.format\n        },\n        \"transcription\": {\n            \"full_text\": result.full_text,\n            \"segments\": self._format_segments_for_json(result.segments)\n        },\n        \"processing\": {\n            \"processing_time\": result.processing_time,\n            \"timestamp\": result.timestamp.isoformat(),\n            \"model_info\": result.model_info\n        },\n        \"metadata\": result.metadata\n    }\n\n    # Format JSON\n    if self.pretty_json:\n        return json.dumps(output, indent=2, ensure_ascii=False)\n    else:\n        return json.dumps(output, ensure_ascii=False)\n</code></pre>"},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter.OutputFormatter.format_as_text","title":"format_as_text","text":"<pre><code>format_as_text(result: TranscriptionResult) -&gt; str\n</code></pre> <p>Format transcription result as human-readable text.</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to format</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Formatted text string</p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def format_as_text(self, result: TranscriptionResult) -&gt; str:\n    \"\"\"\n    Format transcription result as human-readable text.\n\n    Args:\n        result: TranscriptionResult to format\n\n    Returns:\n        Formatted text string\n    \"\"\"\n    lines = []\n\n    # DIAGNOSTIC: Log result object state\n    logger.info(f\"DIAGNOSTIC - Formatting result for: {result.audio_file.path.name}\")\n    logger.info(f\"DIAGNOSTIC - Audio file duration: {result.audio_file.duration} (type: {type(result.audio_file.duration)})\")\n    logger.info(f\"DIAGNOSTIC - Processing time: {result.processing_time} (type: {type(result.processing_time)})\")\n    logger.info(f\"DIAGNOSTIC - Full text length: {len(result.full_text) if result.full_text else 'None'}\")\n    logger.info(f\"DIAGNOSTIC - Segments count: {len(result.segments)}\")\n\n    # Header\n    lines.append(\"=\" * 60)\n    lines.append(\"AUDIO TRANSCRIPTION REPORT\")\n    lines.append(\"=\" * 60)\n    lines.append(f\"File: {result.audio_file.path.name}\")\n\n    # Safe formatting with None checks\n    duration = result.audio_file.duration\n    if duration is not None:\n        lines.append(f\"Duration: {duration:.2f} seconds\")\n    else:\n        lines.append(\"Duration: Unknown\")\n\n    lines.append(f\"Processed: {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n    if result.processing_time:\n        speed_ratio = result.metadata.get('processing_speed_ratio', 0)\n        if speed_ratio is not None:\n            lines.append(f\"Processing Time: {result.processing_time:.2f}s (Speed: {speed_ratio:.2f}x)\")\n        else:\n            lines.append(f\"Processing Time: {result.processing_time:.2f}s\")\n\n    lines.append(\"-\" * 60)\n\n    # Full transcription\n    lines.append(\"FULL TRANSCRIPTION:\")\n    lines.append(\"\")\n    lines.append(result.full_text)\n    lines.append(\"\")\n\n    # Detailed segments (if timestamps enabled)\n    if self.include_timestamps and result.segments:\n        lines.append(\"-\" * 60)\n        lines.append(\"DETAILED SEGMENTS:\")\n        lines.append(\"\")\n\n        for i, segment in enumerate(result.segments, 1):\n            timestamp_str = f\"[{self._format_timestamp(segment.start_time)} - {self._format_timestamp(segment.end_time)}]\"\n            confidence_str = f\" (confidence: {segment.confidence:.2f})\" if self.include_confidence and segment.confidence else \"\"\n\n            lines.append(f\"{i:2d}. {timestamp_str}{confidence_str}\")\n            lines.append(f\"    {segment.text}\")\n            lines.append(\"\")\n\n    # Statistics\n    if result.segments:\n        lines.append(\"-\" * 60)\n        lines.append(\"STATISTICS:\")\n        avg_confidence = result.metadata.get('average_confidence')\n        if avg_confidence and self.include_confidence:\n            lines.append(f\"Average Confidence: {avg_confidence:.2f}\")\n        lines.append(f\"Total Segments: {len(result.segments)}\")\n\n    lines.append(\"=\" * 60)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/audio_aigented/formatting/formatter/#audio_aigented.formatting.formatter.OutputFormatter.format_as_attributed_text","title":"format_as_attributed_text","text":"<pre><code>format_as_attributed_text(\n    result: TranscriptionResult,\n) -&gt; str\n</code></pre> <p>Format transcription result as attributed text (theater play style).</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to format</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Formatted attributed text string with speaker labels</p> Source code in <code>src/audio_aigented/formatting/formatter.py</code> <pre><code>def format_as_attributed_text(self, result: TranscriptionResult) -&gt; str:\n    \"\"\"\n    Format transcription result as attributed text (theater play style).\n\n    Args:\n        result: TranscriptionResult to format\n\n    Returns:\n        Formatted attributed text string with speaker labels\n    \"\"\"\n    lines = []\n\n    # Only proceed if we have segments with speaker information\n    if not result.segments:\n        # Fallback to full text without attribution if no segments\n        lines.append(\"UNKNOWN_SPEAKER: \" + result.full_text)\n        return \"\\n\".join(lines)\n\n    # Group consecutive segments by speaker to avoid repetitive labels\n    current_speaker = None\n    current_text_parts = []\n\n    for segment in result.segments:\n        speaker_id = segment.speaker_id or \"UNKNOWN_SPEAKER\"\n\n        if speaker_id != current_speaker:\n            # Flush previous speaker's text if any\n            if current_speaker is not None and current_text_parts:\n                combined_text = \" \".join(current_text_parts).strip()\n                if combined_text:  # Only add non-empty text\n                    lines.append(f\"{current_speaker}: {combined_text}\")\n\n            # Start new speaker\n            current_speaker = speaker_id\n            current_text_parts = [segment.text.strip()]\n        else:\n            # Continue with same speaker\n            current_text_parts.append(segment.text.strip())\n\n    # Don't forget the last speaker's text\n    if current_speaker is not None and current_text_parts:\n        combined_text = \" \".join(current_text_parts).strip()\n        if combined_text:\n            lines.append(f\"{current_speaker}: {combined_text}\")\n\n    # If no valid segments were processed, use full text\n    if not lines and result.full_text.strip():\n        lines.append(\"UNKNOWN_SPEAKER: \" + result.full_text.strip())\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/audio_aigented/models/__init__/","title":"audio_aigented.models","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models","title":"audio_aigented.models","text":"<p>Data models and schemas module.</p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.AudioFile","title":"AudioFile","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an audio file to be processed.</p> ATTRIBUTE DESCRIPTION <code>path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> <code>sample_rate</code> <p>Sample rate of the audio file</p> <p> TYPE: <code>int | None</code> </p> <code>duration</code> <p>Duration in seconds</p> <p> TYPE: <code>float | None</code> </p> <code>channels</code> <p>Number of audio channels</p> <p> TYPE: <code>int | None</code> </p> <code>format</code> <p>Audio format (e.g., 'wav', 'mp3')</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.AudioFile-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.AudioFile.validate_path_exists","title":"validate_path_exists","text":"<pre><code>validate_path_exists(v: Path) -&gt; Path\n</code></pre> <p>Validate that the audio file exists.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('path')\ndef validate_path_exists(cls, v: Path) -&gt; Path:\n    \"\"\"Validate that the audio file exists.\"\"\"\n    if not v.exists():\n        raise ValueError(f\"Audio file does not exist: {v}\")\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.AudioSegment","title":"AudioSegment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a segment of transcribed audio with timing information.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>Transcribed text content</p> <p> TYPE: <code>str</code> </p> <code>start_time</code> <p>Start time in seconds</p> <p> TYPE: <code>float</code> </p> <code>end_time</code> <p>End time in seconds</p> <p> TYPE: <code>float</code> </p> <code>confidence</code> <p>Confidence score (0.0 to 1.0)</p> <p> TYPE: <code>float | None</code> </p> <code>speaker_id</code> <p>Optional speaker identifier</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.AudioSegment-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.AudioSegment.validate_end_after_start","title":"validate_end_after_start","text":"<pre><code>validate_end_after_start(\n    v: float, values: dict[str, Any]\n) -&gt; float\n</code></pre> <p>Ensure end_time is after start_time.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('end_time')\ndef validate_end_after_start(cls, v: float, values: dict[str, Any]) -&gt; float:\n    \"\"\"Ensure end_time is after start_time.\"\"\"\n    if 'start_time' in values and v &lt;= values['start_time']:\n        raise ValueError(\"end_time must be greater than start_time\")\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.PipelineStatus","title":"PipelineStatus","text":"<p>               Bases: <code>BaseModel</code></p> <p>Status information for pipeline processing.</p> ATTRIBUTE DESCRIPTION <code>total_files</code> <p>Total number of files to process</p> <p> TYPE: <code>int</code> </p> <code>completed_files</code> <p>Number of completed files</p> <p> TYPE: <code>int</code> </p> <code>failed_files</code> <p>Number of failed files</p> <p> TYPE: <code>int</code> </p> <code>current_file</code> <p>Currently processing file</p> <p> TYPE: <code>str | None</code> </p> <code>start_time</code> <p>Pipeline start time</p> <p> TYPE: <code>datetime</code> </p> <code>estimated_completion</code> <p>Estimated completion time</p> <p> TYPE: <code>datetime | None</code> </p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.PipelineStatus-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.PipelineStatus.progress_percentage","title":"progress_percentage  <code>property</code>","text":"<pre><code>progress_percentage: float\n</code></pre> <p>Calculate processing progress as percentage.</p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.PipelineStatus.is_complete","title":"is_complete  <code>property</code>","text":"<pre><code>is_complete: bool\n</code></pre> <p>Check if processing is complete.</p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.ProcessingConfig","title":"ProcessingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for audio processing pipeline.</p> ATTRIBUTE DESCRIPTION <code>input_dir</code> <p>Directory containing input audio files</p> <p> TYPE: <code>Path</code> </p> <code>output_dir</code> <p>Directory for output files</p> <p> TYPE: <code>Path</code> </p> <code>cache_dir</code> <p>Directory for caching models and intermediate results</p> <p> TYPE: <code>Path</code> </p> <code>audio</code> <p>Audio processing configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>transcription</code> <p>ASR model configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>output</code> <p>Output formatting configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>processing</code> <p>General processing options</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.ProcessingConfig-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.ProcessingConfig.ensure_directories_exist","title":"ensure_directories_exist","text":"<pre><code>ensure_directories_exist(v: Path) -&gt; Path\n</code></pre> <p>Ensure directories exist, create if they don't.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('input_dir', 'output_dir', 'cache_dir')\ndef ensure_directories_exist(cls, v: Path) -&gt; Path:\n    \"\"\"Ensure directories exist, create if they don't.\"\"\"\n    v.mkdir(parents=True, exist_ok=True)\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.TranscriptionResult","title":"TranscriptionResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete transcription result for an audio file.</p> ATTRIBUTE DESCRIPTION <code>audio_file</code> <p>Information about the source audio file</p> <p> TYPE: <code>AudioFile</code> </p> <code>segments</code> <p>List of transcribed segments with timing</p> <p> TYPE: <code>list[AudioSegment]</code> </p> <code>full_text</code> <p>Complete transcription text</p> <p> TYPE: <code>str</code> </p> <code>processing_time</code> <p>Time taken for processing in seconds</p> <p> TYPE: <code>float | None</code> </p> <code>model_info</code> <p>Information about the ASR model used</p> <p> TYPE: <code>dict[str, str]</code> </p> <code>timestamp</code> <p>When the transcription was created</p> <p> TYPE: <code>datetime</code> </p> <code>metadata</code> <p>Additional processing metadata</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.TranscriptionResult-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/__init__/#audio_aigented.models.TranscriptionResult.generate_full_text","title":"generate_full_text","text":"<pre><code>generate_full_text(v: str, values: dict[str, Any]) -&gt; str\n</code></pre> <p>Generate full text from segments if not provided.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('full_text', always=True)\ndef generate_full_text(cls, v: str, values: dict[str, Any]) -&gt; str:\n    \"\"\"Generate full text from segments if not provided.\"\"\"\n    if not v and 'segments' in values and values['segments']:\n        return ' '.join(segment.text for segment in values['segments'])\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/schemas/","title":"audio_aigented.models.schemas","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas","title":"audio_aigented.models.schemas","text":"<p>Pydantic data models for the audio transcription pipeline.</p> <p>This module defines the core data structures used throughout the system for type safety, validation, and serialization.</p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.AudioFile","title":"AudioFile","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an audio file to be processed.</p> ATTRIBUTE DESCRIPTION <code>path</code> <p>Path to the audio file</p> <p> TYPE: <code>Path</code> </p> <code>sample_rate</code> <p>Sample rate of the audio file</p> <p> TYPE: <code>int | None</code> </p> <code>duration</code> <p>Duration in seconds</p> <p> TYPE: <code>float | None</code> </p> <code>channels</code> <p>Number of audio channels</p> <p> TYPE: <code>int | None</code> </p> <code>format</code> <p>Audio format (e.g., 'wav', 'mp3')</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.AudioFile-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.AudioFile.validate_path_exists","title":"validate_path_exists","text":"<pre><code>validate_path_exists(v: Path) -&gt; Path\n</code></pre> <p>Validate that the audio file exists.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('path')\ndef validate_path_exists(cls, v: Path) -&gt; Path:\n    \"\"\"Validate that the audio file exists.\"\"\"\n    if not v.exists():\n        raise ValueError(f\"Audio file does not exist: {v}\")\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.AudioSegment","title":"AudioSegment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a segment of transcribed audio with timing information.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>Transcribed text content</p> <p> TYPE: <code>str</code> </p> <code>start_time</code> <p>Start time in seconds</p> <p> TYPE: <code>float</code> </p> <code>end_time</code> <p>End time in seconds</p> <p> TYPE: <code>float</code> </p> <code>confidence</code> <p>Confidence score (0.0 to 1.0)</p> <p> TYPE: <code>float | None</code> </p> <code>speaker_id</code> <p>Optional speaker identifier</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.AudioSegment-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.AudioSegment.validate_end_after_start","title":"validate_end_after_start","text":"<pre><code>validate_end_after_start(\n    v: float, values: dict[str, Any]\n) -&gt; float\n</code></pre> <p>Ensure end_time is after start_time.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('end_time')\ndef validate_end_after_start(cls, v: float, values: dict[str, Any]) -&gt; float:\n    \"\"\"Ensure end_time is after start_time.\"\"\"\n    if 'start_time' in values and v &lt;= values['start_time']:\n        raise ValueError(\"end_time must be greater than start_time\")\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.TranscriptionResult","title":"TranscriptionResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete transcription result for an audio file.</p> ATTRIBUTE DESCRIPTION <code>audio_file</code> <p>Information about the source audio file</p> <p> TYPE: <code>AudioFile</code> </p> <code>segments</code> <p>List of transcribed segments with timing</p> <p> TYPE: <code>list[AudioSegment]</code> </p> <code>full_text</code> <p>Complete transcription text</p> <p> TYPE: <code>str</code> </p> <code>processing_time</code> <p>Time taken for processing in seconds</p> <p> TYPE: <code>float | None</code> </p> <code>model_info</code> <p>Information about the ASR model used</p> <p> TYPE: <code>dict[str, str]</code> </p> <code>timestamp</code> <p>When the transcription was created</p> <p> TYPE: <code>datetime</code> </p> <code>metadata</code> <p>Additional processing metadata</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.TranscriptionResult-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.TranscriptionResult.generate_full_text","title":"generate_full_text","text":"<pre><code>generate_full_text(v: str, values: dict[str, Any]) -&gt; str\n</code></pre> <p>Generate full text from segments if not provided.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('full_text', always=True)\ndef generate_full_text(cls, v: str, values: dict[str, Any]) -&gt; str:\n    \"\"\"Generate full text from segments if not provided.\"\"\"\n    if not v and 'segments' in values and values['segments']:\n        return ' '.join(segment.text for segment in values['segments'])\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.ProcessingConfig","title":"ProcessingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for audio processing pipeline.</p> ATTRIBUTE DESCRIPTION <code>input_dir</code> <p>Directory containing input audio files</p> <p> TYPE: <code>Path</code> </p> <code>output_dir</code> <p>Directory for output files</p> <p> TYPE: <code>Path</code> </p> <code>cache_dir</code> <p>Directory for caching models and intermediate results</p> <p> TYPE: <code>Path</code> </p> <code>audio</code> <p>Audio processing configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>transcription</code> <p>ASR model configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>output</code> <p>Output formatting configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>processing</code> <p>General processing options</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.ProcessingConfig-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.ProcessingConfig.ensure_directories_exist","title":"ensure_directories_exist","text":"<pre><code>ensure_directories_exist(v: Path) -&gt; Path\n</code></pre> <p>Ensure directories exist, create if they don't.</p> Source code in <code>src/audio_aigented/models/schemas.py</code> <pre><code>@validator('input_dir', 'output_dir', 'cache_dir')\ndef ensure_directories_exist(cls, v: Path) -&gt; Path:\n    \"\"\"Ensure directories exist, create if they don't.\"\"\"\n    v.mkdir(parents=True, exist_ok=True)\n    return v\n</code></pre>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.PipelineStatus","title":"PipelineStatus","text":"<p>               Bases: <code>BaseModel</code></p> <p>Status information for pipeline processing.</p> ATTRIBUTE DESCRIPTION <code>total_files</code> <p>Total number of files to process</p> <p> TYPE: <code>int</code> </p> <code>completed_files</code> <p>Number of completed files</p> <p> TYPE: <code>int</code> </p> <code>failed_files</code> <p>Number of failed files</p> <p> TYPE: <code>int</code> </p> <code>current_file</code> <p>Currently processing file</p> <p> TYPE: <code>str | None</code> </p> <code>start_time</code> <p>Pipeline start time</p> <p> TYPE: <code>datetime</code> </p> <code>estimated_completion</code> <p>Estimated completion time</p> <p> TYPE: <code>datetime | None</code> </p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.PipelineStatus-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.PipelineStatus.progress_percentage","title":"progress_percentage  <code>property</code>","text":"<pre><code>progress_percentage: float\n</code></pre> <p>Calculate processing progress as percentage.</p>"},{"location":"reference/audio_aigented/models/schemas/#audio_aigented.models.schemas.PipelineStatus.is_complete","title":"is_complete  <code>property</code>","text":"<pre><code>is_complete: bool\n</code></pre> <p>Check if processing is complete.</p>"},{"location":"reference/audio_aigented/output/__init__/","title":"audio_aigented.output","text":""},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output","title":"audio_aigented.output","text":"<p>File output and writing module.</p>"},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter","title":"FileWriter","text":"<pre><code>FileWriter(config: ProcessingConfig)\n</code></pre> <p>Handles writing transcription results to disk.</p> <p>Creates output directories per audio file and writes JSON and TXT formats according to configuration settings.</p> <p>Initialize the file writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Processing configuration containing output settings</p> <p> TYPE: <code>ProcessingConfig</code> </p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def __init__(self, config: ProcessingConfig) -&gt; None:\n    \"\"\"\n    Initialize the file writer.\n\n    Args:\n        config: Processing configuration containing output settings\n    \"\"\"\n    self.config = config\n    self.output_dir = config.output_dir\n    self.output_formats = config.output.get(\"formats\", [\"json\", \"txt\"])\n\n    # Initialize formatter with config settings\n    self.formatter = OutputFormatter(\n        include_timestamps=config.output.get(\"include_timestamps\", True),\n        include_confidence=config.output.get(\"include_confidence\", True),\n        pretty_json=config.output.get(\"pretty_json\", True)\n    )\n\n    # Ensure output directory exists\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"FileWriter initialized with output dir: {self.output_dir}\")\n    logger.info(f\"Output formats: {self.output_formats}\")\n</code></pre>"},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter.write_transcription_result","title":"write_transcription_result","text":"<pre><code>write_transcription_result(\n    result: TranscriptionResult,\n) -&gt; list[Path]\n</code></pre> <p>Write transcription result to disk.</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to write</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>list[Path]</code> <p>List of created file paths</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def write_transcription_result(self, result: TranscriptionResult) -&gt; list[Path]:\n    \"\"\"\n    Write transcription result to disk.\n\n    Args:\n        result: TranscriptionResult to write\n\n    Returns:\n        List of created file paths\n    \"\"\"\n    # Create output directory for this audio file\n    audio_filename = result.audio_file.path.stem  # filename without extension\n    file_output_dir = self.output_dir / audio_filename\n    file_output_dir.mkdir(parents=True, exist_ok=True)\n\n    created_files = []\n\n    # Write each requested format\n    for format_type in self.output_formats:\n        try:\n            if format_type.lower() == \"json\":\n                json_path = self._write_json_output(result, file_output_dir)\n                created_files.append(json_path)\n\n            elif format_type.lower() == \"txt\":\n                txt_path = self._write_text_output(result, file_output_dir)\n                created_files.append(txt_path)\n\n            elif format_type.lower() == \"attributed_txt\":\n                attributed_txt_path = self._write_attributed_text_output(result, file_output_dir)\n                created_files.append(attributed_txt_path)\n\n            else:\n                logger.warning(f\"Unsupported output format: {format_type}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to write {format_type} output for {audio_filename}: {e}\")\n\n    logger.info(f\"Created {len(created_files)} output files for {audio_filename}\")\n    return created_files\n</code></pre>"},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter.write_batch_results","title":"write_batch_results","text":"<pre><code>write_batch_results(\n    results: list[TranscriptionResult],\n) -&gt; list[list[Path]]\n</code></pre> <p>Write multiple transcription results to disk.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of TranscriptionResult instances</p> <p> TYPE: <code>list[TranscriptionResult]</code> </p> RETURNS DESCRIPTION <code>list[list[Path]]</code> <p>List of lists containing created file paths for each result</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def write_batch_results(self, results: list[TranscriptionResult]) -&gt; list[list[Path]]:\n    \"\"\"\n    Write multiple transcription results to disk.\n\n    Args:\n        results: List of TranscriptionResult instances\n\n    Returns:\n        List of lists containing created file paths for each result\n    \"\"\"\n    all_created_files = []\n\n    for result in results:\n        try:\n            created_files = self.write_transcription_result(result)\n            all_created_files.append(created_files)\n\n        except Exception as e:\n            logger.error(f\"Failed to write result for {result.audio_file.path}: {e}\")\n            all_created_files.append([])  # Empty list for failed writes\n\n    successful_writes = sum(1 for files in all_created_files if files)\n    logger.info(f\"Batch write completed: {successful_writes}/{len(results)} files written successfully\")\n\n    return all_created_files\n</code></pre>"},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter.create_summary_report","title":"create_summary_report","text":"<pre><code>create_summary_report(\n    results: list[TranscriptionResult],\n) -&gt; Path\n</code></pre> <p>Create a summary report for all processed files.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of all TranscriptionResult instances</p> <p> TYPE: <code>list[TranscriptionResult]</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path to created summary report</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def create_summary_report(self, results: list[TranscriptionResult]) -&gt; Path:\n    \"\"\"\n    Create a summary report for all processed files.\n\n    Args:\n        results: List of all TranscriptionResult instances\n\n    Returns:\n        Path to created summary report\n    \"\"\"\n    summary_path = self.output_dir / \"processing_summary.txt\"\n\n    try:\n        lines = []\n        lines.append(\"AUDIO TRANSCRIPTION PROCESSING SUMMARY\")\n        lines.append(\"=\" * 50)\n        lines.append(f\"Total files processed: {len(results)}\")\n        lines.append(f\"Generated on: {results[0].timestamp.strftime('%Y-%m-%d %H:%M:%S') if results else 'N/A'}\")\n        lines.append(\"\")\n\n        # Statistics\n        successful_results = [r for r in results if r.full_text.strip()]\n        failed_results = len(results) - len(successful_results)\n\n        lines.append(\"PROCESSING STATISTICS:\")\n        lines.append(f\"  Successful: {len(successful_results)}\")\n        lines.append(f\"  Failed: {failed_results}\")\n\n        if successful_results:\n            total_duration = sum(r.audio_file.duration or 0 for r in successful_results)\n            total_processing_time = sum(r.processing_time or 0 for r in successful_results)\n            avg_speed_ratio = total_duration / total_processing_time if total_processing_time &gt; 0 else 0\n\n            lines.append(f\"  Total audio duration: {total_duration:.2f} seconds\")\n            lines.append(f\"  Total processing time: {total_processing_time:.2f} seconds\")\n            lines.append(f\"  Average speed ratio: {avg_speed_ratio:.2f}x\")\n\n        lines.append(\"\")\n\n        # File listing\n        lines.append(\"PROCESSED FILES:\")\n        for i, result in enumerate(results, 1):\n            status = \"\u2713\" if result.full_text.strip() else \"\u2717\"\n            duration = f\"{result.audio_file.duration:.2f}s\" if result.audio_file.duration else \"N/A\"\n            lines.append(f\"  {i:2d}. {status} {result.audio_file.path.name} ({duration})\")\n\n        lines.append(\"\")\n        lines.append(\"=\" * 50)\n\n        # Write summary\n        with open(summary_path, 'w', encoding='utf-8') as f:\n            f.write('\\n'.join(lines))\n\n        logger.info(f\"Summary report created: {summary_path}\")\n        return summary_path\n\n    except Exception as e:\n        logger.error(f\"Failed to create summary report: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter.get_output_directory","title":"get_output_directory","text":"<pre><code>get_output_directory(audio_filename: str) -&gt; Path\n</code></pre> <p>Get the output directory path for a specific audio file.</p> PARAMETER DESCRIPTION <code>audio_filename</code> <p>Name of the audio file (without extension)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path to the output directory for this file</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def get_output_directory(self, audio_filename: str) -&gt; Path:\n    \"\"\"\n    Get the output directory path for a specific audio file.\n\n    Args:\n        audio_filename: Name of the audio file (without extension)\n\n    Returns:\n        Path to the output directory for this file\n    \"\"\"\n    return self.output_dir / audio_filename\n</code></pre>"},{"location":"reference/audio_aigented/output/__init__/#audio_aigented.output.FileWriter.cleanup_empty_directories","title":"cleanup_empty_directories","text":"<pre><code>cleanup_empty_directories() -&gt; None\n</code></pre> <p>Clean up any empty output directories.</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def cleanup_empty_directories(self) -&gt; None:\n    \"\"\"\n    Clean up any empty output directories.\n    \"\"\"\n    try:\n        for item in self.output_dir.iterdir():\n            if item.is_dir() and not any(item.iterdir()):\n                item.rmdir()\n                logger.debug(f\"Removed empty directory: {item}\")\n\n    except Exception as e:\n        logger.warning(f\"Failed to cleanup empty directories: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/output/writer/","title":"audio_aigented.output.writer","text":""},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer","title":"audio_aigented.output.writer","text":"<p>File output and writing module.</p> <p>This module handles writing transcription results to disk in various formats with proper directory structure and file naming conventions.</p>"},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter","title":"FileWriter","text":"<pre><code>FileWriter(config: ProcessingConfig)\n</code></pre> <p>Handles writing transcription results to disk.</p> <p>Creates output directories per audio file and writes JSON and TXT formats according to configuration settings.</p> <p>Initialize the file writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Processing configuration containing output settings</p> <p> TYPE: <code>ProcessingConfig</code> </p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def __init__(self, config: ProcessingConfig) -&gt; None:\n    \"\"\"\n    Initialize the file writer.\n\n    Args:\n        config: Processing configuration containing output settings\n    \"\"\"\n    self.config = config\n    self.output_dir = config.output_dir\n    self.output_formats = config.output.get(\"formats\", [\"json\", \"txt\"])\n\n    # Initialize formatter with config settings\n    self.formatter = OutputFormatter(\n        include_timestamps=config.output.get(\"include_timestamps\", True),\n        include_confidence=config.output.get(\"include_confidence\", True),\n        pretty_json=config.output.get(\"pretty_json\", True)\n    )\n\n    # Ensure output directory exists\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"FileWriter initialized with output dir: {self.output_dir}\")\n    logger.info(f\"Output formats: {self.output_formats}\")\n</code></pre>"},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter.write_transcription_result","title":"write_transcription_result","text":"<pre><code>write_transcription_result(\n    result: TranscriptionResult,\n) -&gt; list[Path]\n</code></pre> <p>Write transcription result to disk.</p> PARAMETER DESCRIPTION <code>result</code> <p>TranscriptionResult to write</p> <p> TYPE: <code>TranscriptionResult</code> </p> RETURNS DESCRIPTION <code>list[Path]</code> <p>List of created file paths</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def write_transcription_result(self, result: TranscriptionResult) -&gt; list[Path]:\n    \"\"\"\n    Write transcription result to disk.\n\n    Args:\n        result: TranscriptionResult to write\n\n    Returns:\n        List of created file paths\n    \"\"\"\n    # Create output directory for this audio file\n    audio_filename = result.audio_file.path.stem  # filename without extension\n    file_output_dir = self.output_dir / audio_filename\n    file_output_dir.mkdir(parents=True, exist_ok=True)\n\n    created_files = []\n\n    # Write each requested format\n    for format_type in self.output_formats:\n        try:\n            if format_type.lower() == \"json\":\n                json_path = self._write_json_output(result, file_output_dir)\n                created_files.append(json_path)\n\n            elif format_type.lower() == \"txt\":\n                txt_path = self._write_text_output(result, file_output_dir)\n                created_files.append(txt_path)\n\n            elif format_type.lower() == \"attributed_txt\":\n                attributed_txt_path = self._write_attributed_text_output(result, file_output_dir)\n                created_files.append(attributed_txt_path)\n\n            else:\n                logger.warning(f\"Unsupported output format: {format_type}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to write {format_type} output for {audio_filename}: {e}\")\n\n    logger.info(f\"Created {len(created_files)} output files for {audio_filename}\")\n    return created_files\n</code></pre>"},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter.write_batch_results","title":"write_batch_results","text":"<pre><code>write_batch_results(\n    results: list[TranscriptionResult],\n) -&gt; list[list[Path]]\n</code></pre> <p>Write multiple transcription results to disk.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of TranscriptionResult instances</p> <p> TYPE: <code>list[TranscriptionResult]</code> </p> RETURNS DESCRIPTION <code>list[list[Path]]</code> <p>List of lists containing created file paths for each result</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def write_batch_results(self, results: list[TranscriptionResult]) -&gt; list[list[Path]]:\n    \"\"\"\n    Write multiple transcription results to disk.\n\n    Args:\n        results: List of TranscriptionResult instances\n\n    Returns:\n        List of lists containing created file paths for each result\n    \"\"\"\n    all_created_files = []\n\n    for result in results:\n        try:\n            created_files = self.write_transcription_result(result)\n            all_created_files.append(created_files)\n\n        except Exception as e:\n            logger.error(f\"Failed to write result for {result.audio_file.path}: {e}\")\n            all_created_files.append([])  # Empty list for failed writes\n\n    successful_writes = sum(1 for files in all_created_files if files)\n    logger.info(f\"Batch write completed: {successful_writes}/{len(results)} files written successfully\")\n\n    return all_created_files\n</code></pre>"},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter.create_summary_report","title":"create_summary_report","text":"<pre><code>create_summary_report(\n    results: list[TranscriptionResult],\n) -&gt; Path\n</code></pre> <p>Create a summary report for all processed files.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of all TranscriptionResult instances</p> <p> TYPE: <code>list[TranscriptionResult]</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path to created summary report</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def create_summary_report(self, results: list[TranscriptionResult]) -&gt; Path:\n    \"\"\"\n    Create a summary report for all processed files.\n\n    Args:\n        results: List of all TranscriptionResult instances\n\n    Returns:\n        Path to created summary report\n    \"\"\"\n    summary_path = self.output_dir / \"processing_summary.txt\"\n\n    try:\n        lines = []\n        lines.append(\"AUDIO TRANSCRIPTION PROCESSING SUMMARY\")\n        lines.append(\"=\" * 50)\n        lines.append(f\"Total files processed: {len(results)}\")\n        lines.append(f\"Generated on: {results[0].timestamp.strftime('%Y-%m-%d %H:%M:%S') if results else 'N/A'}\")\n        lines.append(\"\")\n\n        # Statistics\n        successful_results = [r for r in results if r.full_text.strip()]\n        failed_results = len(results) - len(successful_results)\n\n        lines.append(\"PROCESSING STATISTICS:\")\n        lines.append(f\"  Successful: {len(successful_results)}\")\n        lines.append(f\"  Failed: {failed_results}\")\n\n        if successful_results:\n            total_duration = sum(r.audio_file.duration or 0 for r in successful_results)\n            total_processing_time = sum(r.processing_time or 0 for r in successful_results)\n            avg_speed_ratio = total_duration / total_processing_time if total_processing_time &gt; 0 else 0\n\n            lines.append(f\"  Total audio duration: {total_duration:.2f} seconds\")\n            lines.append(f\"  Total processing time: {total_processing_time:.2f} seconds\")\n            lines.append(f\"  Average speed ratio: {avg_speed_ratio:.2f}x\")\n\n        lines.append(\"\")\n\n        # File listing\n        lines.append(\"PROCESSED FILES:\")\n        for i, result in enumerate(results, 1):\n            status = \"\u2713\" if result.full_text.strip() else \"\u2717\"\n            duration = f\"{result.audio_file.duration:.2f}s\" if result.audio_file.duration else \"N/A\"\n            lines.append(f\"  {i:2d}. {status} {result.audio_file.path.name} ({duration})\")\n\n        lines.append(\"\")\n        lines.append(\"=\" * 50)\n\n        # Write summary\n        with open(summary_path, 'w', encoding='utf-8') as f:\n            f.write('\\n'.join(lines))\n\n        logger.info(f\"Summary report created: {summary_path}\")\n        return summary_path\n\n    except Exception as e:\n        logger.error(f\"Failed to create summary report: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter.get_output_directory","title":"get_output_directory","text":"<pre><code>get_output_directory(audio_filename: str) -&gt; Path\n</code></pre> <p>Get the output directory path for a specific audio file.</p> PARAMETER DESCRIPTION <code>audio_filename</code> <p>Name of the audio file (without extension)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path to the output directory for this file</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def get_output_directory(self, audio_filename: str) -&gt; Path:\n    \"\"\"\n    Get the output directory path for a specific audio file.\n\n    Args:\n        audio_filename: Name of the audio file (without extension)\n\n    Returns:\n        Path to the output directory for this file\n    \"\"\"\n    return self.output_dir / audio_filename\n</code></pre>"},{"location":"reference/audio_aigented/output/writer/#audio_aigented.output.writer.FileWriter.cleanup_empty_directories","title":"cleanup_empty_directories","text":"<pre><code>cleanup_empty_directories() -&gt; None\n</code></pre> <p>Clean up any empty output directories.</p> Source code in <code>src/audio_aigented/output/writer.py</code> <pre><code>def cleanup_empty_directories(self) -&gt; None:\n    \"\"\"\n    Clean up any empty output directories.\n    \"\"\"\n    try:\n        for item in self.output_dir.iterdir():\n            if item.is_dir() and not any(item.iterdir()):\n                item.rmdir()\n                logger.debug(f\"Removed empty directory: {item}\")\n\n    except Exception as e:\n        logger.warning(f\"Failed to cleanup empty directories: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/transcription/__init__/","title":"audio_aigented.transcription","text":""},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription","title":"audio_aigented.transcription","text":"<p>Speech recognition and transcription module.</p>"},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber","title":"ASRTranscriber","text":"<pre><code>ASRTranscriber(config: ProcessingConfig)\n</code></pre> <p>NVIDIA NeMo-based automatic speech recognition transcriber.</p> <p>Handles loading pre-trained models, GPU acceleration, and batch processing for efficient speech-to-text conversion.</p> <p>Initialize the ASR transcriber.</p> PARAMETER DESCRIPTION <code>config</code> <p>Processing configuration containing transcription settings</p> <p> TYPE: <code>ProcessingConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If NVIDIA NeMo is not available</p> <code>RuntimeError</code> <p>If GPU is requested but not available</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def __init__(self, config: ProcessingConfig) -&gt; None:\n    \"\"\"\n    Initialize the ASR transcriber.\n\n    Args:\n        config: Processing configuration containing transcription settings\n\n    Raises:\n        ImportError: If NVIDIA NeMo is not available\n        RuntimeError: If GPU is requested but not available\n    \"\"\"\n    if not NEMO_AVAILABLE:\n        raise ImportError(\n            \"NVIDIA NeMo is not available. Please install with: \"\n            \"pip install nemo-toolkit[asr]\"\n        )\n\n    self.config = config\n    self.transcription_config = config.transcription\n    self.device = self._setup_device()\n    self.model: Any | None = None\n    self.model_name = self.transcription_config[\"model_name\"]\n    self.enable_confidence = self.transcription_config.get(\"enable_confidence_scores\", True)\n\n    logger.info(f\"ASRTranscriber initialized with model: {self.model_name}\")\n    logger.info(f\"Using device: {self.device}\")\n</code></pre>"},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber.is_model_loaded","title":"is_model_loaded  <code>property</code>","text":"<pre><code>is_model_loaded: bool\n</code></pre> <p>Check if the ASR model is loaded.</p>"},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber.model_info","title":"model_info  <code>property</code>","text":"<pre><code>model_info: dict[str, Any]\n</code></pre> <p>Get information about the loaded model.</p>"},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber.load_model","title":"load_model","text":"<pre><code>load_model() -&gt; None\n</code></pre> <p>Load the NVIDIA NeMo ASR model.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def load_model(self) -&gt; None:\n    \"\"\"\n    Load the NVIDIA NeMo ASR model.\n\n    Raises:\n        RuntimeError: If model loading fails\n    \"\"\"\n    try:\n        logger.info(f\"Loading NeMo ASR model: {self.model_name}\")\n        start_time = time.time()\n\n        # Load pre-trained model\n        self.model = nemo_asr.models.ASRModel.from_pretrained(\n            model_name=self.model_name,\n            map_location=self.device\n        )\n\n        # Move model to device\n        if self.device == \"cuda\":\n            self.model = self.model.cuda()\n        else:\n            self.model = self.model.cpu()\n\n        # Set to evaluation mode\n        self.model.eval()\n\n        load_time = time.time() - start_time\n        logger.info(f\"Model loaded successfully in {load_time:.2f} seconds\")\n\n        # Cache model info - DIAGNOSTIC: Convert all values to strings for Pydantic compatibility\n        self._model_info = {\n            \"name\": self.model_name,\n            \"device\": self.device,\n            \"load_time\": f\"{load_time:.2f}\",  # Convert float to string\n            \"sample_rate\": str(getattr(self.model, 'sample_rate', 16000))  # Convert int to string\n        }\n\n        logger.info(f\"DIAGNOSTIC - Model info types: {[(k, type(v)) for k, v in self._model_info.items()]}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to load model {self.model_name}: {e}\")\n        raise RuntimeError(f\"Model loading failed: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber.transcribe_audio_file","title":"transcribe_audio_file","text":"<pre><code>transcribe_audio_file(\n    audio_file: AudioFile, audio_data: ndarray\n) -&gt; TranscriptionResult\n</code></pre> <p>Transcribe a complete audio file.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>AudioFile instance with metadata</p> <p> TYPE: <code>AudioFile</code> </p> <code>audio_data</code> <p>Audio data array</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>TranscriptionResult</code> <p>TranscriptionResult with complete transcription</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def transcribe_audio_file(self, audio_file: AudioFile, audio_data: np.ndarray) -&gt; TranscriptionResult:\n    \"\"\"\n    Transcribe a complete audio file.\n\n    Args:\n        audio_file: AudioFile instance with metadata\n        audio_data: Audio data array\n\n    Returns:\n        TranscriptionResult with complete transcription\n    \"\"\"\n    if self.model is None:\n        self.load_model()\n\n    logger.info(f\"Transcribing audio file: {audio_file.path.name}\")\n\n    # DIAGNOSTIC: Log audio data size and memory usage\n    audio_size_mb = audio_data.nbytes / (1024 * 1024)\n    duration = len(audio_data) / self.config.audio[\"sample_rate\"]\n    logger.info(f\"DIAGNOSTIC - Audio size: {audio_size_mb:.2f} MB, Duration: {duration:.2f}s\")\n\n    # Clear CUDA cache before processing\n    if self.device == \"cuda\":\n        torch.cuda.empty_cache()\n        logger.info(\"DIAGNOSTIC - Cleared CUDA cache before processing\")\n\n    start_time = time.time()\n\n    try:\n        # Get transcription segments with memory-efficient processing\n        segments = self._transcribe_segments_chunked(audio_data)\n\n        # Create full text\n        full_text = ' '.join(segment.text for segment in segments)\n\n        processing_time = time.time() - start_time\n\n        # Create transcription result\n        result = TranscriptionResult(\n            audio_file=audio_file,\n            segments=segments,\n            full_text=full_text,\n            processing_time=processing_time,\n            model_info=self._model_info.copy(),\n            metadata={\n                \"segments_count\": len(segments),\n                \"average_confidence\": self._calculate_average_confidence(segments),\n                \"processing_speed_ratio\": audio_file.duration / processing_time if audio_file.duration else 0\n            }\n        )\n\n        logger.info(f\"Transcription completed in {processing_time:.2f}s \"\n                   f\"(speed ratio: {result.metadata['processing_speed_ratio']:.2f}x)\")\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Transcription failed for {audio_file.path}: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/transcription/__init__/#audio_aigented.transcription.ASRTranscriber.transcribe_batch","title":"transcribe_batch","text":"<pre><code>transcribe_batch(\n    audio_files: list[tuple[AudioFile, ndarray]],\n) -&gt; list[TranscriptionResult]\n</code></pre> <p>Transcribe multiple audio files in batch.</p> PARAMETER DESCRIPTION <code>audio_files</code> <p>List of (AudioFile, audio_data) tuples</p> <p> TYPE: <code>list[tuple[AudioFile, ndarray]]</code> </p> RETURNS DESCRIPTION <code>list[TranscriptionResult]</code> <p>List of TranscriptionResult instances</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def transcribe_batch(self, audio_files: list[tuple[AudioFile, np.ndarray]]) -&gt; list[TranscriptionResult]:\n    \"\"\"\n    Transcribe multiple audio files in batch.\n\n    Args:\n        audio_files: List of (AudioFile, audio_data) tuples\n\n    Returns:\n        List of TranscriptionResult instances\n    \"\"\"\n    if self.model is None:\n        self.load_model()\n\n    results = []\n\n    for audio_file, audio_data in tqdm(audio_files, desc=\"Transcribing audio files\"):\n        try:\n            result = self.transcribe_audio_file(audio_file, audio_data)\n            results.append(result)\n\n        except Exception as e:\n            logger.error(f\"Failed to transcribe {audio_file.path}: {e}\")\n            # Create empty result for failed transcription\n            empty_result = TranscriptionResult(\n                audio_file=audio_file,\n                segments=[],\n                full_text=\"\",\n                processing_time=0.0,\n                model_info=self._model_info.copy() if hasattr(self, '_model_info') else {},\n                metadata={\"error\": str(e)}\n            )\n            results.append(empty_result)\n\n    logger.info(f\"Batch transcription completed: {len(results)} files processed\")\n    return results\n</code></pre>"},{"location":"reference/audio_aigented/transcription/asr/","title":"audio_aigented.transcription.asr","text":""},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr","title":"audio_aigented.transcription.asr","text":"<p>Automatic Speech Recognition (ASR) using NVIDIA NeMo.</p> <p>This module provides ASR functionality using NVIDIA NeMo's pre-trained models with GPU acceleration and efficient batch processing.</p>"},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr-classes","title":"Classes","text":""},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber","title":"ASRTranscriber","text":"<pre><code>ASRTranscriber(config: ProcessingConfig)\n</code></pre> <p>NVIDIA NeMo-based automatic speech recognition transcriber.</p> <p>Handles loading pre-trained models, GPU acceleration, and batch processing for efficient speech-to-text conversion.</p> <p>Initialize the ASR transcriber.</p> PARAMETER DESCRIPTION <code>config</code> <p>Processing configuration containing transcription settings</p> <p> TYPE: <code>ProcessingConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If NVIDIA NeMo is not available</p> <code>RuntimeError</code> <p>If GPU is requested but not available</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def __init__(self, config: ProcessingConfig) -&gt; None:\n    \"\"\"\n    Initialize the ASR transcriber.\n\n    Args:\n        config: Processing configuration containing transcription settings\n\n    Raises:\n        ImportError: If NVIDIA NeMo is not available\n        RuntimeError: If GPU is requested but not available\n    \"\"\"\n    if not NEMO_AVAILABLE:\n        raise ImportError(\n            \"NVIDIA NeMo is not available. Please install with: \"\n            \"pip install nemo-toolkit[asr]\"\n        )\n\n    self.config = config\n    self.transcription_config = config.transcription\n    self.device = self._setup_device()\n    self.model: Any | None = None\n    self.model_name = self.transcription_config[\"model_name\"]\n    self.enable_confidence = self.transcription_config.get(\"enable_confidence_scores\", True)\n\n    logger.info(f\"ASRTranscriber initialized with model: {self.model_name}\")\n    logger.info(f\"Using device: {self.device}\")\n</code></pre>"},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber-attributes","title":"Attributes","text":""},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber.is_model_loaded","title":"is_model_loaded  <code>property</code>","text":"<pre><code>is_model_loaded: bool\n</code></pre> <p>Check if the ASR model is loaded.</p>"},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber.model_info","title":"model_info  <code>property</code>","text":"<pre><code>model_info: dict[str, Any]\n</code></pre> <p>Get information about the loaded model.</p>"},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber.load_model","title":"load_model","text":"<pre><code>load_model() -&gt; None\n</code></pre> <p>Load the NVIDIA NeMo ASR model.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def load_model(self) -&gt; None:\n    \"\"\"\n    Load the NVIDIA NeMo ASR model.\n\n    Raises:\n        RuntimeError: If model loading fails\n    \"\"\"\n    try:\n        logger.info(f\"Loading NeMo ASR model: {self.model_name}\")\n        start_time = time.time()\n\n        # Load pre-trained model\n        self.model = nemo_asr.models.ASRModel.from_pretrained(\n            model_name=self.model_name,\n            map_location=self.device\n        )\n\n        # Move model to device\n        if self.device == \"cuda\":\n            self.model = self.model.cuda()\n        else:\n            self.model = self.model.cpu()\n\n        # Set to evaluation mode\n        self.model.eval()\n\n        load_time = time.time() - start_time\n        logger.info(f\"Model loaded successfully in {load_time:.2f} seconds\")\n\n        # Cache model info - DIAGNOSTIC: Convert all values to strings for Pydantic compatibility\n        self._model_info = {\n            \"name\": self.model_name,\n            \"device\": self.device,\n            \"load_time\": f\"{load_time:.2f}\",  # Convert float to string\n            \"sample_rate\": str(getattr(self.model, 'sample_rate', 16000))  # Convert int to string\n        }\n\n        logger.info(f\"DIAGNOSTIC - Model info types: {[(k, type(v)) for k, v in self._model_info.items()]}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to load model {self.model_name}: {e}\")\n        raise RuntimeError(f\"Model loading failed: {e}\")\n</code></pre>"},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber.transcribe_audio_file","title":"transcribe_audio_file","text":"<pre><code>transcribe_audio_file(\n    audio_file: AudioFile, audio_data: ndarray\n) -&gt; TranscriptionResult\n</code></pre> <p>Transcribe a complete audio file.</p> PARAMETER DESCRIPTION <code>audio_file</code> <p>AudioFile instance with metadata</p> <p> TYPE: <code>AudioFile</code> </p> <code>audio_data</code> <p>Audio data array</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>TranscriptionResult</code> <p>TranscriptionResult with complete transcription</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def transcribe_audio_file(self, audio_file: AudioFile, audio_data: np.ndarray) -&gt; TranscriptionResult:\n    \"\"\"\n    Transcribe a complete audio file.\n\n    Args:\n        audio_file: AudioFile instance with metadata\n        audio_data: Audio data array\n\n    Returns:\n        TranscriptionResult with complete transcription\n    \"\"\"\n    if self.model is None:\n        self.load_model()\n\n    logger.info(f\"Transcribing audio file: {audio_file.path.name}\")\n\n    # DIAGNOSTIC: Log audio data size and memory usage\n    audio_size_mb = audio_data.nbytes / (1024 * 1024)\n    duration = len(audio_data) / self.config.audio[\"sample_rate\"]\n    logger.info(f\"DIAGNOSTIC - Audio size: {audio_size_mb:.2f} MB, Duration: {duration:.2f}s\")\n\n    # Clear CUDA cache before processing\n    if self.device == \"cuda\":\n        torch.cuda.empty_cache()\n        logger.info(\"DIAGNOSTIC - Cleared CUDA cache before processing\")\n\n    start_time = time.time()\n\n    try:\n        # Get transcription segments with memory-efficient processing\n        segments = self._transcribe_segments_chunked(audio_data)\n\n        # Create full text\n        full_text = ' '.join(segment.text for segment in segments)\n\n        processing_time = time.time() - start_time\n\n        # Create transcription result\n        result = TranscriptionResult(\n            audio_file=audio_file,\n            segments=segments,\n            full_text=full_text,\n            processing_time=processing_time,\n            model_info=self._model_info.copy(),\n            metadata={\n                \"segments_count\": len(segments),\n                \"average_confidence\": self._calculate_average_confidence(segments),\n                \"processing_speed_ratio\": audio_file.duration / processing_time if audio_file.duration else 0\n            }\n        )\n\n        logger.info(f\"Transcription completed in {processing_time:.2f}s \"\n                   f\"(speed ratio: {result.metadata['processing_speed_ratio']:.2f}x)\")\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Transcription failed for {audio_file.path}: {e}\")\n        raise\n</code></pre>"},{"location":"reference/audio_aigented/transcription/asr/#audio_aigented.transcription.asr.ASRTranscriber.transcribe_batch","title":"transcribe_batch","text":"<pre><code>transcribe_batch(\n    audio_files: list[tuple[AudioFile, ndarray]],\n) -&gt; list[TranscriptionResult]\n</code></pre> <p>Transcribe multiple audio files in batch.</p> PARAMETER DESCRIPTION <code>audio_files</code> <p>List of (AudioFile, audio_data) tuples</p> <p> TYPE: <code>list[tuple[AudioFile, ndarray]]</code> </p> RETURNS DESCRIPTION <code>list[TranscriptionResult]</code> <p>List of TranscriptionResult instances</p> Source code in <code>src/audio_aigented/transcription/asr.py</code> <pre><code>def transcribe_batch(self, audio_files: list[tuple[AudioFile, np.ndarray]]) -&gt; list[TranscriptionResult]:\n    \"\"\"\n    Transcribe multiple audio files in batch.\n\n    Args:\n        audio_files: List of (AudioFile, audio_data) tuples\n\n    Returns:\n        List of TranscriptionResult instances\n    \"\"\"\n    if self.model is None:\n        self.load_model()\n\n    results = []\n\n    for audio_file, audio_data in tqdm(audio_files, desc=\"Transcribing audio files\"):\n        try:\n            result = self.transcribe_audio_file(audio_file, audio_data)\n            results.append(result)\n\n        except Exception as e:\n            logger.error(f\"Failed to transcribe {audio_file.path}: {e}\")\n            # Create empty result for failed transcription\n            empty_result = TranscriptionResult(\n                audio_file=audio_file,\n                segments=[],\n                full_text=\"\",\n                processing_time=0.0,\n                model_info=self._model_info.copy() if hasattr(self, '_model_info') else {},\n                metadata={\"error\": str(e)}\n            )\n            results.append(empty_result)\n\n    logger.info(f\"Batch transcription completed: {len(results)} files processed\")\n    return results\n</code></pre>"},{"location":"reference/audio_aigented/utils/__init__/","title":"audio_aigented.utils","text":""},{"location":"reference/audio_aigented/utils/__init__/#audio_aigented.utils","title":"audio_aigented.utils","text":"<p>Utility functions and decorators.</p>"},{"location":"reference/audio_aigented/utils/__init__/#audio_aigented.utils-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/utils/__init__/#audio_aigented.utils.retry_on_error","title":"retry_on_error","text":"<pre><code>retry_on_error(\n    max_attempts: int = 3,\n    delay: float = 1.0,\n    backoff: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,),\n    logger: Optional[Logger] = None,\n) -&gt; Callable[[Callable[..., T]], Callable[..., T]]\n</code></pre> <p>Decorator to retry a function on error with exponential backoff.</p> PARAMETER DESCRIPTION <code>max_attempts</code> <p>Maximum number of attempts</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>delay</code> <p>Initial delay between retries in seconds</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>backoff</code> <p>Backoff multiplier for each retry</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>exceptions</code> <p>Tuple of exception types to catch and retry</p> <p> TYPE: <code>Tuple[Type[Exception], ...]</code> DEFAULT: <code>(Exception,)</code> </p> <code>logger</code> <p>Optional logger for retry messages</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable[..., T]], Callable[..., T]]</code> <p>Decorated function that retries on specified errors</p> Source code in <code>src/audio_aigented/utils/decorators.py</code> <pre><code>def retry_on_error(\n    max_attempts: int = 3,\n    delay: float = 1.0,\n    backoff: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,),\n    logger: Optional[logging.Logger] = None\n) -&gt; Callable[[Callable[..., T]], Callable[..., T]]:\n    \"\"\"\n    Decorator to retry a function on error with exponential backoff.\n\n    Args:\n        max_attempts: Maximum number of attempts\n        delay: Initial delay between retries in seconds\n        backoff: Backoff multiplier for each retry\n        exceptions: Tuple of exception types to catch and retry\n        logger: Optional logger for retry messages\n\n    Returns:\n        Decorated function that retries on specified errors\n    \"\"\"\n    def decorator(func: Callable[..., T]) -&gt; Callable[..., T]:\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; T:\n            nonlocal logger\n            if logger is None:\n                logger = logging.getLogger(func.__module__)\n\n            last_exception = None\n            current_delay = delay\n\n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n\n                    if attempt &lt; max_attempts - 1:\n                        logger.warning(\n                            f\"{func.__name__} failed (attempt {attempt + 1}/{max_attempts}): {e}\"\n                            f\" - Retrying in {current_delay:.1f}s\"\n                        )\n                        time.sleep(current_delay)\n                        current_delay *= backoff\n                    else:\n                        logger.error(\n                            f\"{func.__name__} failed after {max_attempts} attempts: {e}\"\n                        )\n\n            # If we get here, all attempts failed\n            if last_exception:\n                raise last_exception\n            else:\n                raise RuntimeError(f\"{func.__name__} failed without exception\")\n\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"reference/audio_aigented/utils/decorators/","title":"audio_aigented.utils.decorators","text":""},{"location":"reference/audio_aigented/utils/decorators/#audio_aigented.utils.decorators","title":"audio_aigented.utils.decorators","text":"<p>Utility decorators for common patterns.</p> <p>This module provides reusable decorators for error handling, retries, and other cross-cutting concerns.</p>"},{"location":"reference/audio_aigented/utils/decorators/#audio_aigented.utils.decorators-functions","title":"Functions","text":""},{"location":"reference/audio_aigented/utils/decorators/#audio_aigented.utils.decorators.retry_on_error","title":"retry_on_error","text":"<pre><code>retry_on_error(\n    max_attempts: int = 3,\n    delay: float = 1.0,\n    backoff: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,),\n    logger: Optional[Logger] = None,\n) -&gt; Callable[[Callable[..., T]], Callable[..., T]]\n</code></pre> <p>Decorator to retry a function on error with exponential backoff.</p> PARAMETER DESCRIPTION <code>max_attempts</code> <p>Maximum number of attempts</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>delay</code> <p>Initial delay between retries in seconds</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>backoff</code> <p>Backoff multiplier for each retry</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>exceptions</code> <p>Tuple of exception types to catch and retry</p> <p> TYPE: <code>Tuple[Type[Exception], ...]</code> DEFAULT: <code>(Exception,)</code> </p> <code>logger</code> <p>Optional logger for retry messages</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable[..., T]], Callable[..., T]]</code> <p>Decorated function that retries on specified errors</p> Source code in <code>src/audio_aigented/utils/decorators.py</code> <pre><code>def retry_on_error(\n    max_attempts: int = 3,\n    delay: float = 1.0,\n    backoff: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,),\n    logger: Optional[logging.Logger] = None\n) -&gt; Callable[[Callable[..., T]], Callable[..., T]]:\n    \"\"\"\n    Decorator to retry a function on error with exponential backoff.\n\n    Args:\n        max_attempts: Maximum number of attempts\n        delay: Initial delay between retries in seconds\n        backoff: Backoff multiplier for each retry\n        exceptions: Tuple of exception types to catch and retry\n        logger: Optional logger for retry messages\n\n    Returns:\n        Decorated function that retries on specified errors\n    \"\"\"\n    def decorator(func: Callable[..., T]) -&gt; Callable[..., T]:\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; T:\n            nonlocal logger\n            if logger is None:\n                logger = logging.getLogger(func.__module__)\n\n            last_exception = None\n            current_delay = delay\n\n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n\n                    if attempt &lt; max_attempts - 1:\n                        logger.warning(\n                            f\"{func.__name__} failed (attempt {attempt + 1}/{max_attempts}): {e}\"\n                            f\" - Retrying in {current_delay:.1f}s\"\n                        )\n                        time.sleep(current_delay)\n                        current_delay *= backoff\n                    else:\n                        logger.error(\n                            f\"{func.__name__} failed after {max_attempts} attempts: {e}\"\n                        )\n\n            # If we get here, all attempts failed\n            if last_exception:\n                raise last_exception\n            else:\n                raise RuntimeError(f\"{func.__name__} failed without exception\")\n\n        return wrapper\n    return decorator\n</code></pre>"}]}